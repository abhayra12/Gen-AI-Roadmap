{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f0c148f",
   "metadata": {},
   "source": [
    "# üñºÔ∏è Notebook 02: Convolutional Neural Networks (CNNs)\n",
    "\n",
    "**Week 3-4: Deep Learning & NLP Foundations**  \n",
    "**Gen AI Masters Program**\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Objectives\n",
    "\n",
    "By the end of this notebook, you will master:\n",
    "1. ‚úÖ Convolution operation and filters\n",
    "2. ‚úÖ Pooling layers\n",
    "3. ‚úÖ CNN architecture components\n",
    "4. ‚úÖ Building CNNs with PyTorch\n",
    "5. ‚úÖ Image classification\n",
    "6. ‚úÖ Transfer learning basics\n",
    "\n",
    "**Estimated Time:** 3-4 hours\n",
    "\n",
    "---\n",
    "\n",
    "## üìö What are CNNs?\n",
    "\n",
    "CNNs are specialized neural networks designed for **visual data**. They're used in:\n",
    "- üì∏ Image classification\n",
    "- üéØ Object detection\n",
    "- üè• Medical imaging\n",
    "- üè≠ Quality inspection (our focus!)\n",
    "- ü§ñ Autonomous vehicles\n",
    "\n",
    "**Key Insight**: CNNs automatically learn spatial hierarchies of features!\n",
    "\n",
    "Let's dive in! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88429173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03468c9",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Understanding Convolution\n",
    "\n",
    "### What is Convolution?\n",
    "\n",
    "Convolution slides a **filter (kernel)** over an image to detect features:\n",
    "- Edges\n",
    "- Textures\n",
    "- Patterns\n",
    "- Objects\n",
    "\n",
    "```\n",
    "Image * Filter = Feature Map\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d338c4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple image (8x8)\n",
    "simple_image = np.array([\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 1, 1, 1, 1, 1, 0],\n",
    "    [0, 1, 0, 0, 0, 0, 1, 0],\n",
    "    [0, 1, 0, 0, 0, 0, 1, 0],\n",
    "    [0, 1, 0, 0, 0, 0, 1, 0],\n",
    "    [0, 1, 0, 0, 0, 0, 1, 0],\n",
    "    [0, 1, 1, 1, 1, 1, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "], dtype=np.float32)\n",
    "\n",
    "# Define edge detection filters\n",
    "vertical_filter = np.array([\n",
    "    [-1, 0, 1],\n",
    "    [-1, 0, 1],\n",
    "    [-1, 0, 1]\n",
    "], dtype=np.float32)\n",
    "\n",
    "horizontal_filter = np.array([\n",
    "    [-1, -1, -1],\n",
    "    [ 0,  0,  0],\n",
    "    [ 1,  1,  1]\n",
    "], dtype=np.float32)\n",
    "\n",
    "# Manual convolution\n",
    "def convolve2d(image, kernel):\n",
    "    \"\"\"Simple 2D convolution\"\"\"\n",
    "    kernel_size = kernel.shape[0]\n",
    "    output_size = image.shape[0] - kernel_size + 1\n",
    "    output = np.zeros((output_size, output_size))\n",
    "    \n",
    "    for i in range(output_size):\n",
    "        for j in range(output_size):\n",
    "            region = image[i:i+kernel_size, j:j+kernel_size]\n",
    "            output[i, j] = np.sum(region * kernel)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Apply filters\n",
    "vertical_edges = convolve2d(simple_image, vertical_filter)\n",
    "horizontal_edges = convolve2d(simple_image, horizontal_filter)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Original image\n",
    "axes[0, 0].imshow(simple_image, cmap='gray')\n",
    "axes[0, 0].set_title('Original Image (Rectangle)', fontweight='bold', fontsize=12)\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Vertical filter\n",
    "axes[0, 1].imshow(vertical_filter, cmap='seismic', vmin=-1, vmax=1)\n",
    "axes[0, 1].set_title('Vertical Edge Filter', fontweight='bold', fontsize=12)\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Vertical edges\n",
    "axes[0, 2].imshow(vertical_edges, cmap='seismic')\n",
    "axes[0, 2].set_title('Detected Vertical Edges', fontweight='bold', fontsize=12)\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# Original image (repeat)\n",
    "axes[1, 0].imshow(simple_image, cmap='gray')\n",
    "axes[1, 0].set_title('Original Image (Rectangle)', fontweight='bold', fontsize=12)\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Horizontal filter\n",
    "axes[1, 1].imshow(horizontal_filter, cmap='seismic', vmin=-1, vmax=1)\n",
    "axes[1, 1].set_title('Horizontal Edge Filter', fontweight='bold', fontsize=12)\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# Horizontal edges\n",
    "axes[1, 2].imshow(horizontal_edges, cmap='seismic')\n",
    "axes[1, 2].set_title('Detected Horizontal Edges', fontweight='bold', fontsize=12)\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Convolution extracts features automatically!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7079c0d",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ CNN Components\n",
    "\n",
    "### Key Layers\n",
    "\n",
    "1. **Convolutional Layer**: Detects features\n",
    "2. **Activation (ReLU)**: Adds non-linearity\n",
    "3. **Pooling Layer**: Reduces spatial dimensions\n",
    "4. **Fully Connected**: Final classification\n",
    "\n",
    "### Pooling Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689c0320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate pooling\n",
    "test_feature_map = np.array([\n",
    "    [1, 3, 2, 4],\n",
    "    [5, 6, 1, 2],\n",
    "    [3, 2, 4, 5],\n",
    "    [7, 1, 3, 6]\n",
    "], dtype=np.float32)\n",
    "\n",
    "def max_pool_2x2(matrix):\n",
    "    \"\"\"Max pooling with 2x2 window\"\"\"\n",
    "    h, w = matrix.shape\n",
    "    output = np.zeros((h//2, w//2))\n",
    "    for i in range(0, h, 2):\n",
    "        for j in range(0, w, 2):\n",
    "            output[i//2, j//2] = np.max(matrix[i:i+2, j:j+2])\n",
    "    return output\n",
    "\n",
    "def avg_pool_2x2(matrix):\n",
    "    \"\"\"Average pooling with 2x2 window\"\"\"\n",
    "    h, w = matrix.shape\n",
    "    output = np.zeros((h//2, w//2))\n",
    "    for i in range(0, h, 2):\n",
    "        for j in range(0, w, 2):\n",
    "            output[i//2, j//2] = np.mean(matrix[i:i+2, j:j+2])\n",
    "    return output\n",
    "\n",
    "max_pooled = max_pool_2x2(test_feature_map)\n",
    "avg_pooled = avg_pool_2x2(test_feature_map)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].imshow(test_feature_map, cmap='viridis')\n",
    "axes[0].set_title('Original Feature Map (4x4)', fontweight='bold', fontsize=12)\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        axes[0].text(j, i, f'{test_feature_map[i, j]:.0f}', \n",
    "                    ha='center', va='center', color='white', fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(max_pooled, cmap='viridis')\n",
    "axes[1].set_title('Max Pooling (2x2) ‚Üí 2x2', fontweight='bold', fontsize=12)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[1].text(j, i, f'{max_pooled[i, j]:.0f}', \n",
    "                    ha='center', va='center', color='white', fontweight='bold', fontsize=14)\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(avg_pooled, cmap='viridis')\n",
    "axes[2].set_title('Average Pooling (2x2) ‚Üí 2x2', fontweight='bold', fontsize=12)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[2].text(j, i, f'{avg_pooled[i, j]:.1f}', \n",
    "                    ha='center', va='center', color='white', fontweight='bold', fontsize=14)\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Pooling Benefits:\")\n",
    "print(\"  ‚Ä¢ Reduces spatial dimensions (4x4 ‚Üí 2x2)\")\n",
    "print(\"  ‚Ä¢ Makes detection position-invariant\")\n",
    "print(\"  ‚Ä¢ Reduces computation and parameters\")\n",
    "print(\"  ‚Ä¢ Max pooling: Keeps strongest activations\")\n",
    "print(\"  ‚Ä¢ Avg pooling: Smooths features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2759193d",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Building a CNN with PyTorch\n",
    "\n",
    "### Simple CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb004bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers for 32x32 color images\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)   # 3 input channels (RGB)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Pooling\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # 2x2 pooling\n",
    "        \n",
    "        # Fully connected layers\n",
    "        # After 3 pooling layers, 32x32 -> 16x16 -> 8x8 -> 4x4\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Conv1 + ReLU + Pool (32x32 -> 16x16)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        \n",
    "        # Conv2 + ReLU + Pool (16x16 -> 8x8)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        \n",
    "        # Conv3 + ReLU + Pool (8x8 -> 4x4)\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model_cnn = SimpleCNN(num_classes=10)\n",
    "print(\"üß† Simple CNN Architecture for CIFAR-10\")\n",
    "print(\"=\"*60)\n",
    "print(model_cnn)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model_cnn.parameters())\n",
    "trainable_params = sum(p.numel() for p in model_cnn.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randn(1, 3, 32, 32)  # Batch=1, Channels=3, H=32, W=32\n",
    "output = model_cnn(test_input)\n",
    "print(f\"\\nInput shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output (logits): {output[0][:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1096c33",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Application: Visual Quality Inspection for Manufacturing\n",
    "\n",
    "Our goal is to train a CNN to classify images of manufactured parts as either \"OK\" or \"Defective\". This is a core task for our **Manufacturing Copilot's** quality control module.\n",
    "\n",
    "We'll use the **CIFAR-10 dataset** as a proxy for this task. We can imagine that some classes (e.g., 'automobile', 'truck') represent correctly manufactured parts, while others (e.g., 'airplane', 'ship' - which might look like malformed parts) represent defects. This is a simplified but illustrative example.\n",
    "\n",
    "**Dataset:** CIFAR-10 (32x32 color images, 10 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18930d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "\n",
    "# Download and load training data\n",
    "train_dataset = datasets.CIFAR10('./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10('./data', train=False, transform=transform_test)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "# Let's define our manufacturing context\n",
    "# OK parts: car, truck\n",
    "# Defective parts: plane, ship (malformed parts)\n",
    "# Ambiguous: bird, cat, deer, dog, frog, horse (other items on conveyor belt)\n",
    "\n",
    "print(\"üìä CIFAR-10 Dataset (as Manufacturing Parts)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Number of batches (train): {len(train_loader)}\")\n",
    "\n",
    "# Visualize some samples\n",
    "# Function to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "examples = enumerate(train_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i >= 10: break\n",
    "    # Unnormalize\n",
    "    img = example_data[i] / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    ax.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    ax.set_title(f'Label: {classes[example_targets[i]]}', fontweight='bold')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('CIFAR-10 Sample Images', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dabe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "        \n",
    "        if batch_idx % 200 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "    \n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Test function\n",
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "    \n",
    "    avg_loss = test_loss / len(test_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    print(f'\\nTest set: Average loss: {avg_loss:.4f}, '\n",
    "          f'Accuracy: {correct}/{total} ({accuracy:.2f}%)\\n')\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077a88e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss, and optimizer\n",
    "model = SimpleCNN(num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# A learning rate scheduler can help improve training\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "epochs = 15 # Increased epochs for better convergence\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "print(\"üîÑ Training CNN on CIFAR-10 (as Manufacturing Parts)...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, device, train_loader, optimizer, criterion, epoch)\n",
    "    test_loss, test_acc = test(model, device, test_loader, criterion)\n",
    "    \n",
    "    scheduler.step() # Update learning rate\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch} Summary: Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "print(\"\\n‚úÖ Training Complete!\")\n",
    "print(f\"Final Test Accuracy: {test_accuracies[-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb20b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss\n",
    "ax1.plot(range(1, epochs+1), train_losses, 'b-o', label='Train Loss', linewidth=2, markersize=8)\n",
    "ax1.plot(range(1, epochs+1), test_losses, 'r-s', label='Test Loss', linewidth=2, markersize=8)\n",
    "ax1.set_title('Training and Test Loss', fontweight='bold', fontsize=14)\n",
    "ax1.set_xlabel('Epoch', fontweight='bold')\n",
    "ax1.set_ylabel('Loss', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax2.plot(range(1, epochs+1), train_accuracies, 'b-o', label='Train Accuracy', linewidth=2, markersize=8)\n",
    "ax2.plot(range(1, epochs+1), test_accuracies, 'r-s', label='Test Accuracy', linewidth=2, markersize=8)\n",
    "ax2.set_title('Training and Test Accuracy', fontweight='bold', fontsize=14)\n",
    "ax2.set_xlabel('Epoch', fontweight='bold')\n",
    "ax2.set_ylabel('Accuracy (%)', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb55dafa",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Visualizing CNN Predictions on Manufacturing Parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08a4c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on test set\n",
    "model.eval()\n",
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "example_data, example_targets = example_data.to(device), example_targets.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(example_data)\n",
    "    predictions = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < 15:\n",
    "        # Unnormalize\n",
    "        img = example_data[i].cpu() / 2 + 0.5\n",
    "        npimg = img.numpy()\n",
    "        ax.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "        \n",
    "        pred_class = classes[predictions[i].item()]\n",
    "        true_class = classes[example_targets[i].item()]\n",
    "        \n",
    "        color = 'green' if pred_class == true_class else 'red'\n",
    "        ax.set_title(f'Pred: {pred_class}\\nTrue: {true_class}', color=color, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.suptitle('CNN Predictions (Green=Correct, Red=Wrong)', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Count correct predictions\n",
    "correct = (predictions.view_as(example_targets) == example_targets).sum().item()\n",
    "print(f\"\\n‚úÖ Correctly classified: {correct}/{len(example_targets)} ({100*correct/len(example_targets):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40530ced",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Visualizing Learned Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a94c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first conv layer filters\n",
    "first_layer_weights = model.conv1.weight.data.cpu()\n",
    "\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < first_layer_weights.shape[0]:\n",
    "        filter_img = first_layer_weights[i, 0]\n",
    "        ax.imshow(filter_img, cmap='gray')\n",
    "        ax.set_title(f'Filter {i+1}', fontsize=10)\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.suptitle('Learned Filters in First Convolutional Layer', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üîç These filters automatically learned to detect:\")\n",
    "print(\"   ‚Ä¢ Edges (vertical, horizontal, diagonal)\")\n",
    "print(\"   ‚Ä¢ Curves and corners\")\n",
    "print(\"   ‚Ä¢ Basic patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f4c72f",
   "metadata": {},
   "source": [
    "## üéâ Summary\n",
    "\n",
    "Congratulations! You've mastered CNNs!\n",
    "\n",
    "### Key Concepts\n",
    "- ‚úÖ Convolution operation\n",
    "- ‚úÖ Filters and feature maps\n",
    "- ‚úÖ Pooling layers (Max, Average)\n",
    "- ‚úÖ CNN architecture design\n",
    "- ‚úÖ Training CNNs with PyTorch\n",
    "- ‚úÖ Image classification\n",
    "\n",
    "### What You Built\n",
    "1. üîç Manual convolution implementation\n",
    "2. üß† Complete CNN architecture\n",
    "3. üìä MNIST digit classifier (98%+ accuracy)\n",
    "4. üé® Filter visualization\n",
    "\n",
    "### CNN Applications\n",
    "- üè≠ **Manufacturing**: Defect detection, quality inspection\n",
    "- üè• **Healthcare**: Medical image analysis, disease detection\n",
    "- üöó **Automotive**: Autonomous driving, object detection\n",
    "- üì± **Mobile**: Face recognition, AR filters\n",
    "\n",
    "### Next Steps\n",
    "Continue to **Notebook 03: RNNs and LSTMs** to learn about sequence models!\n",
    "\n",
    "<div align=\"center\">\n",
    "<b>CNNs mastered! Ready for sequences! üöÄ</b>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
