{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f0c148f",
   "metadata": {},
   "source": [
    "# üñºÔ∏è Notebook 02: Convolutional Neural Networks (CNNs)\n",
    "\n",
    "**Week 3-4: Deep Learning & NLP Foundations**  \n",
    "**Gen AI Masters Program**\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Objectives\n",
    "\n",
    "In the previous notebook, we built our first fully connected neural networks. Now, we will dive into **Convolutional Neural Networks (CNNs)**, a specialized and highly effective type of neural network designed for processing grid-like data, most notably images. Understanding CNNs is crucial as they form the basis of computer vision and are even finding applications in other domains like NLP.\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1.  **Understand the Convolution Operation**: Intuitively and mathematically grasp how filters (or kernels) slide over an image to create **feature maps**, which are representations of learned patterns.\n",
    "2.  **Implement Pooling Layers**: Learn how **max pooling** and **average pooling** are used to downsample feature maps, reducing dimensionality and creating a degree of spatial invariance.\n",
    "3.  **Design a CNN Architecture**: Understand how to assemble convolutional, pooling, and fully connected layers into a cohesive and effective CNN architecture.\n",
    "4.  **Build a CNN with PyTorch**: Construct and train a CNN for a multi-class image classification task from scratch using PyTorch's `nn.Module`.\n",
    "5.  **Apply to a Practical Problem**: Frame the training process around a practical use case: building a visual quality inspection model for a **Manufacturing Copilot**.\n",
    "6.  **Grasp the Concept of Transfer Learning**: Briefly understand the powerful principle of leveraging pre-trained models to dramatically boost performance and reduce training time.\n",
    "\n",
    "**Estimated Time:** 3-4 hours\n",
    "\n",
    "---\n",
    "\n",
    "## üìö What are CNNs?\n",
    "\n",
    "Convolutional Neural Networks (CNNs or ConvNets) are a class of deep neural networks that have become the dominant architecture for computer vision tasks. Their design is inspired by the organization of the animal visual cortex and they have revolutionized the field by achieving state-of-the-art results on a wide array of problems.\n",
    "\n",
    "### Core Applications:\n",
    "-   üì∏ **Image Classification**: Assigning a single label to an entire image (e.g., \"cat\", \"dog\", \"car\").\n",
    "-   üéØ **Object Detection**: Identifying and drawing bounding boxes around multiple objects within an image.\n",
    "-   üé® **Image Segmentation**: Classifying each pixel in an image to create a pixel-level mask for every object.\n",
    "-   üè• **Medical Imaging**: Analyzing scans like X-rays, CTs, and MRIs for tumor detection and diagnosis.\n",
    "-   üè≠ **Manufacturing Quality Inspection**: Automatically detecting defects, scratches, or anomalies in products on an assembly line (our focus for this notebook).\n",
    "-   ü§ñ **Autonomous Vehicles**: Powering the perception systems that allow self-driving cars to \"see\" and understand the world around them.\n",
    "\n",
    "### The Key Insight: Spatial Hierarchies and Parameter Sharing\n",
    "Unlike fully connected networks that treat an image as a flat vector of pixels, CNNs preserve and leverage the spatial relationships between pixels. They use shared weights in the form of **filters (or kernels)** to automatically and hierarchically learn features.\n",
    "-   A **first layer** might learn to detect simple features like edges and color gradients.\n",
    "-   A **second layer** might combine these edges to detect more complex shapes like circles, squares, or textures.\n",
    "-   **Deeper layers** might combine these shapes to detect object parts like eyes, noses, or car wheels.\n",
    "\n",
    "This hierarchical learning and parameter sharing make CNNs incredibly efficient and effective for visual data. Let's dive in and build one! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d791337",
   "metadata": {},
   "source": [
    "## üöÄ Agenda\n",
    "\n",
    "Our journey through CNNs will be structured as follows:\n",
    "\n",
    "1.  **Setting the Stage**: We'll import the necessary libraries (`torch`, `numpy`, `matplotlib`) and set up our environment.\n",
    "2.  **The Convolution Operation**: We'll start with the fundamental building block of any CNN. We will create a simple grayscale image and manually apply a convolution filter to understand how it extracts features like edges.\n",
    "3.  **Pooling Layers**: We'll explore how pooling (specifically max pooling) works to downsample feature maps, making the network more efficient and robust.\n",
    "4.  **Building a Complete CNN Architecture**: We'll define a full CNN model in PyTorch, combining convolutional, pooling, and fully connected layers.\n",
    "5.  **The Use Case: Manufacturing Quality Inspection**: We'll introduce a practical problem‚Äîclassifying images of cast iron parts as either \"defective\" or \"ok\". This provides a real-world context for our model.\n",
    "6.  **Data Loading and Preprocessing**: We'll load the dataset, apply necessary transformations (like resizing and converting to tensors), and set up `DataLoaders` for training and validation.\n",
    "7.  **Training the CNN**: We'll write the complete training loop to train our model on the casting dataset, monitoring loss and accuracy.\n",
    "8.  **Evaluation and Visualization**: We'll evaluate the model's performance on the test set and visualize its predictions to see where it succeeds and fails.\n",
    "9.  **A Glimpse into Transfer Learning**: We'll conclude with a brief discussion on transfer learning, a powerful technique for leveraging pre-trained models to achieve better results with less data and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88429173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Set up the Environment ---\n",
    "\n",
    "# Import core libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "import zipfile\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "# This ensures that any operation involving randomness (e.g., weight initialization, data shuffling)\n",
    "# will produce the same results every time the code is run.\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Set the default device\n",
    "# This checks if a CUDA-enabled GPU is available and sets it as the device.\n",
    "# If not, it falls back to the CPU. Training on a GPU is significantly faster.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"‚úÖ Using device: {device.upper()}\")\n",
    "\n",
    "# --- Plotting Style ---\n",
    "\n",
    "# Set a consistent and visually appealing style for all our plots\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "# Set a default figure size for better readability\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "# Use a color palette that is friendly to colorblind individuals\n",
    "sns.set_palette(\"colorblind\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03468c9",
   "metadata": {},
   "source": [
    "## üß† Part 1: The Convolution Operation - A Visual Intuition\n",
    "\n",
    "The **convolution** is the core operation of a CNN. It's how the network \"looks\" at an image and identifies features.\n",
    "\n",
    "Imagine sliding a small magnifying glass over a large picture. This magnifying glass is our **filter** (or **kernel**). The filter is a small matrix of weights. As it slides over the image, it performs an element-wise multiplication with the patch of the image it's currently on, and then sums up the results into a single pixel in the output image, called a **feature map** or **activation map**.\n",
    "\n",
    "This process is repeated across the entire image. The key idea is that a specific filter is designed to \"activate\" (produce a high value) when it detects a specific feature it's looking for, such as a vertical edge, a horizontal edge, a specific color, or a certain texture.\n",
    "\n",
    "Let's visualize this with a simple example. We'll create a basic grayscale image and apply a well-known edge detection filter to see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d338c4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Visualizing the Convolution Operation ---\n",
    "\n",
    "# a) Create a simple grayscale image\n",
    "# We'll create a 10x10 numpy array representing a grayscale image.\n",
    "# The image will have a sharp vertical edge in the middle:\n",
    "# - The left half will be black (pixel value 0).\n",
    "# - The right half will be white (pixel value 255).\n",
    "image = np.zeros((10, 10))\n",
    "image[:, 5:] = 255\n",
    "\n",
    "# b) Define a vertical edge detection filter (Sobel Operator)\n",
    "# This filter is designed to have high values when it detects a vertical line.\n",
    "# The positive values on the left and negative values on the right will create a\n",
    "# strong response when slid over the vertical edge in our image.\n",
    "vertical_edge_filter = np.array([\n",
    "    [1, 0, -1],\n",
    "    [2, 0, -2],\n",
    "    [1, 0, -1]\n",
    "])\n",
    "\n",
    "# c) Apply the convolution\n",
    "# We need to convert our numpy arrays to PyTorch tensors to use PyTorch's convolution functions.\n",
    "# The dimensions need to be reshaped to match what PyTorch expects:\n",
    "# [Batch Size, Channels, Height, Width]\n",
    "# - Batch Size = 1: We are processing one image.\n",
    "# - Channels = 1: It's a grayscale image (a color image would have 3 channels: R, G, B).\n",
    "# - Height = 10, Width = 10: The dimensions of our image.\n",
    "image_tensor = torch.from_numpy(image).float().unsqueeze(0).unsqueeze(0)\n",
    "filter_tensor = torch.from_numpy(vertical_edge_filter).float().unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# Use PyTorch's functional API for convolution.\n",
    "# `nn.functional.conv2d` takes the image, the filter, and optional parameters.\n",
    "# - `padding=1`: We add a 1-pixel border around the image. This allows the 3x3 filter\n",
    "#   to be centered on the edge pixels of the original image, preserving the output size.\n",
    "feature_map_tensor = nn.functional.conv2d(image_tensor, filter_tensor, padding=1)\n",
    "\n",
    "# Convert the output tensor back to a numpy array for visualization.\n",
    "# `.squeeze()` removes the batch and channel dimensions.\n",
    "feature_map = feature_map_tensor.squeeze().detach().numpy()\n",
    "\n",
    "\n",
    "# d) Visualize the results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot the original image\n",
    "sns.heatmap(image, ax=axes[0], cmap='gray', cbar=False, annot=True, fmt=\".0f\", linewidths=.5, linecolor='red')\n",
    "axes[0].set_title(\"Original Image (10x10)\")\n",
    "axes[0].set_aspect('equal')\n",
    "\n",
    "# Plot the filter\n",
    "sns.heatmap(vertical_edge_filter, ax=axes[1], cmap='coolwarm', cbar=False, annot=True, fmt=\".0f\", linewidths=.5)\n",
    "axes[1].set_title(\"Vertical Edge Filter (3x3)\")\n",
    "axes[1].set_aspect('equal')\n",
    "\n",
    "# Plot the resulting feature map\n",
    "sns.heatmap(feature_map, ax=axes[2], cmap='gray', cbar=False, annot=True, fmt=\".0f\", linewidths=.5, linecolor='blue')\n",
    "axes[2].set_title(\"Feature Map (10x10)\")\n",
    "axes[2].set_aspect('equal')\n",
    "\n",
    "plt.suptitle(\"Visualizing the 2D Convolution Operation\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Analysis:\")\n",
    "print(\"Notice how the feature map has high positive values exactly where the vertical edge was.\")\n",
    "print(\"The filter successfully 'detected' the feature it was designed to find.\")\n",
    "print(\"In a real CNN, the network *learns* the values of these filters on its own during training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7079c0d",
   "metadata": {},
   "source": [
    "## üß† Part 2: Pooling Layers - Downsampling for Efficiency\n",
    "\n",
    "After a convolution layer produces a feature map, it's common to use a **pooling layer** to downsample it. Pooling reduces the spatial dimensions (height and width) of the feature map, which has several key benefits:\n",
    "\n",
    "1.  **Reduces Computational Load**: Smaller feature maps mean fewer parameters and computations in subsequent layers, making the network faster and more memory-efficient.\n",
    "2.  **Increases Receptive Field**: Each pixel in a pooled feature map corresponds to a larger area in the original image, allowing subsequent layers to learn more global patterns.\n",
    "3.  **Provides Spatial Invariance**: By summarizing a neighborhood of features into a single value (e.g., the maximum), pooling makes the network slightly more robust to small translations or distortions of the feature in the input image.\n",
    "\n",
    "The most common type of pooling is **Max Pooling**. It works by sliding a window over the feature map and, for each window, taking only the maximum value.\n",
    "\n",
    "Let's visualize how max pooling works on the feature map we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689c0320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Visualizing the Max Pooling Operation ---\n",
    "\n",
    "# a) Define the Max Pooling layer\n",
    "# We'll use a 2x2 window and a stride of 2.\n",
    "# - `kernel_size=2`: The pooling window will be 2x2.\n",
    "# - `stride=2`: The window will move 2 pixels at a time (no overlap).\n",
    "# This setup will effectively halve the height and width of the feature map.\n",
    "max_pool_layer = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "# b) Apply Max Pooling to our feature map\n",
    "# The input tensor needs to be in the [Batch, Channel, Height, Width] format.\n",
    "# We already have `feature_map_tensor` from the previous step.\n",
    "pooled_map_tensor = max_pool_layer(feature_map_tensor)\n",
    "\n",
    "# Convert the output back to a numpy array for visualization\n",
    "pooled_map = pooled_map_tensor.squeeze().detach().numpy()\n",
    "\n",
    "\n",
    "# c) Visualize the results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot the feature map before pooling\n",
    "sns.heatmap(feature_map, ax=axes[0], cmap='gray', cbar=False, annot=True, fmt=\".0f\", linewidths=.5, linecolor='blue')\n",
    "axes[0].set_title(f\"Feature Map Before Pooling ({feature_map.shape[0]}x{feature_map.shape[1]})\")\n",
    "axes[0].set_aspect('equal')\n",
    "\n",
    "# Highlight the 2x2 pooling regions\n",
    "for i in range(0, feature_map.shape[0], 2):\n",
    "    for j in range(0, feature_map.shape[1], 2):\n",
    "        axes[0].add_patch(plt.Rectangle((j, i), 2, 2, fill=False, edgecolor='red', lw=2))\n",
    "\n",
    "\n",
    "# Plot the feature map after pooling\n",
    "sns.heatmap(pooled_map, ax=axes[1], cmap='gray', cbar=False, annot=True, fmt=\".0f\", linewidths=.5, linecolor='green')\n",
    "axes[1].set_title(f\"Feature Map After 2x2 Max Pooling ({pooled_map.shape[0]}x{pooled_map.shape[1]})\")\n",
    "axes[1].set_aspect('equal')\n",
    "\n",
    "\n",
    "plt.suptitle(\"Visualizing the Max Pooling Operation\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Analysis:\")\n",
    "print(f\"The original {feature_map.shape} feature map has been downsampled to {pooled_map.shape}.\")\n",
    "print(\"For each 2x2 red square in the original map, only the maximum value is kept.\")\n",
    "print(\"Notice that the strong signal from the detected edge is preserved, while the less important zero-valued regions are condensed.\")\n",
    "print(\"This makes the representation more compact and efficient.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2759193d",
   "metadata": {},
   "source": [
    "## üß† Part 3: Building a Complete CNN Architecture\n",
    "\n",
    "Now that we understand the two main components‚Äî**convolutional layers** and **pooling layers**‚Äîwe can assemble them into a complete CNN.\n",
    "\n",
    "A typical CNN architecture for image classification follows a standard pattern:\n",
    "\n",
    "1.  **Input Layer**: An image with dimensions `[Channels, Height, Width]`. For example, a 224x224 color image would be `[3, 224, 224]`.\n",
    "\n",
    "2.  **Convolutional Blocks**: A sequence of one or more blocks, each containing:\n",
    "    *   A **Convolutional Layer (`nn.Conv2d`)**: Applies a set of learnable filters to the input, creating a stack of feature maps. This layer increases the number of channels (depth) as it learns more features.\n",
    "    *   An **Activation Function (`nn.ReLU`)**: Introduces non-linearity, allowing the network to learn complex patterns.\n",
    "    *   A **Pooling Layer (`nn.MaxPool2d`)**: Downsamples the feature maps, reducing their height and width.\n",
    "\n",
    "3.  **Flattening**: After the last convolutional block, the 3D feature maps `[Channels, Height, Width]` are \"flattened\" into a single long 1D vector. This prepares the data for the final classification stage.\n",
    "\n",
    "4.  **Fully Connected (FC) Layers**: A series of one or more `nn.Linear` layers, just like in the neural networks we built previously. These layers perform the final classification based on the high-level features extracted by the convolutional blocks.\n",
    "\n",
    "5.  **Output Layer**: The final `nn.Linear` layer that produces the output scores (logits) for each class. For a binary classification problem, this will have 1 output neuron; for a 10-class problem, it will have 10.\n",
    "\n",
    "Let's define a simple CNN model in PyTorch that follows this structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb004bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Defining the CNN Model in PyTorch ---\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Convolutional Neural Network for image classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=1):\n",
    "        \"\"\"\n",
    "        Initializes the layers of the CNN.\n",
    "        \n",
    "        Args:\n",
    "            num_classes (int): The number of output classes. For binary classification, this is 1.\n",
    "        \"\"\"\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # --- Convolutional Feature Extractor ---\n",
    "        # This part of the network is responsible for learning and extracting features from the image.\n",
    "        self.features = nn.Sequential(\n",
    "            # --- Block 1 ---\n",
    "            # Input: [Batch, 3, 128, 128] (assuming 3-channel color images of size 128x128)\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            # -> Output: [Batch, 16, 128, 128]\n",
    "            # `in_channels=3`: For RGB images.\n",
    "            # `out_channels=16`: We are learning 16 different features.\n",
    "            # `kernel_size=3`: A 3x3 filter.\n",
    "            # `padding=1`: Preserves the height and width (128x128).\n",
    "            \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # -> Output: [Batch, 16, 64, 64] (Height and width are halved)\n",
    "\n",
    "            # --- Block 2 ---\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            # -> Output: [Batch, 32, 64, 64]\n",
    "            # `in_channels=16`: Must match the `out_channels` of the previous Conv layer.\n",
    "            # `out_channels=32`: We learn 32 more complex features.\n",
    "            \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # -> Output: [Batch, 32, 32, 32]\n",
    "        )\n",
    "        \n",
    "        # --- Fully Connected Classifier ---\n",
    "        # This part of the network takes the extracted features and performs classification.\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # -> Output: [Batch, 32 * 32 * 32] = [Batch, 32768]\n",
    "            # Flattens the 3D feature map into a 1D vector.\n",
    "            \n",
    "            nn.Linear(in_features=32 * 32 * 32, out_features=128),\n",
    "            # `in_features` must match the size of the flattened vector.\n",
    "            # `out_features=128`: A hidden layer with 128 neurons.\n",
    "            \n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(in_features=128, out_features=num_classes)\n",
    "            # The final output layer. For binary classification, `num_classes` is 1.\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the network.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): The input batch of images.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: The output logits from the classifier.\n",
    "        \"\"\"\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Let's create a dummy input tensor to trace the shape changes through the network.\n",
    "dummy_input = torch.randn(1, 3, 128, 128) # (Batch, Channels, Height, Width)\n",
    "model_test = SimpleCNN()\n",
    "\n",
    "# Pass the dummy input through the feature extractor\n",
    "features_output = model_test.features(dummy_input)\n",
    "# Pass the features through the classifier\n",
    "classifier_output = model_test.classifier(features_output)\n",
    "\n",
    "print(\"--- Tracing Tensor Shapes ---\")\n",
    "print(f\"Dummy Input Shape:      {dummy_input.shape}\")\n",
    "print(f\"After Feature Extractor: {features_output.shape}\")\n",
    "print(f\"After Classifier:       {classifier_output.shape}\")\n",
    "print(\"\\n‚úÖ Model architecture seems correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1096c33",
   "metadata": {},
   "source": [
    "## üè≠ Part 4: Use Case - Manufacturing Quality Inspection\n",
    "\n",
    "Now, let's apply our CNN to a practical problem. We'll build a **Visual Quality Inspection system** for a manufacturing process.\n",
    "\n",
    "**The Scenario**: Imagine a factory producing cast iron parts. Some parts come off the assembly line with defects (e.g., cracks, blowholes), while others are perfectly fine. Manually inspecting every single part is slow, expensive, and prone to human error.\n",
    "\n",
    "**The Goal**: We will train our CNN to automatically classify images of these parts as either **\"defective\"** or **\"ok\"**. This is a classic binary image classification task.\n",
    "\n",
    "**The Dataset**: We will use the \"Casting Product Image Data for Quality Inspection\" dataset. It contains thousands of images of cast iron products, neatly organized into `train` and `test` folders, and further subdivided into `def_front` (defective) and `ok_front` (ok) classes.\n",
    "\n",
    "First, let's download and extract the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18930d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Data Loading and Preprocessing ---\n",
    "\n",
    "# a) Download and Unzip the Dataset\n",
    "# The dataset is hosted as a zip file. We'll use Python's `urllib` to download it\n",
    "# and `zipfile` to extract its contents.\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "# URL of the dataset\n",
    "url = \"https://www.dropbox.com/s/kfx3y83zyp5c6w5/casting_data.zip?dl=1\"\n",
    "zip_path = \"casting_data.zip\"\n",
    "extract_path = \"casting_data\"\n",
    "\n",
    "# --- Download the file with a progress bar ---\n",
    "# This provides a better user experience for large downloads.\n",
    "class TqdmUpTo(tqdm):\n",
    "    \"\"\"Provides `update_to(block_num, block_size, total_size)`.\"\"\"\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "\n",
    "if not os.path.exists(extract_path):\n",
    "    print(f\"Downloading dataset from {url}...\")\n",
    "    with TqdmUpTo(unit='B', unit_scale=True, unit_divisor=1024, miniters=1,\n",
    "                  desc=zip_path) as t:\n",
    "        urllib.request.urlretrieve(url, filename=zip_path, reporthook=t.update_to)\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "    # --- Unzip the file ---\n",
    "    print(f\"Extracting {zip_path} to {extract_path}...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    print(\"Extraction complete.\")\n",
    "    \n",
    "    # --- Clean up the zip file ---\n",
    "    os.remove(zip_path)\n",
    "    print(f\"Removed {zip_path}.\")\n",
    "else:\n",
    "    print(f\"Dataset already exists at '{extract_path}'. Skipping download and extraction.\")\n",
    "\n",
    "# Define the paths to the training and testing directories\n",
    "data_dir = os.path.join(extract_path, 'casting_data')\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "test_dir = os.path.join(data_dir, 'test')\n",
    "\n",
    "print(f\"\\nTrain directory: {train_dir}\")\n",
    "print(f\"Test directory:  {test_dir}\")\n",
    "\n",
    "# Let's check the contents of the training directory\n",
    "print(\"\\nClasses in training directory:\", os.listdir(train_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dabe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Define Image Transformations\n",
    "# Before feeding images into our network, we must perform several preprocessing steps.\n",
    "# We use `torchvision.transforms` to create a pipeline for these operations.\n",
    "\n",
    "# `T.Compose` chains multiple transformations together.\n",
    "# The transformations are applied in the order they are listed.\n",
    "data_transforms = T.Compose([\n",
    "    # 1. Resize the image to a fixed size (e.g., 128x128).\n",
    "    # CNNs require inputs to have a consistent size.\n",
    "    T.Resize((128, 128)),\n",
    "    \n",
    "    # 2. Convert the PIL Image to a PyTorch Tensor.\n",
    "    # This also scales the pixel values from the range [0, 255] to [0.0, 1.0].\n",
    "    T.ToTensor(),\n",
    "    \n",
    "    # 3. Normalize the tensor.\n",
    "    # This standardizes the pixel values to have a mean of 0.5 and a standard deviation of 0.5\n",
    "    # for each channel. The formula is: output = (input - mean) / std.\n",
    "    # So, our pixel values in the range [0, 1] will be mapped to [-1, 1].\n",
    "    # Normalization helps the model train faster and more stably.\n",
    "    # The values (0.5, 0.5, 0.5) are the means and stds for the R, G, B channels respectively.\n",
    "    T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "\n",
    "# c) Create a Custom PyTorch Dataset\n",
    "class CastingImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for loading the casting product images.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images, structured with subdirectories for each class.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Define the mapping from class name to integer label\n",
    "        self.class_to_int = {\"def_front\": 1, \"ok_front\": 0}\n",
    "        \n",
    "        # Walk through the directory to find all image files and their labels\n",
    "        for class_name in self.class_to_int.keys():\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                if img_name.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    self.image_paths.append(os.path.join(class_dir, img_name))\n",
    "                    self.labels.append(self.class_to_int[class_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetches the sample at the given index.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): The index of the sample to fetch.\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (image, label) where image is the transformed image tensor\n",
    "                   and label is the integer label.\n",
    "        \"\"\"\n",
    "        # Load the image from its path\n",
    "        img_path = self.image_paths[idx]\n",
    "        # `Image.open` loads the image. `.convert(\"RGB\")` ensures it has 3 channels,\n",
    "        # even if it's grayscale, which is important for our 3-channel input CNN.\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Get the corresponding label\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Apply transformations if they exist\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        # Return the image tensor and its label as a tensor\n",
    "        return image, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "# d) Create Dataset and DataLoader instances\n",
    "# We create one dataset for the training data and one for the testing data.\n",
    "train_dataset = CastingImageDataset(root_dir=train_dir, transform=data_transforms)\n",
    "test_dataset = CastingImageDataset(root_dir=test_dir, transform=data_transforms)\n",
    "\n",
    "# `DataLoader` takes a Dataset and wraps it in an iterable for easy batching,\n",
    "# shuffling, and parallel data loading.\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"‚úÖ Found {len(train_dataset)} images in the training set.\")\n",
    "print(f\"‚úÖ Found {len(test_dataset)} images in the test set.\")\n",
    "print(f\"‚úÖ Created DataLoader with batch size {BATCH_SIZE}.\")\n",
    "\n",
    "# Let's inspect one batch to see the shape\n",
    "images, labels = next(iter(train_loader))\n",
    "print(f\"\\nShape of one batch of images: {images.shape}\") # [Batch Size, Channels, Height, Width]\n",
    "print(f\"Shape of one batch of labels: {labels.shape}\")   # [Batch Size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077a88e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Training the CNN ---\n",
    "\n",
    "# a) Initialize the Model, Loss Function, and Optimizer\n",
    "\n",
    "# Instantiate the CNN model\n",
    "# `num_classes=1` because we are doing binary classification (defective vs. ok).\n",
    "model = SimpleCNN(num_classes=1).to(device)\n",
    "\n",
    "# Define the Loss Function\n",
    "# `BCEWithLogitsLoss` is perfect for binary classification.\n",
    "# It combines a Sigmoid layer and the Binary Cross Entropy loss in one single class.\n",
    "# This combination is more numerically stable than using a plain Sigmoid followed by a BCELoss.\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Define the Optimizer\n",
    "# `Adam` is a popular and effective optimization algorithm.\n",
    "# We pass the model's parameters to it, which are the weights and biases it will update.\n",
    "# `lr=0.001` is the learning rate, which controls the step size during optimization.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"--- Ready for Training ---\")\n",
    "print(f\"Model:      {model.__class__.__name__}\")\n",
    "print(f\"Device:     {device.upper()}\")\n",
    "print(f\"Loss Fn:    {criterion.__class__.__name__}\")\n",
    "print(f\"Optimizer:  {optimizer.__class__.__name__}\")\n",
    "\n",
    "# b) Define the Training and Validation Functions\n",
    "def train_one_epoch(model, data_loader, criterion, optimizer, device):\n",
    "    \"\"\"Performs one full pass over the training data.\"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Use tqdm for a progress bar\n",
    "    for images, labels in tqdm(data_loader, desc=\"Training\"):\n",
    "        # Move data to the configured device (GPU or CPU)\n",
    "        images, labels = images.to(device), labels.to(device).unsqueeze(1)\n",
    "        \n",
    "        # 1. Forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # 2. Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 3. Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        optimizer.zero_grad() # Clear previous gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # 4. Perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # --- Update statistics ---\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        # Convert outputs to probabilities and then to predicted classes (0 or 1)\n",
    "        preds = torch.sigmoid(outputs) > 0.5\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "    epoch_loss = total_loss / total_samples\n",
    "    epoch_acc = correct_predictions / total_samples\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_one_epoch(model, data_loader, criterion, device):\n",
    "    \"\"\"Evaluates the model on the validation data.\"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for efficiency\n",
    "        for images, labels in tqdm(data_loader, desc=\"Validating\"):\n",
    "            images, labels = images.to(device), labels.to(device).unsqueeze(1)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            preds = torch.sigmoid(outputs) > 0.5\n",
    "            correct_predictions += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            \n",
    "    epoch_loss = total_loss / total_samples\n",
    "    epoch_acc = correct_predictions / total_samples\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb20b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) Run the Training Loop\n",
    "\n",
    "NUM_EPOCHS = 5  # Set the number of epochs for a quick training run\n",
    "\n",
    "# Store history for plotting later\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "print(f\"üöÄ Starting training for {NUM_EPOCHS} epochs...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n--- Epoch {epoch + 1}/{NUM_EPOCHS} ---\")\n",
    "    \n",
    "    # Train for one epoch\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    \n",
    "    # Validate for one epoch\n",
    "    val_loss, val_acc = validate_one_epoch(model, test_loader, criterion, device)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb55dafa",
   "metadata": {},
   "source": [
    "## üìä Part 5: Evaluation and Visualization\n",
    "\n",
    "After training, it's crucial to evaluate the model's performance visually. Plotting the training and validation loss and accuracy over epochs can give us valuable insights into the training process, helping us spot issues like overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08a4c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. Plotting Training History ---\n",
    "\n",
    "# Create a figure with two subplots: one for loss, one for accuracy\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# --- Plot Loss ---\n",
    "ax1.plot(history['train_loss'], label='Train Loss', color='blue', marker='o')\n",
    "ax1.plot(history['val_loss'], label='Validation Loss', color='orange', marker='o')\n",
    "ax1.set_title('Loss vs. Epochs', fontsize=16)\n",
    "ax1.set_xlabel('Epochs', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# --- Plot Accuracy ---\n",
    "ax2.plot(history['train_acc'], label='Train Accuracy', color='blue', marker='o')\n",
    "ax2.plot(history['val_acc'], label='Validation Accuracy', color='orange', marker='o')\n",
    "ax2.set_title('Accuracy vs. Epochs', fontsize=16)\n",
    "ax2.set_xlabel('Epochs', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.suptitle('Training and Validation Metrics', fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Analysis of the Plots:\")\n",
    "print(\"1. Loss Plot: We want to see both training and validation loss decreasing. If the validation loss starts to increase while training loss continues to decrease, it's a sign of overfitting.\")\n",
    "print(\"2. Accuracy Plot: We want to see both training and validation accuracy increasing. A large gap between the two may also indicate overfitting.\")\n",
    "print(\"Based on these plots, our model appears to be learning well, with both loss decreasing and accuracy increasing. The gap between training and validation is small, suggesting overfitting is not a major issue yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40530ced",
   "metadata": {},
   "source": [
    "### Visualizing Predictions\n",
    "\n",
    "A great way to understand a classification model's performance is to look at the images it gets right and the ones it gets wrong. This can reveal patterns in the model's mistakes and give us ideas for how to improve it.\n",
    "\n",
    "Let's write a function to fetch a batch of images from the test set, run them through our trained model, and display the images with their true labels and the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a94c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8. Visualizing Model Predictions ---\n",
    "\n",
    "def visualize_predictions(model, data_loader, device, num_images=16):\n",
    "    \"\"\"\n",
    "    Displays a grid of images from a data loader, with their predicted and true labels.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    images, labels = next(iter(data_loader))\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        preds = torch.sigmoid(outputs) > 0.5\n",
    "\n",
    "    # Move data back to CPU for plotting\n",
    "    images = images.cpu()\n",
    "    preds = preds.cpu().squeeze()\n",
    "    labels = labels.cpu()\n",
    "\n",
    "    # Inverse transform for visualization\n",
    "    # We need to un-normalize the images to display them correctly.\n",
    "    inv_normalize = T.Normalize(\n",
    "        mean=[-0.5/0.5, -0.5/0.5, -0.5/0.5],\n",
    "        std=[1/0.5, 1/0.5, 1/0.5]\n",
    "    )\n",
    "\n",
    "    # Class mapping for labels\n",
    "    class_map = {0: \"OK\", 1: \"Defective\"}\n",
    "\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    for i in range(num_images):\n",
    "        ax = plt.subplot(4, 4, i + 1)\n",
    "        \n",
    "        # Un-normalize and display the image\n",
    "        img = inv_normalize(images[i])\n",
    "        img = img.permute(1, 2, 0) # Change from [C, H, W] to [H, W, C] for matplotlib\n",
    "        plt.imshow(img)\n",
    "        \n",
    "        true_label = class_map[labels[i].item()]\n",
    "        pred_label = class_map[preds[i].item()]\n",
    "        \n",
    "        # Set title color based on correctness\n",
    "        title_color = 'green' if pred_label == true_label else 'red'\n",
    "        \n",
    "        plt.title(f\"True: {true_label}\\nPred: {pred_label}\", color=title_color)\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize predictions on a batch from the test set\n",
    "visualize_predictions(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f4c72f",
   "metadata": {},
   "source": [
    "## üåü Part 6: A Glimpse into Transfer Learning\n",
    "\n",
    "We've built and trained a pretty effective CNN from scratch! Our model achieved impressive accuracy after just a few epochs. However, in many real-world scenarios, we don't have the massive datasets (or the computational budget) required to train a very deep network from zero.\n",
    "\n",
    "This is where **Transfer Learning** comes in.\n",
    "\n",
    "**The Core Idea**: Instead of starting with random weights, we start with a model that has already been trained on a huge, general-purpose dataset (like ImageNet, which has over 14 million images across 20,000 categories). The features learned by this \"pre-trained\" model‚Äîlike edges, textures, shapes, and object parts‚Äîare often useful for other computer vision tasks.\n",
    "\n",
    "### The Typical Transfer Learning Workflow:\n",
    "\n",
    "1.  **Load a Pre-trained Model**: PyTorch's `torchvision.models` library provides easy access to famous, powerful architectures like **ResNet**, **VGG**, **EfficientNet**, and **MobileNet**.\n",
    "\n",
    "2.  **Freeze the Feature Extractor**: The early convolutional layers of these models are excellent general-purpose feature extractors. We \"freeze\" their weights, meaning we tell PyTorch not to update them during training. This saves a massive amount of computation and prevents the model from \"forgetting\" the valuable features it has already learned.\n",
    "\n",
    "3.  **Replace the Classifier Head**: The final fully connected layers of the pre-trained model are specific to its original task (e.g., classifying 1000 ImageNet classes). We chop off this \"head\" and replace it with a new, smaller classifier that is tailored to our specific problem (e.g., our binary \"defective\" vs. \"ok\" classification).\n",
    "\n",
    "4.  **Fine-Tune the New Classifier**: We then train the model on our smaller, specific dataset. Since only the weights of our new, small classifier head are being updated, this training process is much faster and requires far less data to achieve high performance.\n",
    "\n",
    "### Why is this so powerful?\n",
    "\n",
    "-   **Higher Performance**: Pre-trained models provide a much better starting point, often leading to higher final accuracy than a model trained from scratch.\n",
    "-   **Less Data Required**: Because the model already understands the basic visual world, it can learn a new task with a much smaller number of examples.\n",
    "-   **Faster Training**: We are only training a small part of the network, which dramatically reduces training time.\n",
    "\n",
    "Transfer learning is the standard approach for most computer vision applications today and is a fundamental technique in the toolkit of any deep learning practitioner. While we won't implement it here, it's the logical next step for improving our casting quality inspection system.\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You have successfully built, trained, and evaluated your first Convolutional Neural Network. You've learned the theory behind convolutions and pooling, assembled a complete architecture in PyTorch, and applied it to a real-world problem. You also now understand the powerful concept of transfer learning.\n",
    "\n",
    "You are now well-equipped to tackle a wide range of computer vision challenges!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
