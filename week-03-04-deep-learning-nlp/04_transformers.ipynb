{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c605c023",
   "metadata": {},
   "source": [
    "# ‚ö° Notebook 04: Transformers & Self-Attention\n",
    "\n",
    "**Week 3-4: Deep Learning & NLP Foundations**  \n",
    "**Gen AI Masters Program**\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Objectives\n",
    "\n",
    "By the end of this notebook, you will master:\n",
    "1. ‚úÖ Transformer architecture and encoder-decoder pipeline\n",
    "2. ‚úÖ Scaled dot-product self-attention\n",
    "3. ‚úÖ Multi-head attention and positional encoding\n",
    "4. ‚úÖ Building transformer blocks from scratch in PyTorch\n",
    "5. ‚úÖ Applying transformers to manufacturing maintenance logs\n",
    "6. ‚úÖ Visualizing attention weights and interpreting model focus\n",
    "\n",
    "**Estimated Time:** 4-5 hours\n",
    "\n",
    "---\n",
    "\n",
    "## üåü Why Transformers?\n",
    "\n",
    "RNNs struggle with long-range dependencies and parallelization. Transformers solve this with **self-attention**, enabling:\n",
    "- ‚ö° Parallel processing of entire sequences\n",
    "- üß† Global context capture\n",
    "- üèÜ State-of-the-art performance in NLP, CV, Speech\n",
    "- üè≠ Enhanced understanding of manufacturing logs and procedures\n",
    "\n",
    "Let's dive deep into the architecture that powers modern Generative AI! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc90d548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "# Visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'‚úÖ Using device: {device}')\n",
    "print(f'PyTorch version: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c055ed77",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Limitations of RNN-based Seq2Seq\n",
    "\n",
    "Traditional sequence-to-sequence models rely on **encoder-decoder RNNs**:\n",
    "- Encoder compresses entire input into a single vector\n",
    "- Decoder generates outputs step-by-step\n",
    "\n",
    "**Problems:**\n",
    "- ‚ùå Bottleneck: All information squeezed into one vector\n",
    "- ‚ùå Sequential computation: Hard to parallelize\n",
    "- ‚ùå Long-range dependencies: Context forgotten\n",
    "\n",
    "Transformers replace recurrence with **attention** to overcome these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81681e4",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Scaled Dot-Product Attention\n",
    "\n",
    "Given query $Q$, key $K$, and value $V$ matrices:\n",
    "$$\text{Attention}(Q, K, V) = \text{softmax}eft(\frac{QK^T}{qrt{d_k}}\n",
    "ight) V$$\n",
    "\n",
    "- $d_k$: dimension of keys\n",
    "- Scaling prevents extremely large dot products\n",
    "- Softmax generates attention weights\n",
    "\n",
    "We'll implement it step-by-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf02957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query: torch.Tensor,\n",
    "                                    key: torch.Tensor,\n",
    "                                    value: torch.Tensor,\n",
    "                                    mask: torch.Tensor = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc131bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention weights\n",
    "plt.figure(figsize=(6, 4))\n",
    "weights_matrix = attn_weights.squeeze().detach().numpy()\n",
    "sns.heatmap(weights_matrix, annot=True, cmap='Blues', cbar=True)\n",
    "plt.title('Attention Weight Distribution', fontweight='bold')\n",
    "plt.xlabel('Key Positions')\n",
    "plt.ylabel('Query Positions')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b60446c",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Multi-Head Attention\n",
    "\n",
    "Instead of a single attention function, Transformers use **multiple heads** to capture diverse relationships.\n",
    "\n",
    "### Steps:\n",
    "1. Project $Q, K, V$ into multiple subspaces.\n",
    "2. Apply attention in each head independently.\n",
    "3. Concatenate and project back.\n",
    "\n",
    "$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h) W^O$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d4aca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Linear projections for Q, K, V\n",
    "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None, need_weights=False):\n",
    "        batch_size, seq_length, embed_dim = query.size()\n",
    "\n",
    "        # Linear projections\n",
    "        Q = self.q_linear(query)\n",
    "        K = self.k_linear(key)\n",
    "        V = self.v_linear(value)\n",
    "\n",
    "        # Split into heads: (batch, num_heads, seq_len, head_dim)\n",
    "        Q = Q.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Compute attention per head\n",
    "        attn_output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # Concatenate heads\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_length, embed_dim)\n",
    "\n",
    "        # Final linear projection\n",
    "        output = self.out_proj(attn_output)\n",
    "\n",
    "        if need_weights:\n",
    "            # Average attention weights across heads\n",
    "            attn_weights = attn_weights.mean(dim=1)\n",
    "            return output, attn_weights\n",
    "        return output, None\n",
    "\n",
    "\n",
    "# Test multi-head attention\n",
    "test_embed = torch.randn(2, 6, 32)  # (batch=2, seq=6, embed=32)\n",
    "mha = MultiHeadAttention(embed_dim=32, num_heads=4)\n",
    "out, weights = mha(test_embed, test_embed, test_embed, need_weights=True)\n",
    "\n",
    "print(\n",
    ")\n",
    "print(\n",
    "*60)\n",
    "print(f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29f9ba3",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Positional Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e9a219",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * -(math.log(10000.0) / embed_dim))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, embed_dim)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (batch_size, seq_len, embed_dim)\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len, :]\n",
    "        return x\n",
    "\n",
    "\n",
    "# Visualize positional encodings\n",
    "pos_encoder = PositionalEncoding(embed_dim=32, max_len=100)\n",
    "PE = pos_encoder.pe.squeeze().detach().numpy()[:100, :16]  # First 100 positions, 16 dims\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(PE, aspect='auto', cmap='coolwarm')\n",
    "plt.colorbar(label='Encoding Value')\n",
    "plt.title('Sinusoidal Positional Encoding (Dim 0-15)', fontweight='bold')\n",
    "plt.xlabel('Embedding Dimension')\n",
    "plt.ylabel('Position Index')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e154f75f",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Transformer Encoder Block\n",
    "\n",
    "A single encoder layer consists of:\n",
    "1. Multi-head self-attention + residual + layer norm\n",
    "2. Position-wise feed-forward network + residual + layer norm\n",
    "\n",
    "We'll build it from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b111027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, ff_hidden_dim: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_hidden_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output, attn_weights = self.self_attn(x, x, x, mask=mask, need_weights=True)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        ff_output = self.ff(x)\n",
    "        x = x + self.dropout(ff_output)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x, attn_weights\n",
    "\n",
    "\n",
    "# Test block\n",
    "block = TransformerEncoderBlock(embed_dim=64, num_heads=8, ff_hidden_dim=256)\n",
    "sample_input = torch.randn(4, 10, 64)\n",
    "out, attn = block(sample_input)\n",
    "\n",
    "print(\n",
    ")\n",
    "print(\n",
    "*60)\n",
    "print(f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ef4629",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Manufacturing Use Case: Maintenance Log Classification\n",
    "\n",
    "We'll classify maintenance log entries into severity levels:\n",
    "- üü¢ **Normal**\n",
    "- üü† **Warning**\n",
    "- üî¥ **Critical**\n",
    "\n",
    "### Pipeline\n",
    "1. Create synthetic maintenance sentences\n",
    "2. Tokenize and build vocabulary\n",
    "3. Encode sequences + positional encodings\n",
    "4. Train transformer encoder classifier\n",
    "5. Visualize attention to interpret model focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77b8a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic manufacturing maintenance log dataset\n",
    "NORMAL_SENTENCES = [\n",
    "    \n",
    ",\n",
    "    \n",
    ",\n",
    "    \n",
    ",\n",
    "    \n",
    ",\n",
    "    \n",
    ",\n",
    "    \n",
    ",\n",
    "    \n",
    ",\n",
    "    \n",
    ",\n",
    "    \n",
    ",\n",
    "    \n",
    "\n",
    "]\n",
    "\n",
    "WARNING_SENTENCES = [\n",
    "    \n",
    ",\n",
    "    \n",
    ",\n",
    "    \n",
    ",\n",
    "    \n",
    ",\n",
    "    \n",
    ",\n",
    "    \n",
    ",\n",
    "    \n",
    ",\n",
    "    \n",
    ",\n",
    "    \n",
    ",\n",
    "    \n",
    "\n",
    "]\n",
    "\n",
    "CRITICAL_SENTENCES = [\n",
    "    \n",
    ",\n",
    "    \n",
    ",\n",
    "    \n",
    ",\n",
    "    \n",
    ",\n",
    "    \n",
    ",\n",
    "    \n",
    ",\n",
    "    \n",
    ",\n",
    "    \n",
    ",\n",
    "    \n",
    ",\n",
    "    \n",
    "\n",
    "]\n",
    "\n",
    "labels_map = {\n",
    ": 0, \n",
    ": 1, \n",
    ": 2}\n",
    "\n",
    "all_sentences = NORMAL_SENTENCES + WARNING_SENTENCES + CRITICAL_SENTENCES\n",
    "all_labels = ([labels_map['normal']] * len(NORMAL_SENTENCES) +\n",
    "              [labels_map['warning']] * len(WARNING_SENTENCES) +\n",
    "              [labels_map['critical']] * len(CRITICAL_SENTENCES))\n",
    "\n",
    "# Create DataFrame for analysis\n",
    "df_logs = pd.DataFrame({\n",
    "    'sentence': all_sentences,\n",
    "    'label': all_labels\n",
    "})\n",
    "\n",
    "df_logs['label_name'] = df_logs['label'].map({v: k.title() for k, v in labels_map.items()})\n",
    "print(df_logs.sample(6, random_state=42))\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(data=df_logs, x='label_name', palette='viridis')\n",
    "plt.title('Class Distribution of Maintenance Logs', fontweight='bold')\n",
    "plt.xlabel('Severity Level')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678d4dbd",
   "metadata": {},
   "source": [
    "### Tokenization & Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef103bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> List[str]:\n",
    "    tokens = text.lower().replace('-', ' ').split()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Build vocabulary\n",
    "all_tokens = [token for sentence in all_sentences for token in tokenize(sentence)]\n",
    "vocab_counter = Counter(all_tokens)\n",
    "\n",
    "# Reserve special tokens\n",
    "PAD_TOKEN = '<pad>'\n",
    "UNK_TOKEN = '<unk>'\n",
    "CLS_TOKEN = '<cls>'\n",
    "\n",
    "# Build vocab dictionary\n",
    "vocab = {PAD_TOKEN: 0, UNK_TOKEN: 1, CLS_TOKEN: 2}\n",
    "for token, freq in vocab_counter.most_common():\n",
    "    vocab[token] = len(vocab)\n",
    "\n",
    "inv_vocab = {idx: token for token, idx in vocab.items()}\n",
    "\n",
    "print(f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c52cbf4",
   "metadata": {},
   "source": [
    "### PyTorch Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46c19ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaintenanceLogDataset(Dataset):\n",
    "    def __init__(self, encoded_sentences: np.ndarray, labels: List[int]):\n",
    "        self.encoded_sentences = torch.tensor(encoded_sentences, dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoded_sentences[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# Train-test split\n",
    "indices = np.arange(len(encoded_sentences))\n",
    "np.random.shuffle(indices)\n",
    "split = int(0.8 * len(indices))\n",
    "train_indices, test_indices = indices[:split], indices[split:]\n",
    "\n",
    "train_dataset = MaintenanceLogDataset(encoded_array[train_indices], df_logs['label'].iloc[train_indices].tolist())\n",
    "test_dataset = MaintenanceLogDataset(encoded_array[test_indices], df_logs['label'].iloc[test_indices].tolist())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "print(f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723d0464",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Transformer-Based Maintenance Classifier\n",
    "\n",
    "Architecture:\n",
    "- Token Embedding\n",
    "- Positional Encoding\n",
    "- Stacked Transformer Encoder Blocks\n",
    "- Global pooling (CLS token)\n",
    "- Classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baccbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaintenanceTransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, num_heads: int, ff_hidden_dim: int,\n",
    "                 num_layers: int, num_classes: int, max_len: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab[PAD_TOKEN])\n",
    "        self.positional_encoding = PositionalEncoding(embed_dim, max_len=max_len)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderBlock(embed_dim, num_heads, ff_hidden_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)\n",
    "        mask = (x != vocab[PAD_TOKEN]).unsqueeze(1).unsqueeze(2)  # (batch, 1, 1, seq_len)\n",
    "\n",
    "        embeddings = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
    "        embeddings = self.positional_encoding(embeddings)\n",
    "\n",
    "        attn_weights_collection = []\n",
    "        out = embeddings\n",
    "        for layer in self.layers:\n",
    "            out, attn_weights = layer(out, mask=mask)\n",
    "            attn_weights_collection.append(attn_weights)\n",
    "\n",
    "        # Use CLS token representation (index 0)\n",
    "        cls_repr = out[:, 0, :]\n",
    "        logits = self.classifier(self.dropout(cls_repr))\n",
    "\n",
    "        return logits, attn_weights_collection\n",
    "\n",
    "\n",
    "# Instantiate model\n",
    "model = MaintenanceTransformerClassifier(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=128,\n",
    "    num_heads=4,\n",
    "    ff_hidden_dim=256,\n",
    "    num_layers=2,\n",
    "    num_classes=len(labels_map),\n",
    "    max_len=MAX_SEQ_LEN,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba139012",
   "metadata": {},
   "source": [
    "### Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7c71df",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(inputs)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * inputs.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return epoch_loss / total, correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits, _ = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            epoch_loss += loss.item() * inputs.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return epoch_loss / total, correct / total, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be1d207",
   "metadata": {},
   "source": [
    "### Train the Transformer Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778c513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "train_losses, test_losses = [], []\n",
    "train_accuracies, test_accuracies = [], []\n",
    "\n",
    "print('üîÑ Training Transformer Classifier...')\n",
    "print('=' * 60)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    test_loss, test_acc, _, _ = evaluate(model, test_loader, criterion)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        print(f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78e66ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "axes[0].plot(range(1, EPOCHS + 1), train_losses, label='Train Loss', color='blue', linewidth=2)\n",
    "axes[0].plot(range(1, EPOCHS + 1), test_losses, label='Test Loss', color='red', linewidth=2)\n",
    "axes[0].set_title('Loss Curves', fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Cross-Entropy Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(range(1, EPOCHS + 1), train_accuracies, label='Train Acc', color='green', linewidth=2)\n",
    "axes[1].plot(range(1, EPOCHS + 1), test_accuracies, label='Test Acc', color='orange', linewidth=2)\n",
    "axes[1].set_title('Accuracy Curves', fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('üìà Visualization complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aa9166",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Inspect Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbc72c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map indices to labels\n",
    "idx_to_label = {v: k.title() for k, v in labels_map.items()}\n",
    "\n",
    "model.eval()\n",
    "inputs, true_labels = next(iter(test_loader))\n",
    "inputs = inputs.to(device)\n",
    "true_labels = true_labels.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits, attn_history = model(inputs)\n",
    "    preds = logits.argmax(dim=1)\n",
    "\n",
    "for i in range(min(len(inputs), 5)):\n",
    "    token_ids = inputs[i].cpu().numpy()\n",
    "    tokens = [inv_vocab.get(idx, UNK_TOKEN) for idx in token_ids if idx != vocab[PAD_TOKEN]]\n",
    "    sentence = ' '.join(tokens).replace(CLS_TOKEN + ' ', '')\n",
    "    print(f"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
