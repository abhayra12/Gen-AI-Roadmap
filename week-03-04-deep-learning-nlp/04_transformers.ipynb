{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c605c023",
   "metadata": {},
   "source": [
    "# âœ¨ Notebook 04: The Transformer Architecture\n",
    "\n",
    "**Week 3-4: Deep Learning & NLP Foundations**  \n",
    "**Gen AI Masters Program**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Objectives\n",
    "\n",
    "Welcome to one of the most significant breakthroughs in the history of deep learning: the **Transformer architecture**. Introduced in the 2017 paper \"Attention Is All You Need,\" Transformers have completely revolutionized Natural Language Processing (NLP) and now form the backbone of modern Large Language Models (LLMs) like GPT, BERT, and T5.\n",
    "\n",
    "In the previous notebook, we saw how RNNs and LSTMs process sequences one token at a time. While effective, this sequential nature has two major drawbacks:\n",
    "1.  **It's Slow**: The computation for step `t` depends on the output of step `t-1`, making it **impossible to parallelize** the processing of a sequence. This becomes a major bottleneck for very long sequences.\n",
    "2.  **Long-Range Dependencies**: While LSTMs were designed to handle long-range dependencies, they still struggle to maintain context over extremely long distances. Information can get diluted as it passes through the recurrent chain.\n",
    "\n",
    "Transformers solve these problems by getting rid of recurrence entirely and relying on a powerful mechanism called **self-attention**.\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1.  **Understand the Core Concept of Self-Attention**: Grasp the intuition behind how self-attention allows the model to weigh the importance of all other words in a sequence when processing a specific word, looking at the entire sentence at once.\n",
    "2.  **Implement Positional Encodings**: Understand why positional information is critical in a non-recurrent model and implement the sine/cosine positional encoding scheme from the original paper.\n",
    "3.  **Build a Transformer Encoder**: Construct a Transformer model for classification from scratch using PyTorch's built-in `TransformerEncoderLayer`.\n",
    "4.  **Apply Transformers to a Practical Task**: Apply your Transformer model to the same **IMDb sentiment classification task** from the previous notebook, allowing for a direct comparison between the LSTM and Transformer approaches.\n",
    "\n",
    "**Estimated Time:** 3-4 hours\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š What is a Transformer?\n",
    "\n",
    "A Transformer is a neural network architecture that, like an RNN, is designed to handle sequential data. However, it processes the entire sequence at once rather than one element at a time.\n",
    "\n",
    "The key innovation is the **self-attention mechanism**. For any given word, self-attention allows the model to directly look at and draw information from all other words in the sequence. It calculates \"attention scores\" that determine how much focus to place on other words when representing the current word. This allows it to build highly contextualized representations and capture complex relationships, no matter how far apart the words are.\n",
    "\n",
    "By stacking these self-attention layers, Transformers can build incredibly rich and deep understandings of language, which is why they have become the dominant architecture in modern NLP. Let's build one! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a1957d",
   "metadata": {},
   "source": [
    "## ðŸš€ Agenda\n",
    "\n",
    "Our exploration of the Transformer architecture will be structured as follows:\n",
    "\n",
    "1.  **Setting the Stage**: We'll import the necessary libraries and configure our environment.\n",
    "2.  **Data Preparation**: We will use the **exact same** IMDb dataset and preprocessing pipeline as the previous notebook. This allows for a fair, direct comparison between the performance of an LSTM and a Transformer on the same task.\n",
    "3.  **Building the Transformer Model**: We'll construct our model piece by piece, understanding each component's role.\n",
    "    *   **Positional Encoding**: We'll implement the clever sine/cosine positional encoding scheme to give the model a sense of word order.\n",
    "    *   **The Full `SentimentTransformer`**: We'll assemble the complete model, incorporating an embedding layer, our positional encoding, and a stack of PyTorch's `TransformerEncoder` layers.\n",
    "4.  **Training the Model**: We'll define the optimizer and loss function and write the training and evaluation loops.\n",
    "5.  **Inference**: We'll use our trained Transformer to predict the sentiment of new, unseen movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc90d548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Set up the Environment ---\n",
    "\n",
    "# We'll be using the same libraries as the previous notebook.\n",
    "!pip install torchtext portalocker --quiet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Import torchtext components\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import numpy as np\n",
    "import math # For the positional encoding calculation\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Set the default device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"âœ… Using device: {device.upper()}\")\n",
    "\n",
    "# --- Plotting Style ---\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "sns.set_palette(\"colorblind\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c055ed77",
   "metadata": {},
   "source": [
    "## ðŸ’¬ Part 2: Preparing the IMDb Dataset\n",
    "\n",
    "This step is **identical** to the one in our previous LSTM notebook. A robust model architecture is only as good as the data it's fed, so we'll follow the same rigorous preprocessing pipeline. Using the same data setup is crucial for making a fair comparison between the LSTM and Transformer models.\n",
    "\n",
    "**Reminder of the Pipeline:**\n",
    "1.  **Load Data**: Fetch the IMDb training and test sets using `torchtext`.\n",
    "2.  **Tokenization**: Break sentences into words (tokens) using a basic English tokenizer.\n",
    "3.  **Build Vocabulary**: Create a word-to-index mapping from our training data, capped at the 10,000 most frequent words.\n",
    "4.  **Numericalize & Pad**: Convert sentences to sequences of integers and pad or truncate them to a fixed length (`MAX_LEN`) so they can be processed in batches.\n",
    "\n",
    "Let's execute the code. It will look very familiar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf02957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2.1. Load Data, Define Tokenizer, and Build Vocabulary ---\n",
    "print(\"Loading IMDb dataset and defining tokenizer...\")\n",
    "train_iter, test_iter = IMDB(split=('train', 'test'))\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# Define constants for vocabulary and special tokens\n",
    "VOCAB_SIZE = 10000\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    \"\"\"Helper generator function to yield tokens from the dataset.\"\"\"\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# We need a fresh iterator to build the vocabulary, as it gets consumed.\n",
    "train_iter_for_vocab, _ = IMDB(split=('train', 'test'))\n",
    "\n",
    "print(f\"Building vocabulary with top {VOCAB_SIZE} words...\")\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(train_iter_for_vocab),\n",
    "    specials=[UNK_TOKEN, PAD_TOKEN],\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    special_first=True\n",
    ")\n",
    "vocab.set_default_index(vocab[UNK_TOKEN])\n",
    "\n",
    "pad_index = vocab[PAD_TOKEN]\n",
    "\n",
    "print(f\"âœ… Vocabulary created. Size: {len(vocab)}\")\n",
    "print(f\"   - Index for '<pad>': {pad_index}\")\n",
    "print(f\"   - Example mapping: 'transformer' -> {vocab['transformer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc131bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2.2. Define Data Processing and Create DataLoaders ---\n",
    "MAX_LEN = 250    # Maximum sequence length\n",
    "BATCH_SIZE = 32  # Batch size for the DataLoader\n",
    "\n",
    "def create_dataloader(data_iter, is_train=True):\n",
    "    \"\"\"\n",
    "    A function to convert the raw data iterator into a batched and padded DataLoader.\n",
    "    \"\"\"\n",
    "    all_texts, all_labels = [], []\n",
    "    \n",
    "    print(f\"Processing data... (This may take a moment)\")\n",
    "    for label, text in tqdm(data_iter, desc=\"Numericalizing and Padding\"):\n",
    "        numericalized_text = vocab(tokenizer(text))\n",
    "        \n",
    "        if len(numericalized_text) > MAX_LEN:\n",
    "            numericalized_text = numericalized_text[:MAX_LEN]\n",
    "            \n",
    "        all_texts.append(torch.tensor(numericalized_text, dtype=torch.int64))\n",
    "        all_labels.append(label - 1)\n",
    "\n",
    "    padded_texts = pad_sequence(all_texts, batch_first=True, padding_value=pad_index)\n",
    "    \n",
    "    dataset = TensorDataset(padded_texts, torch.tensor(all_labels, dtype=torch.float32))\n",
    "    return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=is_train)\n",
    "\n",
    "# --- Create the DataLoaders ---\n",
    "# Get fresh iterators before processing\n",
    "train_iter, test_iter = IMDB(split=('train', 'test'))\n",
    "\n",
    "train_dataloader = create_dataloader(train_iter, is_train=True)\n",
    "test_dataloader = create_dataloader(test_iter, is_train=False)\n",
    "\n",
    "print(\"\\nâœ… Data preparation complete.\")\n",
    "print(f\"   - Number of training batches: {len(train_dataloader)}\")\n",
    "print(f\"   - Number of testing batches:  {len(test_dataloader)}\")\n",
    "\n",
    "# --- Inspect a Single Batch ---\n",
    "texts, labels = next(iter(train_dataloader))\n",
    "print(f\"\\nShape of a single batch of texts: {texts.shape}\")\n",
    "print(f\"Shape of a single batch of labels: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b60446c",
   "metadata": {},
   "source": [
    "## ðŸ§  Part 3: Building the Transformer Model\n",
    "\n",
    "Now, let's construct the components of our Transformer-based sentiment classifier. We'll build it up piece by piece, starting with the solution to the \"orderless\" problem.\n",
    "\n",
    "### 3.1. Positional Encoding\n",
    "\n",
    "The first challenge with removing recurrence is that the model no longer has any inherent sense of word order. Without extra information, a Transformer would see the sentences \"I love this movie\" and \"this movie I love\" as identical.\n",
    "\n",
    "**Solution:** We inject **Positional Encodings** into the input embeddings. These are vectors that provide information about the absolute position of a token within the sequence. The original paper used a clever method with sine and cosine functions of different frequencies.\n",
    "\n",
    "The formula for the positional encoding `PE` at position `pos` and dimension `i` is:\n",
    "$$ PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right) $$\n",
    "$$ PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right) $$\n",
    "\n",
    "Where:\n",
    "-   `pos` is the position of the token in the sequence (0, 1, 2, ...).\n",
    "-   `i` is the index of the dimension within the embedding vector.\n",
    "-   `d_model` is the total dimension of the embedding (our `embed_dim`).\n",
    "\n",
    "This method has two great properties:\n",
    "1.  It produces a unique encoding for each time-step.\n",
    "2.  It allows the model to easily learn to attend to relative positions, since for any fixed offset `k`, `PE_{pos+k}` can be represented as a linear function of `PE_{pos}`.\n",
    "\n",
    "We will implement this as a `nn.Module`. The positional encodings are calculated once and then added to the input embeddings during the forward pass. They are not learnable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d4aca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.1. Implement Positional Encoding ---\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Injects positional information into the input embeddings.\n",
    "    This is a fixed, non-learnable component.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): The dimensionality of the embeddings.\n",
    "            dropout (float): The dropout probability.\n",
    "            max_len (int): The maximum possible sequence length.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Create a positional encoding matrix of shape (max_len, embed_dim).\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        \n",
    "        # Create a tensor representing the positions (0, 1, 2, ..., max_len-1).\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Create the division term for the sine and cosine functions.\n",
    "        # This term creates a geometric progression of frequencies.\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        \n",
    "        # Calculate the positional encodings using the sine/cosine formula.\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # Apply to even indices.\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # Apply to odd indices.\n",
    "        \n",
    "        # Add a batch dimension to the positional encoding matrix so it can be\n",
    "        # easily added to the batch of embeddings. Shape becomes [1, max_len, embed_dim].\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # Register 'pe' as a buffer. A buffer is part of the model's state but is not\n",
    "        # considered a parameter to be trained. It will be moved to the correct device\n",
    "        # along with the model.\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        The forward pass adds the positional encodings to the input embeddings.\n",
    "        \n",
    "        Args:\n",
    "            x: The input embeddings, shape [batch_size, seq_len, embedding_dim].\n",
    "        \n",
    "        Returns:\n",
    "            The embeddings with added positional information.\n",
    "        \"\"\"\n",
    "        # Add the positional encoding to the input tensor.\n",
    "        # We only need the encodings up to the sequence length of the current batch.\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# --- Visualize the Positional Encodings ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "pos_encoding_visualization = PositionalEncoding(embed_dim=128, max_len=500)\n",
    "pe_matrix = pos_encoding_visualization.pe.squeeze().numpy()\n",
    "\n",
    "plt.imshow(pe_matrix, cmap='viridis', aspect='auto')\n",
    "plt.title(\"Visualization of Positional Encodings Matrix\")\n",
    "plt.xlabel(\"Embedding Dimension\")\n",
    "plt.ylabel(\"Position in Sequence\")\n",
    "plt.colorbar(label=\"Encoding Value\")\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¡ Analysis: The unique pattern for each position allows the model to distinguish between words at different locations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29f9ba3",
   "metadata": {},
   "source": [
    "### 3.2. Assembling the Full Classifier Model\n",
    "\n",
    "With positional information in place, we can build the main model. A Transformer Encoder is a stack of identical layers. Each layer has two main sub-components:\n",
    "\n",
    "1.  **A Multi-Head Self-Attention Mechanism:** This is the core of the Transformer. It allows the model to weigh the importance of different words in the input sequence when producing a representation for a specific word. Instead of building this from scratch, we'll use PyTorch's highly optimized `nn.TransformerEncoderLayer`, which contains this mechanism internally.\n",
    "\n",
    "2.  **A Position-wise Feed-Forward Network:** This is a simple fully connected network that is applied independently to each position (i.e., to each word's representation) in the sequence.\n",
    "\n",
    "Each of these sub-layers has a residual connection around it, followed by a layer normalization. This is a standard practice that helps stabilize the training of deep networks. The full operation is: `output = LayerNorm(x + Sublayer(x))`.\n",
    "\n",
    "Our complete sentiment classifier will stack these components:\n",
    "\n",
    "1.  **Input Embedding Layer (`nn.Embedding`):** Converts input word indices into dense vectors.\n",
    "2.  **Positional Encoding Layer (`PositionalEncoding`):** Injects information about word order into the embeddings.\n",
    "3.  **Transformer Encoder (`nn.TransformerEncoder`):** A stack of `nn.TransformerEncoderLayer`s that processes the sequence and builds rich, context-aware representations for every token.\n",
    "4.  **Classification Head (`nn.Linear`):** A final linear layer that takes the representation of the **first token** in the sequence and maps it to our output class. We only need the output for one token because the self-attention mechanism ensures that this single token's final representation has already gathered information from the entire sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e9a219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.2. Define the Transformer Classifier ---\n",
    "\n",
    "class SentimentTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, hidden_dim, dropout=0.1, max_len=5000):\n",
    "        \"\"\"\n",
    "        Initializes the Transformer model for sentiment classification.\n",
    "        \n",
    "        Args:\n",
    "            vocab_size (int): The size of the vocabulary.\n",
    "            embed_dim (int): The dimensionality of the embeddings.\n",
    "            num_heads (int): The number of attention heads in the multi-head attention mechanism.\n",
    "            num_layers (int): The number of stacked Transformer encoder layers.\n",
    "            hidden_dim (int): The dimensionality of the feed-forward network inside the encoder.\n",
    "            dropout (float): The dropout probability.\n",
    "            max_len (int): The maximum sequence length for positional encoding.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_index)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, dropout, max_len)\n",
    "        \n",
    "        # A single Transformer Encoder Layer.\n",
    "        # `d_model` is the same as `embed_dim`.\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=hidden_dim, \n",
    "            dropout=dropout,\n",
    "            batch_first=True # Crucial: ensures input format is [batch, seq_len, features].\n",
    "        )\n",
    "        \n",
    "        # A stack of identical encoder layers.\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # The final linear layer for classification.\n",
    "        self.classifier = nn.Linear(embed_dim, 1)\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        Defines the forward pass.\n",
    "        \n",
    "        Args:\n",
    "            src (Tensor): The input tensor of word indices, shape [batch_size, seq_len].\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The raw output logits, shape [batch_size].\n",
    "        \"\"\"\n",
    "        # src shape: [batch_size, seq_len]\n",
    "        \n",
    "        # Create a padding mask.\n",
    "        # The mask should be `True` for positions that should be ignored (i.e., padding tokens).\n",
    "        # The attention mechanism will not attend to these positions.\n",
    "        src_key_padding_mask = (src == pad_index)\n",
    "        # src_key_padding_mask shape: [batch_size, seq_len]\n",
    "\n",
    "        # 1. Get embeddings and add positional encoding.\n",
    "        # We scale the embeddings by sqrt(embed_dim) as recommended in the original paper.\n",
    "        embedded = self.embedding(src) * math.sqrt(self.embed_dim)\n",
    "        pos_encoded = self.pos_encoder(embedded)\n",
    "        \n",
    "        # 2. Pass through the Transformer Encoder.\n",
    "        # The mask ensures that the attention mechanism ignores padding tokens.\n",
    "        transformer_output = self.transformer_encoder(pos_encoded, src_key_padding_mask=src_key_padding_mask)\n",
    "        # transformer_output shape: [batch_size, seq_len, embed_dim]\n",
    "        \n",
    "        # 3. Perform classification.\n",
    "        # We use the output corresponding to the first token (at index 0) for classification.\n",
    "        # Due to self-attention, this token's representation contains information from the whole sequence.\n",
    "        cls_output = transformer_output[:, 0, :]\n",
    "        # cls_output shape: [batch_size, embed_dim]\n",
    "        \n",
    "        output = self.classifier(cls_output)\n",
    "        # output shape: [batch_size, 1]\n",
    "        \n",
    "        return output.squeeze(1)\n",
    "\n",
    "# --- Instantiate the Model ---\n",
    "EMBED_DIM = 128       # Dimensionality of the embeddings.\n",
    "NUM_HEADS = 4         # Number of attention heads. Must be a divisor of EMBED_DIM.\n",
    "NUM_LAYERS = 2        # Number of Transformer Encoder layers to stack.\n",
    "HIDDEN_DIM = 512      # Hidden dimension of the feed-forward network.\n",
    "DROPOUT = 0.2\n",
    "\n",
    "model = SentimentTransformer(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    dropout=DROPOUT,\n",
    "    max_len=MAX_LEN\n",
    ").to(device)\n",
    "\n",
    "# --- Count Model Parameters ---\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"âœ… Transformer Model created and moved to device.\")\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters.')\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e154f75f",
   "metadata": {},
   "source": [
    "## ðŸš€ Part 4: Training the Transformer Model\n",
    "\n",
    "The training and evaluation process will be very similar to the one we used for the LSTM model, demonstrating the modularity of PyTorch workflows.\n",
    "\n",
    "1.  **Optimizer:** We'll use the **Adam** optimizer. The learning rate for Transformers is often set to be smaller than for LSTMs, so we'll start with `1e-4`.\n",
    "2.  **Loss Function:** We'll again use `BCEWithLogitsLoss`, which is perfect for our binary classification task and is numerically stable.\n",
    "3.  **Training & Evaluation Loops:** We will reuse our `train` and `evaluate` functions to handle the epoch loops, loss calculation, and accuracy monitoring. We'll also save the best-performing model based on validation loss.\n",
    "\n",
    "Let's set up the training components and start the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b111027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.1. Define Optimizer, Loss, and Accuracy Functions ---\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"Calculates accuracy for binary classification.\"\"\"\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "# --- 4.2. Define Training and Evaluation Functions ---\n",
    "# These functions are identical to the ones from the LSTM notebook.\n",
    "def train_one_epoch(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch_texts, batch_labels in tqdm(iterator, desc=\"Training\"):\n",
    "        batch_texts, batch_labels = batch_texts.to(device), batch_labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch_texts)\n",
    "        loss = criterion(predictions, batch_labels)\n",
    "        acc = binary_accuracy(predictions, batch_labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate_one_epoch(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_texts, batch_labels in tqdm(iterator, desc=\"Evaluating\"):\n",
    "            batch_texts, batch_labels = batch_texts.to(device), batch_labels.to(device)\n",
    "            \n",
    "            predictions = model(batch_texts)\n",
    "            loss = criterion(predictions, batch_labels)\n",
    "            acc = binary_accuracy(predictions, batch_labels)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ef4629",
   "metadata": {},
   "source": [
    "### 4.3. Run the Training Loop\n",
    "\n",
    "Now we'll execute the main training loop for a few epochs. We'll keep track of the best validation loss to save the best version of our model.\n",
    "\n",
    "Note that training a Transformer can be more computationally intensive per epoch than an LSTM due to the self-attention mechanism's quadratic complexity with respect to sequence length. However, it often converges in fewer epochs. We'll train for just **3 epochs** here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77b8a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.3. Run the Training Loop ---\n",
    "N_EPOCHS = 3 # Transformers can converge in fewer epochs, but each epoch is computationally heavier.\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "print(\"ðŸš€ Starting Transformer model training...\")\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    train_loss, train_acc = train_one_epoch(model, train_dataloader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate_one_epoch(model, test_dataloader, criterion)\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(valid_loss)\n",
    "    history['val_acc'].append(valid_acc)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best_transformer_model.pt')\n",
    "    \n",
    "    print(f'\\nEpoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "print(\"\\nâœ… Training complete.\")\n",
    "print(f\"Best validation loss: {best_valid_loss:.3f}\")\n",
    "print(\"The best model has been saved to 'best_transformer_model.pt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678d4dbd",
   "metadata": {},
   "source": [
    "## ðŸŽ¬ Part 5: Use the Model for Inference\n",
    "\n",
    "Just like with our LSTM, let's use our trained Transformer to predict the sentiment of new sentences. The process is identical: load the best model, preprocess the input sentence (tokenize, numericalize, batch), and pass it to the model to get a prediction.\n",
    "\n",
    "A key difference in preprocessing for the Transformer is that we don't need to worry about truncating the input to the *exact* `MAX_LEN` during inference, as the model can handle variable-length sequences (though we must still pad it to be rectangular within a batch, which is handled by the `unsqueeze` for a single sentence). However, for consistency with our training setup, we'll pad/truncate to `MAX_LEN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef103bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.1. Load the Best Model and Define Prediction Function ---\n",
    "print(\"Loading the best Transformer model from 'best_transformer_model.pt'...\")\n",
    "model.load_state_dict(torch.load('best_transformer_model.pt'))\n",
    "model.to(device)\n",
    "print(\"âœ… Model loaded.\")\n",
    "\n",
    "def predict_sentiment_transformer(sentence):\n",
    "    \"\"\"\n",
    "    Predicts the sentiment of a sentence using the trained Transformer model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess the sentence\n",
    "    tokenized = tokenizer(sentence)\n",
    "    indexed = vocab(tokenized)\n",
    "    \n",
    "    # Ensure the sequence is not longer than the model's max length\n",
    "    if len(indexed) > MAX_LEN:\n",
    "        indexed = indexed[:MAX_LEN]\n",
    "        \n",
    "    # Convert to tensor and add batch dimension\n",
    "    tensor = torch.LongTensor(indexed).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        prediction = model(tensor)\n",
    "        \n",
    "    # Convert to probability and classify\n",
    "    probability = torch.sigmoid(prediction).item()\n",
    "    sentiment = \"Positive\" if probability > 0.5 else \"Negative\"\n",
    "    return sentiment, probability\n",
    "\n",
    "# --- 5.2. Test with Example Sentences ---\n",
    "print(\"\\n--- Testing with a positive review ---\")\n",
    "positive_review = \"This is one of the best films I have ever seen, a true masterpiece of cinema. The acting, direction, and storyline were all flawless.\"\n",
    "sentiment, prob = predict_sentiment_transformer(positive_review)\n",
    "print(f\"Sentence:  '{positive_review}'\")\n",
    "print(f\"Sentiment: {sentiment}\")\n",
    "print(f\"Probability (Positive): {prob:.4f}\\n\")\n",
    "\n",
    "print(\"--- Testing with a negative review ---\")\n",
    "negative_review = \"A complete waste of time. The plot was predictable, the acting was terrible, and I wouldn't recommend it to anyone.\"\n",
    "sentiment, prob = predict_sentiment_transformer(negative_review)\n",
    "print(f\"Sentence:  '{negative_review}'\")\n",
    "print(f\"Sentiment: {sentiment}\")\n",
    "print(f\"Probability (Positive): {prob:.4f}\\n\")\n",
    "\n",
    "print(\"--- Comparing with the LSTM notebook's ambiguous review ---\")\n",
    "ambiguous_review = \"The movie was okay, I guess. Some parts were good, others not so much. I'm not sure if I would recommend it.\"\n",
    "sentiment, prob = predict_sentiment_transformer(ambiguous_review)\n",
    "print(f\"Sentence:  '{ambiguous_review}'\")\n",
    "print(f\"Sentiment: {sentiment}\")\n",
    "print(f\"Probability (Positive): {prob:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
