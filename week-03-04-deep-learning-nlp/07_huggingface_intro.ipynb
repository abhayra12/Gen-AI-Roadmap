{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73edf05a",
   "metadata": {},
   "source": [
    "# ü§ó Notebook 07: Introduction to the Hugging Face Ecosystem\n",
    "\n",
    "**Week 3-4: Deep Learning & NLP Foundations**  \n",
    "**Gen AI Masters Program**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objectives\n",
    "\n",
    "Welcome to Hugging Face! This is your gateway to the world of modern, pre-trained NLP models. The `transformers` library and the Hugging Face Hub have become the industry standard for building, sharing, and deploying state-of-the-art models.\n",
    "\n",
    "In this notebook, you will learn the complete end-to-end workflow:\n",
    "\n",
    "1.  **The Ecosystem:** Get a high-level overview of the Hugging Face Hub, `transformers`, `datasets`, and other key libraries.\n",
    "2.  **Zero-Shot Inference:** Use powerful, pre-trained models for tasks like classification and summarization *without any training*.\n",
    "3.  **The Core Components:** Understand the roles of `AutoTokenizer` and `AutoModel`.\n",
    "4.  **Fine-Tuning:** Learn how to adapt a pre-trained model to a specific, custom task using the `Trainer` API.\n",
    "5.  **Inference and Deployment:** Use your fine-tuned model for predictions and save it for future use.\n",
    "\n",
    "**Estimated Time:** 3-4 hours\n",
    "\n",
    "---\n",
    "\n",
    "## üåç The Hugging Face Ecosystem at a Glance\n",
    "\n",
    "Hugging Face is more than just a library; it's a collaborative platform for machine learning.\n",
    "\n",
    "*   **The Hub:** A central repository with over 500,000 models, 100,000 datasets, and 300,000 \"Spaces\" (apps). It's like GitHub for machine learning.\n",
    "*   **`transformers`:** The core Python library. It provides a unified API for accessing thousands of pre-trained models (like BERT, GPT, T5) and tools for training them.\n",
    "*   **`datasets`:** A library for efficiently loading, processing, and sharing large datasets.\n",
    "*   **`evaluate`:** A library for easily calculating standard NLP metrics (like accuracy, F1, BLEU).\n",
    "*   **`tokenizers`:** A high-performance library for converting text into the numerical inputs that models understand.\n",
    "\n",
    "We'll touch on all of these as we build a practical text classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bac9383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core Library Imports ---\n",
    "import torch\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "# --- Hugging Face Library Imports ---\n",
    "# The 'pipeline' is a high-level, easy-to-use API for inference\n",
    "from transformers import pipeline\n",
    "\n",
    "# Auto* classes are smart wrappers that can automatically load the correct\n",
    "# architecture from a given model checkpoint\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# The Trainer API provides a complete training and evaluation loop\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# A helper for dynamically padding batches of text\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# The 'datasets' library for loading and processing data\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# The 'evaluate' library for metrics\n",
    "import evaluate\n",
    "\n",
    "# --- Configuration ---\n",
    "# Set a seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set the device (use GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "\n",
    "# Check the transformers version\n",
    "import transformers\n",
    "print(f\"Transformers version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca787b3b",
   "metadata": {},
   "source": [
    "## üß† Part 1: Instant Results with Inference Pipelines\n",
    "\n",
    "The `pipeline` is the easiest way to get started with Hugging Face. It abstracts away all the complexity of tokenization, model loading, and post-processing, allowing you to use a pre-trained model for a specific task in just a few lines of code.\n",
    "\n",
    "This is often called **zero-shot inference** because we are using a model for a task it wasn't *explicitly* trained on, but which it can perform due to its general language understanding.\n",
    "\n",
    "### Example: Zero-Shot Text Classification\n",
    "\n",
    "Let's say we have a maintenance log and we want to classify its severity. We can use a model pre-trained on a general \"Natural Language Inference\" (NLI) task to do this, without any fine-tuning. The model understands whether a \"hypothesis\" is an \"entailment,\" \"contradiction,\" or \"neutral\" to a \"premise.\" We can cleverly adapt this to our classification task.\n",
    "\n",
    "*   **Premise:** The maintenance log text.\n",
    "*   **Hypotheses:** Our candidate labels (e.g., \"This text is about a critical issue,\" \"This text is about a normal event\").\n",
    "\n",
    "The model will tell us which \"hypothesis\" is most entailed by the \"premise.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879eb228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create an inference pipeline for 'zero-shot-classification'\n",
    "# The model 'facebook/bart-large-mnli' is a popular choice for this task.\n",
    "# We specify the device to ensure it runs on the GPU if available.\n",
    "classifier = pipeline(\n",
    "    task='zero-shot-classification',\n",
    "    model='facebook/bart-large-mnli',\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# 2. Define the text we want to classify and our candidate labels\n",
    "maintenance_log = \"Hydraulic pump output pressure collapsed, leading to an immediate line shutdown.\"\n",
    "candidate_labels = ['normal operation', 'warning', 'critical failure']\n",
    "\n",
    "# 3. Run the classification\n",
    "result = classifier(maintenance_log, candidate_labels)\n",
    "\n",
    "# The output is a dictionary containing the labels and their corresponding scores\n",
    "print(f\"Log Entry: '{result['sequence']}'\\n\")\n",
    "print(\"Classification Scores:\")\n",
    "for label, score in zip(result['labels'], result['scores']):\n",
    "    print(f\"- {label}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1262dae9",
   "metadata": {},
   "source": [
    "### Example: Summarization\n",
    "\n",
    "Another powerful zero-shot task is summarization. We can take a long, detailed log entry and use a model to condense it into a short, actionable summary. This is incredibly useful for generating daily reports or quickly understanding the gist of a complex issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8400c172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a summarization pipeline\n",
    "# 'philschmid/bart-large-cnn-samsum' is a model fine-tuned for summarizing conversations,\n",
    "# but it works well for technical logs too.\n",
    "summarizer = pipeline(\n",
    "    task='summarization',\n",
    "    model='philschmid/bart-large-cnn-samsum',\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# 2. Define a long log entry\n",
    "long_log_entry = (\n",
    "    \"During the midnight shift, operators in Bay 3 reported a series of persistent, high-frequency \"\n",
    "    \"vibration spikes originating from the main gearbox assembly of Conveyor Belt #5. \"\n",
    "    \"Following the vibration alerts, the coolant temperature for the associated motor began to rise, \"\n",
    "    \"exceeding the nominal operating threshold of 85¬∞C. A manual inspection by the on-site technician \"\n",
    "    \"confirmed a partial blockage in the primary coolant loop, likely due to sediment buildup. \"\n",
    "    \"As a temporary measure, the secondary coolant bypass was engaged, which successfully restored \"\n",
    "    \"coolant flow and stabilized the temperature. However, the root cause of the blockage has not \"\n",
    "    \"been addressed, and the gearbox pressure remains unstable.\"\n",
    ")\n",
    "\n",
    "# 3. Generate the summary\n",
    "# We can control the length of the desired summary.\n",
    "summary = summarizer(\n",
    "    long_log_entry,\n",
    "    max_length=50,  # Maximum number of words in the summary\n",
    "    min_length=20,  # Minimum number of words\n",
    "    do_sample=False # Use deterministic decoding (no randomness)\n",
    ")[0]['summary_text']\n",
    "\n",
    "print(\"--- Original Log ---\")\n",
    "print(long_log_entry)\n",
    "print(\"\\n--- Generated Summary ---\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5556eb",
   "metadata": {},
   "source": [
    "## üß† Part 2: The Core Components - Tokenizers and Models\n",
    "\n",
    "While pipelines are great for quick results, building custom solutions requires understanding the two core components that power them: the **Tokenizer** and the **Model**.\n",
    "\n",
    "### The Tokenizer\n",
    "\n",
    "A tokenizer's job is to convert raw text into a numerical format that the model can understand. This is a multi-step process:\n",
    "1.  **Splitting:** The text is broken down into words, subwords, or characters. Most modern models use **subword tokenization** (like WordPiece or BPE), which can handle out-of-vocabulary words and capture morphological information.\n",
    "2.  **Converting to IDs:** Each token is mapped to a unique integer ID from the model's vocabulary.\n",
    "3.  **Adding Special Tokens:** Special tokens required by the model are added, such as `[CLS]` (start of sequence) and `[SEP]` (separator).\n",
    "4.  **Creating Attention Masks:** An attention mask (a tensor of 1s and 0s) is created to tell the model which tokens are real and which are padding.\n",
    "\n",
    "The `AutoTokenizer` class automatically downloads and configures the correct tokenizer for a given model checkpoint from the Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de91c3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the tokenizer for 'distilbert-base-uncased'\n",
    "# This is a fast and popular model, good for general-purpose tasks.\n",
    "model_checkpoint = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# 2. Tokenize a sentence\n",
    "# We pass the text and ask for PyTorch tensors in return ('pt').\n",
    "encoded_input = tokenizer(maintenance_log, return_tensors='pt')\n",
    "\n",
    "print(\"--- Tokenizer Output ---\")\n",
    "# The output is a dictionary containing the input IDs and the attention mask\n",
    "for key, value in encoded_input.items():\n",
    "    print(f\"{key}:\")\n",
    "    print(value)\n",
    "\n",
    "# 3. Decode the input IDs back to tokens to see what's happening\n",
    "input_ids = encoded_input['input_ids'][0]\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "print(\"\\n--- Decoded Tokens ---\")\n",
    "print(tokens)\n",
    "\n",
    "# Notice the special tokens '[CLS]' and '[SEP]' that have been added.\n",
    "# Also, see how \"shutdown\" was split into \"shut\" and \"##down\". This is subword tokenization in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d580f27",
   "metadata": {},
   "source": [
    "## üß† Part 3: Fine-Tuning a Transformer for a Custom Task\n",
    "\n",
    "Zero-shot inference is powerful, but for high accuracy on a specific domain, you need to **fine-tune**. Fine-tuning takes a general-purpose pre-trained model and further trains it on a small, labeled dataset specific to your task.\n",
    "\n",
    "**Our Goal:** Build a classifier to categorize maintenance logs into three severity levels: **Normal**, **Warning**, and **Critical**.\n",
    "\n",
    "**The Workflow:**\n",
    "1.  **Prepare the Dataset:** We'll create a small, labeled dataset of maintenance logs.\n",
    "2.  **Tokenize the Data:** We'll use our `AutoTokenizer` to process the entire dataset.\n",
    "3.  **Load the Model:** We'll use `AutoModelForSequenceClassification` to load a pre-trained model with a classification head on top.\n",
    "4.  **Define Training Arguments:** We'll configure the training process (learning rate, batch size, epochs, etc.) using `TrainingArguments`.\n",
    "5.  **Train:** We'll use the `Trainer` API to run the fine-tuning loop.\n",
    "\n",
    "### Step 1: Prepare the Dataset\n",
    "\n",
    "In a real-world scenario, this data would come from a database or log files. For this example, we'll create a small `pandas` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf5a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a labeled dataset of maintenance logs\n",
    "incident_data = {\n",
    "    'text': [\n",
    "        'Lubrication schedule for gearbox #3 completed with no deviations noted.',\n",
    "        'Pressure fluctuations observed above the 5% tolerance threshold on the main hydraulic press.',\n",
    "        'Emergency stop was triggered by a high voltage surge on production line B.',\n",
    "        'Routine monthly inspection confirmed all safety sensors are calibrated and functional.',\n",
    "        'A minor coolant leak was observed near the heat exchanger housing during the night shift.',\n",
    "        'The critical alarm for furnace #2 temperature persisted for 15 minutes despite a manual override attempt.',\n",
    "        'Conveyor speed oscillation was resolved after a standard system reset procedure.',\n",
    "        'The primary bearing temperature exceeded the critical safety threshold of 120¬∞C.',\n",
    "        'Complete hydraulic pump failure on the stamping machine caused an immediate production halt.',\n",
    "        'A minor increase in vibration was logged during the swing shift for the milling machine.'\n",
    "    ],\n",
    "    'label': [\n",
    "        0, # Normal\n",
    "        1, # Warning\n",
    "        2, # Critical\n",
    "        0, # Normal\n",
    "        1, # Warning\n",
    "        2, # Critical\n",
    "        0, # Normal\n",
    "        2, # Critical\n",
    "        2, # Critical\n",
    "        1  # Warning\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "incident_df = pd.DataFrame(incident_data)\n",
    "\n",
    "# Create mappings between integer labels and human-readable names\n",
    "id2label = {0: 'NORMAL', 1: 'WARNING', 2: 'CRITICAL'}\n",
    "label2id = {'NORMAL': 0, 'WARNING': 1, 'CRITICAL': 2}\n",
    "\n",
    "print(\"--- Sample of the Labeled Dataset ---\")\n",
    "incident_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8c4be8",
   "metadata": {},
   "source": [
    "### Step 2: Convert to a `Dataset` Object and Tokenize\n",
    "\n",
    "The `Trainer` API works best with the `Dataset` object from the `datasets` library. We'll convert our DataFrame to a `Dataset` and then apply our tokenizer to the entire dataset at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beca7bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pandas DataFrame into a Hugging Face Dataset object\n",
    "full_dataset = Dataset.from_pandas(incident_df)\n",
    "\n",
    "# The .map() function is a powerful way to apply a function to the entire dataset.\n",
    "# We define a simple function to tokenize a batch of examples.\n",
    "def tokenize_function(examples):\n",
    "    # The tokenizer will pad to the length of the longest example in the batch\n",
    "    # and truncate examples that are longer than the model's max input size.\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Apply the tokenization to the entire dataset\n",
    "tokenized_dataset = full_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# For training, we need to split our data into a training set and a validation set.\n",
    "# This helps us check if the model is overfitting.\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# The result is a DatasetDict, which is a dictionary-like object holding our splits.\n",
    "split_dataset = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'validation': train_test_split['test']\n",
    "})\n",
    "\n",
    "print(\"--- Dataset Structure after Splitting and Tokenization ---\")\n",
    "print(split_dataset)\n",
    "\n",
    "print(\"\\n--- Example from the Training Set ---\")\n",
    "print(split_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bd53e6",
   "metadata": {},
   "source": [
    "### Step 3: Load the Model and Define Metrics\n",
    "\n",
    "We'll now load our pre-trained model. It's crucial to use `AutoModelForSequenceClassification` because it loads the base `distilbert` model *plus* a randomly initialized classification head on top, ready for fine-tuning.\n",
    "\n",
    "We also need to define the metric we'll use to evaluate the model during training. The `evaluate` library makes this easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea40c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model with a classification head\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint, \n",
    "    num_labels=3, # We have 3 classes: Normal, Warning, Critical\n",
    "    id2label=id2label, # Pass our mappings to the model\n",
    "    label2id=label2id\n",
    ").to(device) # Move the model to the GPU if available\n",
    "\n",
    "# The DataCollatorWithPadding will dynamically pad the sentences to the\n",
    "# longest length in a batch during training. This is more efficient than\n",
    "# padding all sentences to the overall maximum length.\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Load the accuracy metric from the 'evaluate' library\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Define a function to compute metrics during evaluation\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    This function is called by the Trainer during evaluation.\n",
    "    It takes the model's predictions and the true labels, and returns a dictionary of metrics.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    # The model outputs raw logits; we get the predicted class by taking the argmax\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    # The metric's 'compute' method returns a dictionary, e.g., {'accuracy': 0.9}\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea5507d",
   "metadata": {},
   "source": [
    "### Step 4 & 5: Define Training Arguments and Train!\n",
    "\n",
    "This is the final step before training. The `TrainingArguments` class is a powerful configuration object that lets you control every aspect of the training loop.\n",
    "\n",
    "We'll then instantiate the `Trainer`, passing it all the components we've prepared: the model, tokenizer, datasets, and training arguments. Calling `trainer.train()` will kick off the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c11db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where model checkpoints will be saved\n",
    "output_dir = 'incident_classifier_model'\n",
    "\n",
    "# Configure the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=2e-5,               # A small learning rate is crucial for fine-tuning\n",
    "    per_device_train_batch_size=4,    # Batch size for training\n",
    "    per_device_eval_batch_size=4,     # Batch size for evaluation\n",
    "    num_train_epochs=15,              # Number of times to iterate over the training data\n",
    "    weight_decay=0.01,                # Adds a bit of regularization\n",
    "    evaluation_strategy=\"epoch\",      # Run evaluation at the end of each epoch\n",
    "    save_strategy=\"epoch\",            # Save a model checkpoint at the end of each epoch\n",
    "    load_best_model_at_end=True,      # Automatically load the best model at the end of training\n",
    "    logging_strategy=\"epoch\",         # Log metrics at the end of each epoch\n",
    "    push_to_hub=False,                # We won't push to the Hub in this example\n",
    ")\n",
    "\n",
    "# Instantiate the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=split_dataset[\"train\"],\n",
    "    eval_dataset=split_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start the training!\n",
    "print(\"üöÄ Starting fine-tuning...\")\n",
    "trainer.train()\n",
    "print(\"‚úÖ Fine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e1df08",
   "metadata": {},
   "source": [
    "## üß† Part 4: Using and Evaluating the Fine-Tuned Model\n",
    "\n",
    "Now that the model is trained, let's use it for what it was built for: **inference**. We can use the `trainer.predict()` method to get predictions on our validation set, or we can build a `pipeline` for easy inference on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467233bc",
   "metadata": {},
   "source": [
    "### Inference on New, Unseen Data\n",
    "\n",
    "This is the true test of our model. Let's give it a log entry it has never seen before and see how it classifies it. We can wrap our fine-tuned model in a `pipeline` for maximum convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051c685c",
   "metadata": {},
   "source": [
    "## üß† Part 5: Saving Your Model for Deployment\n",
    "\n",
    "Training a model is only half the battle. To use it in a real application, you need to save it and be able to load it later. The `Trainer` makes this incredibly simple.\n",
    "\n",
    "When you save a model, Hugging Face saves everything you need to recreate it:\n",
    "*   The model weights (`pytorch_model.bin`).\n",
    "*   The model configuration file (`config.json`).\n",
    "*   The tokenizer files (`tokenizer.json`, `vocab.txt`, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efec68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a path to save the final, best-performing model\n",
    "final_model_path = \"./final_incident_classifier\"\n",
    "\n",
    "# Use the trainer's 'save_model' method to save the model and tokenizer\n",
    "trainer.save_model(final_model_path)\n",
    "\n",
    "print(f\"‚úÖ Model and tokenizer saved to: {final_model_path}\")\n",
    "\n",
    "# You can now find a new directory with all the necessary files.\n",
    "# This directory can be zipped and sent to a server or loaded in another notebook.\n",
    "import os\n",
    "print(\"\\n--- Files in the saved model directory ---\")\n",
    "for filename in os.listdir(final_model_path):\n",
    "    print(f\"- {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e1b11a",
   "metadata": {},
   "source": [
    "### Loading and Using the Saved Model\n",
    "\n",
    "Let's simulate a new session. We'll pretend we've just opened our notebook and want to use our previously trained model. We can load it directly from the directory we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c74723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer from the saved path\n",
    "# The Auto* classes are smart enough to figure everything out from the directory.\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(final_model_path)\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(final_model_path)\n",
    "\n",
    "# Create a new pipeline with the loaded components\n",
    "loaded_pipeline = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=loaded_model,\n",
    "    tokenizer=loaded_tokenizer\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Successfully loaded model from disk and created a new pipeline.\")\n",
    "\n",
    "# Test the loaded pipeline with another new log\n",
    "another_new_log = \"All systems are running within normal parameters after the quarterly maintenance check was completed.\"\n",
    "loaded_result = loaded_pipeline(another_new_log)\n",
    "\n",
    "print(\"\\n--- Inference with Loaded Model ---\")\n",
    "print(f\"Log: '{another_new_log}'\")\n",
    "print(f\"Predicted Result: {loaded_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b160b7ed",
   "metadata": {},
   "source": [
    "## üéâ Summary & Next Steps\n",
    "\n",
    "Congratulations! You have completed the full, end-to-end workflow for fine-tuning a Transformer model for a custom task.\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "*   **Pipelines for Quick Inference:** The `pipeline` function is the fastest way to use pre-trained models for standard tasks.\n",
    "*   **Tokenizer is Key:** The tokenizer prepares your text for the model, handling subword splitting, special tokens, and padding.\n",
    "*   **Fine-Tuning Unlocks Performance:** By fine-tuning a general-purpose model on your specific data, you can achieve state-of-the-art performance on your custom task.\n",
    "*   **The `Trainer` API:** This high-level API handles the entire training and evaluation loop, making the process streamlined and repeatable.\n",
    "*   **Save and Load for Production:** Saving a trained model with `trainer.save_model()` is the first step toward deploying it in a real-world application.\n",
    "\n",
    "### Where to Go From Here:\n",
    "\n",
    "*   **Push to Hub:** Learn how to use `trainer.push_to_hub()` to share your model with the community or your team.\n",
    "*   **More Complex Tasks:** Explore other tasks like Named Entity Recognition (NER), Question Answering, or Text Generation. The workflow is very similar.\n",
    "*   **Model Optimization:** For production, you might want to optimize your model for speed and size using techniques like quantization or pruning.\n",
    "*   **MLOps:** Integrate your training workflow into a robust MLOps pipeline for automated testing, deployment, and monitoring.\n",
    "\n",
    "This notebook provides the foundational workflow. You are now equipped to tackle a wide range of NLP problems using the power of the Hugging Face ecosystem.\n",
    "\n",
    "<div align=\"center\">\n",
    "<b>You've fine-tuned your first Transformer. The possibilities are endless! üöÄ</b>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
