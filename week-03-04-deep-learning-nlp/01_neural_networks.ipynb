{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9ce9f11",
   "metadata": {},
   "source": [
    "# üß† Notebook 01: Neural Network Fundamentals with PyTorch\n",
    "\n",
    "**Week 3-4: Deep Learning & NLP Foundations**  \n",
    "**Gen AI Masters Program**\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Objectives\n",
    "\n",
    "Welcome to the first notebook of the Deep Learning & NLP phase! This is where we transition from classical machine learning models to the powerful world of **neural networks**. This notebook lays the essential groundwork for understanding all subsequent deep learning architectures, including the Transformers that power modern Generative AI.\n",
    "\n",
    "By the end of this notebook, you will have a solid, hands-on understanding of:\n",
    "1.  **Core Neural Network Components**: You will grasp the fundamental architecture of a neural network, including the roles of **neurons**, **layers**, **weights**, and **biases**.\n",
    "2.  **The Training Mechanism**: You will master the key processes that allow a network to learn: **forward propagation** (making a prediction), **backpropagation** (calculating the error gradient), and **gradient descent** (updating the weights).\n",
    "3.  **The Role of Activation Functions**: You will understand why non-linearity is crucial for learning complex patterns and implement key activation functions like **Sigmoid**, **Tanh**, and the ubiquitous **ReLU**.\n",
    "4.  **Building Networks with PyTorch**: You will learn to construct a neural network from scratch using PyTorch's elegant and powerful `nn.Module` class.\n",
    "5.  **End-to-End Training Workflow**: You will go through the complete cycle of defining a model, preparing data, training the model, calculating its loss, and evaluating its performance on both simple and complex datasets.\n",
    "\n",
    "**Estimated Time:** 3-4 hours\n",
    "\n",
    "---\n",
    "\n",
    "## üìö What are Neural Networks?\n",
    "\n",
    "Neural networks are a class of machine learning models inspired by the structure and function of the human brain. They form the backbone of modern **deep learning** and are the engine behind the recent breakthroughs in Generative AI. Their power lies in their ability to automatically learn complex, non-linear patterns and representations directly from data.\n",
    "\n",
    "### Key Components:\n",
    "-   **üîµ Neurons (or Nodes)**: The most fundamental computational units. Each neuron receives inputs, processes them, and passes the result to the next layer.\n",
    "-   **üîó Layers**: Neurons are organized into layers. A typical network has an **Input Layer** (which receives the raw data), one or more **Hidden Layers** (where the majority of the computation happens), and an **Output Layer** (which produces the final prediction).\n",
    "-   **‚ö° Weights & Biases**: These are the **learnable parameters** of the network. The entire training process is about finding the optimal values for these weights and biases to minimize the prediction error.\n",
    "-   **üéØ Activation Functions**: Mathematical functions (like ReLU) applied to the output of each neuron. They introduce essential **non-linearity**, allowing the network to learn far more complex relationships than a simple linear model.\n",
    "-   **üìâ Loss Function**: A function that quantifies how wrong the model's predictions are compared to the actual ground truth. The goal of training is to minimize this function.\n",
    "-   **üîÑ Optimizer**: An algorithm (e.g., Adam, SGD) that updates the network's weights and biases based on the gradients computed during backpropagation. Its job is to navigate the \"loss landscape\" to find the point of minimum loss.\n",
    "\n",
    "Let's dive in and build one from the ground up! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d244528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core Deep Learning Libraries ---\n",
    "import torch\n",
    "import torch.nn as nn  # `nn` is PyTorch's module for building neural networks\n",
    "import torch.optim as optim  # `optim` contains optimization algorithms like Adam or SGD\n",
    "\n",
    "# --- Data Handling & Visualization ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- Notebook Setup ---\n",
    "# Set a consistent and professional style for all plots.\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)  # Default figure size\n",
    "plt.rcParams['figure.dpi'] = 100  # High-resolution figures\n",
    "\n",
    "# --- Reproducibility ---\n",
    "# Setting random seeds is crucial for reproducibility. It ensures that anyone running\n",
    "# this notebook will get the exact same results, as operations like weight initialization\n",
    "# will be consistent.\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# --- Device Configuration ---\n",
    "# This is a critical step in any deep learning script. We check if a CUDA-enabled GPU is\n",
    "# available and set the device accordingly. Training on a GPU can be orders of magnitude\n",
    "# faster than on a CPU.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951addac",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ The Perceptron: The Foundational Building Block\n",
    "\n",
    "The **perceptron** is the simplest possible form of a neural network, representing a single artificial neuron. It was one of the earliest supervised learning algorithms and is the conceptual ancestor of the complex deep learning models we use today. Understanding the perceptron is key to understanding how more complex networks operate.\n",
    "\n",
    "A perceptron takes multiple binary inputs, computes a weighted sum of these inputs, adds a bias, and then passes this result through a **step function** (or another activation function) to produce a single binary output.\n",
    "\n",
    "### Mathematical Formulation\n",
    "For a single neuron with inputs `X = [x‚ÇÅ, x‚ÇÇ, ..., x‚Çô]` and corresponding weights `W = [w‚ÇÅ, w‚ÇÇ, ..., w‚Çô]`, the process is as follows:\n",
    "\n",
    "1.  **Compute the Linear Combination**: The neuron first computes a weighted sum of its inputs and adds a bias term `b`. This is often called the *logit* or *pre-activation*.\n",
    "    `z = (w‚ÇÅ*x‚ÇÅ + w‚ÇÇ*x‚ÇÇ + ... + w‚Çô*x‚Çô) + b`\n",
    "\n",
    "2.  **Apply the Activation Function**: The result `z` is then passed through an activation function to produce the final output.\n",
    "    `output = activation(z)`\n",
    "\n",
    "This can be expressed more concisely using vector notation, which is how it's implemented in libraries like NumPy and PyTorch for efficiency:\n",
    "\n",
    "`output = activation( W ¬∑ X + b )`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3fe445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- A Simple Perceptron Implemented from Scratch using NumPy ---\n",
    "# Building a perceptron from scratch is a great way to understand the core mechanics\n",
    "# before moving to a high-level framework like PyTorch.\n",
    "class Perceptron:\n",
    "    \"\"\"A simple implementation of a single neuron (perceptron) using NumPy.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int):\n",
    "        \"\"\"\n",
    "        Initializes the perceptron's parameters: weights and bias.\n",
    "        \n",
    "        Args:\n",
    "            input_size (int): The number of input features the perceptron will accept.\n",
    "        \"\"\"\n",
    "        # The weights and bias are the learnable parameters. We initialize them randomly.\n",
    "        # The shape of the weights must match the number of input features.\n",
    "        self.weights = np.random.randn(input_size)\n",
    "        # The bias is a single scalar value.\n",
    "        self.bias = np.random.randn()\n",
    "    \n",
    "    def _sigmoid(self, x: float) -> float:\n",
    "        \"\"\"\n",
    "        The Sigmoid activation function. It squashes any input value into a range between 0 and 1.\n",
    "        This is useful for interpreting the output as a probability.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def forward(self, X: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Performs the forward pass: computes the weighted sum and applies the activation function.\n",
    "        \n",
    "        Args:\n",
    "            X (np.ndarray): A NumPy array representing the input features.\n",
    "            \n",
    "        Returns:\n",
    "            float: The output of the perceptron, a value between 0 and 1.\n",
    "        \"\"\"\n",
    "        # 1. Compute the linear combination (weighted sum + bias).\n",
    "        # `np.dot` calculates the dot product between the input features and the weights.\n",
    "        linear_combination = np.dot(X, self.weights) + self.bias\n",
    "        \n",
    "        # 2. Apply the activation function to the linear combination.\n",
    "        return self._sigmoid(linear_combination)\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> int:\n",
    "        \"\"\"Makes a binary prediction (0 or 1) based on the forward pass output.\"\"\"\n",
    "        # The standard threshold for a sigmoid output is 0.5.\n",
    "        # If the output probability is 0.5 or greater, we predict class 1; otherwise, class 0.\n",
    "        return 1 if self.forward(X) >= 0.5 else 0\n",
    "\n",
    "# --- Test the Perceptron ---\n",
    "# Create a perceptron that accepts 2 input features.\n",
    "perceptron = Perceptron(input_size=2)\n",
    "test_input = np.array([0.7, 0.2])\n",
    "\n",
    "# Perform a forward pass to get the raw output probability.\n",
    "output_value = perceptron.forward(test_input)\n",
    "# Make a final binary prediction based on the output.\n",
    "prediction = perceptron.predict(test_input)\n",
    "\n",
    "print(\"üîµ Single Perceptron Test (from Scratch)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Input Features: {test_input}\")\n",
    "print(f\"Initialized Weights: {perceptron.weights}\")\n",
    "print(f\"Initialized Bias: {perceptron.bias:.4f}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Output (Probability): {output_value:.4f}\")\n",
    "print(f\"Final Prediction (Threshold at 0.5): {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5eb707",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Activation Functions: Introducing Non-Linearity\n",
    "\n",
    "Activation functions are a critical component of any neural network. Their primary purpose is to introduce **non-linearity** into the model.\n",
    "\n",
    "**Why is non-linearity so important?**\n",
    "Without a non-linear activation function, a neural network, no matter how many layers it has, would behave just like a single-layer linear model. Each layer would just be performing a linear transformation (matrix multiplication). A series of linear transformations can be collapsed into a single, equivalent linear transformation. This means the network would only be able to learn linear relationships in the data, which is far too restrictive for complex, real-world problems like image recognition or natural language understanding.\n",
    "\n",
    "By introducing non-linearity, activation functions allow the network to learn much more complex, curved decision boundaries and model intricate patterns in the data.\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-c5689868\" language=\"python\">\n",
    "# --- Visualizing Common Activation Functions ---\n",
    "# Generate a range of input values to see how each function behaves.\n",
    "x = np.linspace(-6, 6, 100)\n",
    "\n",
    "# --- Calculate the output of each activation function ---\n",
    "# Sigmoid: Squashes values to a (0, 1) range.\n",
    "sigmoid = 1 / (1 + np.exp(-x))\n",
    "# Tanh (Hyperbolic Tangent): Squashes values to a (-1, 1) range.\n",
    "tanh = np.tanh(x)\n",
    "# ReLU (Rectified Linear Unit): The most common activation for hidden layers. It's simply max(0, x).\n",
    "relu = np.maximum(0, x)\n",
    "# Leaky ReLU: An improvement on ReLU that allows a small, non-zero gradient for negative inputs.\n",
    "leaky_relu = np.where(x > 0, x, 0.1 * x) # Using a common alpha (slope for negative values) of 0.1\n",
    "\n",
    "# --- Plotting ---\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "fig.suptitle('Common Activation Functions', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Sigmoid Plot\n",
    "axes[0, 0].plot(x, sigmoid, color='blue', lw=2)\n",
    "axes[0, 0].set_title('Sigmoid', fontweight='bold')\n",
    "axes[0, 0].text(-5.5, 0.8, 'Range: (0, 1)\\nUse: Output layer for binary classification', bbox=dict(facecolor='blue', alpha=0.1))\n",
    "axes[0, 0].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Tanh Plot\n",
    "axes[0, 1].plot(x, tanh, color='green', lw=2)\n",
    "axes[0, 1].set_title('Tanh', fontweight='bold')\n",
    "axes[0, 1].text(-5.5, 0.7, 'Range: (-1, 1)\\nUse: Hidden layers (often better than sigmoid)', bbox=dict(facecolor='green', alpha=0.1))\n",
    "axes[0, 1].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# ReLU Plot\n",
    "axes[1, 0].plot(x, relu, color='red', lw=2)\n",
    "axes[1, 0].set_title('ReLU', fontweight='bold')\n",
    "axes[1, 0].text(-5.5, 5, 'Range: [0, ‚àû)\\nUse: Most common for hidden layers', bbox=dict(facecolor='red', alpha=0.1))\n",
    "axes[1, 0].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Leaky ReLU Plot\n",
    "axes[1, 1].plot(x, leaky_relu, color='purple', lw=2)\n",
    "axes[1, 1].set_title('Leaky ReLU', fontweight='bold')\n",
    "axes[1, 1].text(-5.5, 4, 'Range: (-‚àû, ‚àû)\\nUse: Fixes the \"dying ReLU\" problem', bbox=dict(facecolor='purple', alpha=0.1))\n",
    "axes[1, 1].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Common formatting for all subplots\n",
    "for ax in axes.flat:\n",
    "    ax.axhline(0, color='black', linewidth=0.5)\n",
    "    ax.axvline(0, color='black', linewidth=0.5)\n",
    "    ax.set_xlabel('Input (z)')\n",
    "    ax.set_ylabel('Output')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Key Properties & Use Cases ---\")\n",
    "print(\"üîπ Sigmoid: Historically popular, but now mostly used in the output layer of a binary classifier to produce a probability.\")\n",
    "print(\"üîπ Tanh: Often preferred over sigmoid for hidden layers because it is zero-centered, which can help with optimization during training.\")\n",
    "print(\"üîπ ReLU: The default choice for hidden layers. It's computationally very efficient and helps mitigate the 'vanishing gradient' problem. Its main drawback is the 'dying ReLU' problem where neurons can become inactive.\")\n",
    "print(\"üîπ Leaky ReLU: An improvement over ReLU that allows a small, non-zero gradient when the unit is not active, preventing neurons from 'dying'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f29e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualizing Common Activation Functions ---\n",
    "# Generate a range of input values.\n",
    "x = np.linspace(-6, 6, 100)\n",
    "\n",
    "# Calculate the output of each activation function.\n",
    "sigmoid = 1 / (1 + np.exp(-x))\n",
    "tanh = np.tanh(x)\n",
    "relu = np.maximum(0, x)\n",
    "leaky_relu = np.where(x > 0, x, 0.1 * x) # Using a common alpha of 0.1\n",
    "\n",
    "# --- Plotting ---\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "fig.suptitle('Common Activation Functions', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Sigmoid\n",
    "axes[0, 0].plot(x, sigmoid, color='blue')\n",
    "axes[0, 0].set_title('Sigmoid', fontweight='bold')\n",
    "axes[0, 0].text(-5.5, 0.8, 'Range: (0, 1)\\nUse: Output layer for binary classification', bbox=dict(facecolor='blue', alpha=0.1))\n",
    "axes[0, 0].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Tanh (Hyperbolic Tangent)\n",
    "axes[0, 1].plot(x, tanh, color='green')\n",
    "axes[0, 1].set_title('Tanh', fontweight='bold')\n",
    "axes[0, 1].text(-5.5, 0.7, 'Range: (-1, 1)\\nUse: Hidden layers (often better than sigmoid)', bbox=dict(facecolor='green', alpha=0.1))\n",
    "axes[0, 1].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# ReLU (Rectified Linear Unit)\n",
    "axes[1, 0].plot(x, relu, color='red')\n",
    "axes[1, 0].set_title('ReLU', fontweight='bold')\n",
    "axes[1, 0].text(-5.5, 5, 'Range: [0, ‚àû)\\nUse: Most common for hidden layers', bbox=dict(facecolor='red', alpha=0.1))\n",
    "axes[1, 0].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Leaky ReLU\n",
    "axes[1, 1].plot(x, leaky_relu, color='purple')\n",
    "axes[1, 1].set_title('Leaky ReLU', fontweight='bold')\n",
    "axes[1, 1].text(-5.5, 4, 'Range: (-‚àû, ‚àû)\\nUse: Fixes the \"dying ReLU\" problem', bbox=dict(facecolor='purple', alpha=0.1))\n",
    "axes[1, 1].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.axhline(0, color='black', linewidth=0.5)\n",
    "    ax.axvline(0, color='black', linewidth=0.5)\n",
    "    ax.set_xlabel('Input (z)')\n",
    "    ax.set_ylabel('Output')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Key Properties ---\")\n",
    "print(\"üîπ Sigmoid: Squashes values into a (0, 1) range, making it suitable for output probabilities.\")\n",
    "print(\"üîπ Tanh: Zero-centered (output ranges from -1 to 1), which can help with optimization.\")\n",
    "print(\"üîπ ReLU: Computationally efficient and helps mitigate the vanishing gradient problem. The default choice for hidden layers.\")\n",
    "print(\"üîπ Leaky ReLU: An improvement over ReLU that allows a small, non-zero gradient when the unit is not active, preventing 'dying neurons'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88faecf9",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Building a Neural Network with PyTorch\n",
    "\n",
    "While building a perceptron from scratch with NumPy is insightful, it's not practical for building the large, complex networks used in modern AI. High-level frameworks like **PyTorch** and **TensorFlow** provide optimized, pre-built components that make it much easier to build, train, and deploy deep learning models.\n",
    "\n",
    "### The `nn.Module` Class: The Heart of PyTorch Models\n",
    "\n",
    "In PyTorch, every neural network you build should be a subclass of `nn.Module`. This base class provides a tremendous amount of functionality:\n",
    "-   It tracks all the layers and sub-modules of your network.\n",
    "-   It manages the model's parameters (`weights` and `biases`), making it easy to access them and pass them to an optimizer.\n",
    "-   It provides helper methods for moving the model to different devices (like a GPU), saving and loading the model, and switching between training and evaluation modes.\n",
    "\n",
    "### A Simple Multi-Layer Perceptron (MLP)\n",
    "\n",
    "Let's build a simple network with one hidden layer to classify data. This is often called a **Multi-Layer Perceptron (MLP)**. We will define the layers in the `__init__` method and specify how data flows through them in the `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc5731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Defining a Neural Network using PyTorch's nn.Module ---\n",
    "class SimpleMLP(nn.Module):\n",
    "    \"\"\"A simple Multi-Layer Perceptron with one hidden layer, built using PyTorch.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        \"\"\"\n",
    "        Defines the architecture of the network by specifying its layers.\n",
    "        \n",
    "        Args:\n",
    "            input_size (int): The number of features in the input data (e.g., 2 for the XOR problem).\n",
    "            hidden_size (int): The number of neurons in the hidden layer. This is a hyperparameter you can tune.\n",
    "            output_size (int): The number of output neurons (e.g., 1 for binary classification).\n",
    "        \"\"\"\n",
    "        # `super().__init__()` must be called first to initialize the parent `nn.Module` class.\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        \n",
    "        # `nn.Sequential` is a container that holds a sequence of layers.\n",
    "        # Data passed to it will flow through each layer in the defined order.\n",
    "        self.layers = nn.Sequential(\n",
    "            # 1. First Linear Layer: Maps from the input layer to the hidden layer.\n",
    "            # `nn.Linear(in_features, out_features)` creates a fully connected layer.\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            \n",
    "            # 2. Activation Function: Introduce non-linearity after the first linear layer.\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # 3. Second Linear Layer: Maps from the hidden layer to the output layer.\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            \n",
    "            # 4. Output Activation: Sigmoid is used to squash the output to a [0, 1] probability.\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the network. This method is automatically called\n",
    "        when you pass data to an instance of the model (e.g., `model(input_data)`).\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor (predictions).\n",
    "        \"\"\"\n",
    "        return self.layers(x)\n",
    "\n",
    "# --- Instantiate and Inspect the Model ---\n",
    "# Create an instance of our network.\n",
    "# It will take 2 input features, has a hidden layer with 8 neurons, and produces 1 output.\n",
    "model = SimpleMLP(input_size=2, hidden_size=8, output_size=1)\n",
    "\n",
    "print(\"üß† Neural Network Architecture (SimpleMLP)\")\n",
    "print(\"=\" * 50)\n",
    "# Printing the model object gives a nice summary of its architecture.\n",
    "print(model)\n",
    "\n",
    "# --- Count the Learnable Parameters ---\n",
    "# This is a good practice to understand the complexity (and memory footprint) of your model.\n",
    "# `p.numel()` returns the number of elements in a tensor.\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal learnable parameters: {total_params}\")\n",
    "# Calculation:\n",
    "# Layer 1 weights: 2 * 8 = 16\n",
    "# Layer 1 bias: 8\n",
    "# Layer 2 weights: 8 * 1 = 8\n",
    "# Layer 2 bias: 1\n",
    "# Total: 16 + 8 + 8 + 1 = 33\n",
    "\n",
    "# --- Test the Forward Pass with a Dummy Input ---\n",
    "# Create a dummy input tensor of shape (batch_size, num_features)\n",
    "test_input = torch.tensor([[0.5, -1.2]], dtype=torch.float32)\n",
    "# Pass the input through the model to get an output.\n",
    "output = model(test_input)\n",
    "print(f\"\\nTest input: {test_input.numpy()}\")\n",
    "print(f\"Model output (probability): {output.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989f426d",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Training a Neural Network: The XOR Problem\n",
    "\n",
    "The **XOR (exclusive OR)** problem is a classic \"Hello, World!\" for neural networks. It's a binary classification task that holds historical significance because it demonstrated the limitations of single-layer perceptrons and the necessity of multi-layer networks.\n",
    "\n",
    "The core issue is that the XOR data is **not linearly separable**. You cannot draw a single straight line to separate the two classes (the 0s and the 1s). This makes it impossible for a single perceptron to solve. However, a multi-layer network with a non-linear activation function can learn the complex, non-linear decision boundary required to solve it perfectly.\n",
    "\n",
    "### The XOR Logic Table:\n",
    "- `0 XOR 0 = 0`\n",
    "- `0 XOR 1 = 1`\n",
    "- `1 XOR 0 = 1`\n",
    "- `1 XOR 1 = 0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cea4f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- XOR Dataset Definition ---\n",
    "# Input features for the XOR problem, represented as a PyTorch tensor.\n",
    "X_xor = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "# Corresponding target labels for the XOR problem.\n",
    "y_xor = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "print(\"‚ö° The XOR Problem Dataset\")\n",
    "print(\"=\" * 30)\n",
    "print(\" Input (X)  | Output (y)\")\n",
    "print(\"------------|------------\")\n",
    "for i in range(len(X_xor)):\n",
    "    print(f\" {X_xor[i].numpy()} |     {int(y_xor[i].item())}\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# --- Visualize the XOR Data ---\n",
    "plt.figure(figsize=(6, 5))\n",
    "# Plot points belonging to Class 0 (y=0) in red.\n",
    "plt.scatter(X_xor[y_xor.squeeze() == 0, 0], X_xor[y_xor.squeeze() == 0, 1], c='red', s=200, label='Class 0', edgecolors='k', alpha=0.8)\n",
    "# Plot points belonging to Class 1 (y=1) in blue.\n",
    "plt.scatter(X_xor[y_xor.squeeze() == 1, 0], X_xor[y_xor.squeeze() == 1, 1], c='blue', s=200, label='Class 1', edgecolors='k', alpha=0.8)\n",
    "\n",
    "plt.title('The XOR Problem (Not Linearly Separable)', fontweight='bold')\n",
    "plt.xlabel('Input Feature 1')\n",
    "plt.ylabel('Input Feature 2')\n",
    "plt.xticks([0, 1])\n",
    "plt.yticks([0, 1])\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a5180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model, Loss, and Optimizer Setup ---\n",
    "# 1. Instantiate the model with a hidden layer of 8 neurons.\n",
    "model_xor = SimpleMLP(input_size=2, hidden_size=8, output_size=1)\n",
    "\n",
    "# 2. Define the Loss Function. Binary Cross-Entropy (BCE) is the standard choice for\n",
    "#    binary classification problems. It measures the difference between the predicted\n",
    "#    probability and the actual label.\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 3. Define the Optimizer. The optimizer's job is to update the model's parameters\n",
    "#    based on the gradients computed during backpropagation. Adam is a popular,\n",
    "#    robust, and effective choice that often works well with default settings.\n",
    "#    We pass the model's parameters (`model_xor.parameters()`) to the optimizer\n",
    "#    so it knows which tensors to update. `lr` is the learning rate.\n",
    "optimizer = optim.Adam(model_xor.parameters(), lr=0.1)\n",
    "\n",
    "# --- The Training Loop ---\n",
    "# This is the core of the training process. We will iterate over the data multiple times (epochs).\n",
    "epochs = 200\n",
    "losses = []\n",
    "\n",
    "print(\"\\nüîÑ Training the network to solve the XOR problem...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # --- The 5 Core Steps of a Training Iteration ---\n",
    "    \n",
    "    # 1. Forward Pass: Pass the input data through the model to get predictions.\n",
    "    y_pred = model_xor(X_xor)\n",
    "    \n",
    "    # 2. Compute Loss: Compare the model's predictions (y_pred) with the true labels (y_xor)\n",
    "    #    using the defined loss function.\n",
    "    loss = criterion(y_pred, y_xor)\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # 3. Zero Gradients: Before the backward pass, we must clear the gradients from the\n",
    "    #    previous iteration. If we don't, gradients will accumulate.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 4. Backward Pass (Backpropagation): This is where PyTorch automatically computes the\n",
    "    #    gradient of the loss with respect to each of the model's parameters.\n",
    "    loss.backward()\n",
    "    \n",
    "    # 5. Update Weights: The optimizer uses the computed gradients to update the model's\n",
    "    #    weights and biases in the direction that minimizes the loss.\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print progress periodically to monitor training.\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch [{epoch+1: >3}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Training Complete!\")\n",
    "\n",
    "# --- Evaluate the Trained Model ---\n",
    "# After training, we evaluate the model's performance on the same data.\n",
    "# `torch.no_grad()` is a context manager that disables gradient calculation.\n",
    "# This is important for inference as it reduces memory consumption and speeds up computation.\n",
    "with torch.no_grad():\n",
    "    predictions = model_xor(X_xor)\n",
    "    # Convert the output probabilities to binary class predictions (0 or 1).\n",
    "    predicted_classes = (predictions >= 0.5).float()\n",
    "\n",
    "print(\"\\nüéØ Final Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(\" Input    | True | Predicted | Probability\")\n",
    "print(\"----------|------|-----------|-------------\")\n",
    "for i in range(len(X_xor)):\n",
    "    print(f\" {X_xor[i].numpy()} |  {int(y_xor[i].item())}   |     {int(predicted_classes[i].item())}     |   {predictions[i].item():.4f}\")\n",
    "\n",
    "# Calculate the final accuracy of the model.\n",
    "accuracy = (predicted_classes == y_xor).float().mean()\n",
    "print(f\"\\nFinal Accuracy: {accuracy.item() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85a53d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plotting the Training Loss ---\n",
    "# Visualizing the loss is a crucial step in diagnosing the training process.\n",
    "# A decreasing loss curve indicates that the model is learning.\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, color='purple', lw=2)\n",
    "plt.title('Training Loss Over Epochs', fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('BCE Loss')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773c75a3",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Real-World Example: Manufacturing Defect Classification\n",
    "\n",
    "Now, let's apply these concepts to a more realistic problem. Imagine we are building a **Manufacturing Copilot**, an AI assistant designed to monitor a production line in real-time. A key feature of this copilot is its ability to predict whether a manufactured part is defective based on sensor readings, allowing for immediate quality control.\n",
    "\n",
    "### The Dataset\n",
    "We'll use a synthetic dataset that mimics this scenario. Each data point represents a manufactured part, and the features are two sensor readings:\n",
    "-   `temperature_deviation`: How much the part's temperature varied from the optimal setting during production.\n",
    "-   `pressure_deviation`: How much the pressure varied from the optimal setting.\n",
    "\n",
    "The data is intentionally designed to be **non-linearly separable**, forming two concentric circles.\n",
    "-   The **inner circle** (Class 0) represents non-defective parts.\n",
    "-   The **outer ring** (Class 1) represents defective parts.\n",
    "\n",
    "This is a classic \"toy\" problem that is more complex than XOR and effectively mimics real-world scenarios where simple linear models would fail completely, but a neural network can excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de26401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate Synthetic Manufacturing Data ---\n",
    "# `make_circles` is a function from scikit-learn that is perfect for creating a dataset\n",
    "# where the classes are not linearly separable.\n",
    "X, y = make_circles(n_samples=1000, noise=0.1, factor=0.5, random_state=SEED)\n",
    "\n",
    "# Using a Pandas DataFrame provides better context and makes visualization easier.\n",
    "df = pd.DataFrame(X, columns=['temperature_deviation', 'pressure_deviation'])\n",
    "df['is_defective'] = y\n",
    "\n",
    "print(\"üè≠ Synthetic Manufacturing Dataset\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Defective parts (Class 1): {df['is_defective'].sum()}\")\n",
    "print(f\"Non-defective parts (Class 0): {len(df) - df['is_defective'].sum()}\")\n",
    "print(f\"Defect rate: {df['is_defective'].mean():.1%}\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Sample Data:\")\n",
    "print(df.head())\n",
    "\n",
    "# --- Visualize the Data ---\n",
    "plt.figure(figsize=(8, 7))\n",
    "sns.scatterplot(\n",
    "    data=df,\n",
    "    x='temperature_deviation',\n",
    "    y='pressure_deviation',\n",
    "    hue='is_defective',\n",
    "    palette={0: 'green', 1: 'red'},\n",
    "    style='is_defective',\n",
    "    markers={0: 'o', 1: 'X'},\n",
    "    s=100,\n",
    "    alpha=0.8\n",
    ")\n",
    "plt.title('Manufacturing Data: Temperature vs. Pressure Deviations', fontweight='bold')\n",
    "plt.xlabel('Temperature Deviation (Normalized)')\n",
    "plt.ylabel('Pressure Deviation (Normalized)')\n",
    "plt.legend(title='Status', labels=['Non-defective', 'Defective'])\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4626140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Train-Test Split ---\n",
    "# We split the data into a training set (for model training) and a testing set (for unbiased evaluation).\n",
    "# `test_size=0.2` means 20% of the data will be reserved for testing.\n",
    "# `stratify=y` is crucial for classification problems, especially with imbalanced datasets. It ensures\n",
    "# that the proportion of each class is the same in both the training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED, stratify=y)\n",
    "\n",
    "# --- Step 2: Feature Scaling ---\n",
    "# Neural networks are sensitive to the scale of input features. Features with larger ranges can\n",
    "# dominate the learning process. `StandardScaler` standardizes features by removing the mean\n",
    "# and scaling to unit variance.\n",
    "scaler = StandardScaler()\n",
    "# We `fit_transform` on the training data to learn the scaling parameters (mean and std).\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "# We only `transform` the test data using the *same* scaler fitted on the training data.\n",
    "# This prevents data leakage from the test set into the training process.\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Step 3: Convert to PyTorch Tensors ---\n",
    "# PyTorch models operate on tensors. We convert our NumPy arrays to PyTorch tensors.\n",
    "# We also move the data to the configured device (CPU or GPU) to enable hardware acceleration.\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1).to(device) # `unsqueeze(1)` adds a dimension to match model output\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test).unsqueeze(1).to(device)\n",
    "\n",
    "print(\"‚úÖ Data Preprocessing Complete\")\n",
    "print(f\"Training samples: {len(X_train_tensor)}\")\n",
    "print(f\"Testing samples: {len(X_test_tensor)}\")\n",
    "print(f\"Data tensors are on device: '{X_train_tensor.device}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a16c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- A Deeper Network for a More Complex Problem ---\n",
    "# For this non-linear problem, a single hidden layer might not be enough.\n",
    "# We'll build a slightly deeper network with two hidden layers to give it more\n",
    "# capacity to learn the complex circular decision boundary.\n",
    "class ManufacturingClassifier(nn.Module):\n",
    "    \"\"\"A neural network designed to classify manufacturing defects based on sensor data.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(ManufacturingClassifier, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            # Layer 1: Input (2 features) to Hidden Layer 1 (16 neurons)\n",
    "            nn.Linear(2, 16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Layer 2: Hidden Layer 1 (16 neurons) to Hidden Layer 2 (16 neurons)\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Layer 3: Hidden Layer 2 (16 neurons) to Output Layer (1 neuron)\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid() # Sigmoid for binary classification output\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# --- Instantiate the Model and Move to the Configured Device ---\n",
    "model_manufacturing = ManufacturingClassifier().to(device)\n",
    "\n",
    "print(\"üß† Manufacturing Defect Classifier Architecture\")\n",
    "print(\"=\" * 50)\n",
    "print(model_manufacturing)\n",
    "\n",
    "# Count the learnable parameters to understand model complexity.\n",
    "total_params = sum(p.numel() for p in model_manufacturing.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal learnable parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6d3ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Setup ---\n",
    "# Use the same loss function and optimizer as before.\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model_manufacturing.parameters(), lr=0.01)\n",
    "\n",
    "# --- Full Training and Evaluation Loop ---\n",
    "epochs = 500\n",
    "train_losses, test_losses = [], []\n",
    "train_accuracies, test_accuracies = [], []\n",
    "\n",
    "print(\"\\nüîÑ Training Manufacturing Defect Classifier...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # --- Training Phase ---\n",
    "    model_manufacturing.train() # Set the model to training mode. This enables features like dropout.\n",
    "    \n",
    "    # Forward pass: get predictions on the training data.\n",
    "    y_train_pred = model_manufacturing(X_train_tensor)\n",
    "    \n",
    "    # Calculate loss.\n",
    "    train_loss = criterion(y_train_pred, y_train_tensor)\n",
    "    \n",
    "    # Backward pass and optimization.\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # --- Evaluation Phase ---\n",
    "    # It's good practice to evaluate the model on both training and test data at each epoch\n",
    "    # to monitor for overfitting.\n",
    "    model_manufacturing.eval() # Set the model to evaluation mode. This disables features like dropout.\n",
    "    with torch.no_grad(): # Disable gradient computation for evaluation to save memory and speed up.\n",
    "        # Calculate training accuracy\n",
    "        train_preds_class = (y_train_pred >= 0.5).float()\n",
    "        train_acc = (train_preds_class == y_train_tensor).float().mean().item()\n",
    "        \n",
    "        # Make predictions on the test data\n",
    "        y_test_pred = model_manufacturing(X_test_tensor)\n",
    "        # Calculate test loss\n",
    "        test_loss = criterion(y_test_pred, y_test_tensor)\n",
    "        # Calculate test accuracy\n",
    "        test_preds_class = (y_test_pred >= 0.5).float()\n",
    "        test_acc = (test_preds_class == y_test_tensor).float().mean().item()\n",
    "    \n",
    "    # Record metrics for this epoch to plot later.\n",
    "    train_losses.append(train_loss.item())\n",
    "    test_losses.append(test_loss.item())\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "    \n",
    "    # Print progress periodically.\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch [{epoch+1: >3}/{epochs}] | \"\n",
    "              f\"Train Loss: {train_loss.item():.4f}, Train Acc: {train_acc:.2%} | \"\n",
    "              f\"Test Loss: {test_loss.item():.4f}, Test Acc: {test_acc:.2%}\")\n",
    "\n",
    "print(\"\\n‚úÖ Training Complete!\")\n",
    "print(f\"Final Test Accuracy: {test_accuracies[-1]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213d9e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plotting Training and Evaluation Metrics ---\n",
    "# Visualizing metrics is essential for understanding the training process.\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Loss vs. Epochs\n",
    "# We expect both training and test loss to decrease over time.\n",
    "# A large gap between the two curves can indicate overfitting.\n",
    "ax1.plot(train_losses, label='Training Loss', color='blue', lw=2)\n",
    "ax1.plot(test_losses, label='Test Loss', color='orange', linestyle='--', lw=2)\n",
    "ax1.set_title('Loss Over Epochs', fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('BCE Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Plot 2: Accuracy vs. Epochs\n",
    "# We expect both accuracies to increase over time.\n",
    "ax2.plot(train_accuracies, label='Training Accuracy', color='blue', lw=2)\n",
    "ax2.plot(test_accuracies, label='Test Accuracy', color='orange', linestyle='--', lw=2)\n",
    "ax2.set_title('Accuracy Over Epochs', fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee540bc",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Visualizing the Decision Boundary\n",
    "\n",
    "A **decision boundary** is a hypersurface that partitions the underlying vector space into two or more sets, one for each class. In simpler terms, it's the line or surface that the model \"learns\" to separate the different classes.\n",
    "\n",
    "Visualizing the decision boundary is an excellent way to understand *how* the model is making its decisions and to intuitively assess its performance. For our non-linear, circular dataset, we hope to see a circular or near-circular boundary that correctly separates the green \"non-defective\" points from the red \"defective\" points. A linear model would only be able to draw a straight line, which would fail miserably at this task.\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-f813091b\" language=\"python\">\n",
    "def plot_decision_boundary(model, X, y, scaler):\n",
    "    \"\"\"\n",
    "    Plots the decision boundary of a trained PyTorch model on a 2D dataset.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The trained PyTorch model.\n",
    "        X (torch.Tensor): The input features (unscaled).\n",
    "        y (torch.Tensor): The true labels.\n",
    "        scaler (StandardScaler): The scaler used to transform the training data.\n",
    "    \"\"\"\n",
    "    # Move model and data to CPU for plotting, as Matplotlib works with NumPy arrays.\n",
    "    model.to('cpu')\n",
    "    X, y = X.to('cpu'), y.to('cpu')\n",
    "    \n",
    "    # 1. Create a mesh grid of points to cover the entire feature space.\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                         np.arange(y_min, y_max, 0.02))\n",
    "    \n",
    "    # 2. Predict the class for each point in the mesh grid.\n",
    "    # `np.c_` concatenates the flattened xx and yy arrays to create a list of points.\n",
    "    mesh_data = np.c_[xx.ravel(), yy.ravel()]\n",
    "    # Important: We must scale this new mesh data using the *same scaler* that was fitted on the training data.\n",
    "    mesh_data_scaled = scaler.transform(mesh_data)\n",
    "    mesh_tensor = torch.FloatTensor(mesh_data_scaled)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get the model's predictions for every point on the grid.\n",
    "        Z = model(mesh_tensor).numpy()\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # 3. Plot the contour and the actual data points.\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    # `contourf` creates a filled contour plot. This will color the background\n",
    "    # based on the model's predicted probability for each point.\n",
    "    contour = plt.contourf(xx, yy, Z, cmap='RdYlGn', alpha=0.7)\n",
    "    plt.colorbar(contour, label='Probability of being Defective (Class 1)')\n",
    "    \n",
    "    # Overlay the actual data points from the test set to see how well the model did.\n",
    "    sns.scatterplot(\n",
    "        x=X[:, 0], \n",
    "        y=X[:, 1], \n",
    "        hue=y.squeeze(), \n",
    "        palette={0: 'green', 1: 'red'},\n",
    "        style=y.squeeze(),\n",
    "        markers={0: 'o', 1: 'X'},\n",
    "        s=100,\n",
    "        edgecolor='k'\n",
    "    )\n",
    "    \n",
    "    plt.title('Neural Network Decision Boundary on Test Data', fontweight='bold')\n",
    "    plt.xlabel('Temperature Deviation (Normalized)')\n",
    "    plt.ylabel('Pressure Deviation (Normalized)')\n",
    "    plt.legend(title='Actual Class', labels=['Non-defective', 'Defective'])\n",
    "    plt.show()\n",
    "\n",
    "# --- Visualize the boundary on the test set ---\n",
    "# We pass the unscaled test data for plotting, as the scaling is handled inside the function.\n",
    "plot_decision_boundary(model_manufacturing, torch.FloatTensor(X_test), y_test_tensor.cpu(), scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad4b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, scaler):\n",
    "    \"\"\"\n",
    "    Plots the decision boundary of a trained model on a 2D dataset.\n",
    "    \"\"\"\n",
    "    # Move model and data to CPU for plotting\n",
    "    model.to('cpu')\n",
    "    X, y = X.to('cpu'), y.to('cpu')\n",
    "    \n",
    "    # 1. Create a mesh grid of points\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                         np.arange(y_min, y_max, 0.02))\n",
    "    \n",
    "    # 2. Predict the class for each point in the mesh\n",
    "    mesh_data = np.c_[xx.ravel(), yy.ravel()]\n",
    "    # Important: Scale the mesh data using the same scaler as the training data\n",
    "    mesh_data_scaled = scaler.transform(mesh_data)\n",
    "    mesh_tensor = torch.FloatTensor(mesh_data_scaled)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        Z = model(mesh_tensor).numpy()\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # 3. Plot the contour and the data points\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    # The contour plot shows the model's predicted probability for each point.\n",
    "    contour = plt.contourf(xx, yy, Z, cmap='RdYlGn', alpha=0.7)\n",
    "    plt.colorbar(contour, label='Probability of Defect (Class 1)')\n",
    "    \n",
    "    # Overlay the actual data points from the test set.\n",
    "    sns.scatterplot(\n",
    "        x=X[:, 0], \n",
    "        y=X[:, 1], \n",
    "        hue=y.squeeze(), \n",
    "        palette={0: 'green', 1: 'red'},\n",
    "        style=y.squeeze(),\n",
    "        markers={0: 'o', 1: 'X'},\n",
    "        s=100,\n",
    "        edgecolor='k'\n",
    "    )\n",
    "    \n",
    "    plt.title('Neural Network Decision Boundary on Test Data', fontweight='bold')\n",
    "    plt.xlabel('Temperature Deviation (Normalized)')\n",
    "    plt.ylabel('Pressure Deviation (Normalized)')\n",
    "    plt.legend(title='Actual Class', labels=['Non-defective', 'Defective'])\n",
    "    plt.show()\n",
    "\n",
    "# --- Visualize the boundary on the test set ---\n",
    "# We pass the unscaled test data for plotting, as the scaler is handled inside the function.\n",
    "plot_decision_boundary(model_manufacturing, torch.FloatTensor(X_test), y_test_tensor.cpu(), scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b073f239",
   "metadata": {},
   "source": [
    "## üéâ Summary & Key Takeaways\n",
    "\n",
    "Congratulations! You have successfully built, trained, and evaluated your first neural networks using PyTorch. You've progressed from a theoretical understanding to practical implementation, tackling both a classic computer science problem (XOR) and a more realistic, industry-inspired example (manufacturing defect detection).\n",
    "\n",
    "### Core Concepts You've Mastered:\n",
    "-   **Neural Network Architecture**: You now understand how layers, weights, biases, and activation functions are combined to create a complete network. You appreciate the critical role of non-linear activations.\n",
    "-   **The Training Trinity**: You have seen firsthand how the **loss function** (measuring error), the **optimizer** (updating weights), and **backpropagation** (calculating gradients) work together in a cycle to enable a model to learn from data.\n",
    "-   **PyTorch Fundamentals**: You are now equipped to define a custom neural network using `nn.Module`, set up a complete training loop, and use your model to make predictions on new data.\n",
    "-   **Non-Linearity is Key**: You've witnessed how multi-layer networks with non-linear activations (like ReLU) can learn complex, curved decision boundaries to solve problems that are impossible for simpler linear models.\n",
    "\n",
    "### What You Built:\n",
    "1.  **üîµ A Perceptron from Scratch**: To deeply understand the fundamental mechanics of a single neuron.\n",
    "2.  **üß† A Multi-Layer Perceptron in PyTorch**: To solve the classic, non-linearly separable XOR problem and prove the power of multi-layer architectures.\n",
    "3.  **üè≠ A Manufacturing Defect Classifier**: A deeper, more practical network designed to solve a complex classification task, complete with data preprocessing, training/evaluation loops, and visualization of its learned decision boundary.\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Next Steps\n",
    "\n",
    "With this solid grasp of fundamental neural network concepts, you are now fully prepared to explore the more specialized and powerful architectures that are used for specific data types like images and text.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <h3>Neural networks are no longer a black box! You're ready for the next challenge.</h3>\n",
    "    <p>Proceed to <strong>Notebook 02: Convolutional Neural Networks (CNNs)</strong> to learn how to build models that can \"see\" and process images, a critical step towards multi-modal Generative AI.</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
