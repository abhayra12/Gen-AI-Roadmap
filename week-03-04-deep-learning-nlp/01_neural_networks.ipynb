{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9ce9f11",
   "metadata": {},
   "source": [
    "# üß† Notebook 01: Neural Networks Fundamentals\n",
    "\n",
    "**Week 3-4: Deep Learning & NLP Foundations**  \n",
    "**Gen AI Masters Program**\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Objectives\n",
    "\n",
    "By the end of this notebook, you will master:\n",
    "1. ‚úÖ Neural network architecture and components\n",
    "2. ‚úÖ Forward propagation\n",
    "3. ‚úÖ Backpropagation and gradient descent\n",
    "4. ‚úÖ Activation functions\n",
    "5. ‚úÖ Building neural networks with PyTorch\n",
    "6. ‚úÖ Training and evaluation\n",
    "\n",
    "**Estimated Time:** 3-4 hours\n",
    "\n",
    "---\n",
    "\n",
    "## üìö What are Neural Networks?\n",
    "\n",
    "Neural networks are the foundation of deep learning and Gen AI. They're inspired by biological neurons and can learn complex patterns from data.\n",
    "\n",
    "**Key Components:**\n",
    "- üîµ **Neurons**: Basic computational units\n",
    "- üîó **Layers**: Input, Hidden, Output\n",
    "- ‚ö° **Weights & Biases**: Learnable parameters\n",
    "- üéØ **Activation Functions**: Add non-linearity\n",
    "- üìâ **Loss Function**: Measure error\n",
    "- üîÑ **Optimizer**: Update weights\n",
    "\n",
    "Let's dive deep! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d244528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951addac",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ The Perceptron: Building Block of Neural Networks\n",
    "\n",
    "### Single Neuron (Perceptron)\n",
    "\n",
    "A perceptron performs:\n",
    "```\n",
    "output = activation(w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô + b)\n",
    "       = activation(W¬∑X + b)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3fe445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a simple perceptron from scratch\n",
    "class Perceptron:\n",
    "    def __init__(self, input_size):\n",
    "        # Initialize weights and bias randomly\n",
    "        self.weights = np.random.randn(input_size)\n",
    "        self.bias = np.random.randn()\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation function\"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        # Linear combination\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        # Apply activation\n",
    "        return self.sigmoid(z)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make binary prediction\"\"\"\n",
    "        return (self.forward(X) >= 0.5).astype(int)\n",
    "\n",
    "# Test perceptron\n",
    "perceptron = Perceptron(input_size=2)\n",
    "test_input = np.array([0.5, 0.8])\n",
    "output = perceptron.forward(test_input)\n",
    "\n",
    "print(\"üîµ Single Perceptron Test\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Input: {test_input}\")\n",
    "print(f\"Weights: {perceptron.weights}\")\n",
    "print(f\"Bias: {perceptron.bias:.4f}\")\n",
    "print(f\"Output: {output:.4f}\")\n",
    "print(f\"Prediction: {perceptron.predict(test_input)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5eb707",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Activation Functions\n",
    "\n",
    "Activation functions introduce non-linearity, allowing neural networks to learn complex patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f29e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common activation functions\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "# Define activation functions\n",
    "sigmoid = 1 / (1 + np.exp(-x))\n",
    "tanh = np.tanh(x)\n",
    "relu = np.maximum(0, x)\n",
    "leaky_relu = np.where(x > 0, x, 0.01 * x)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Sigmoid\n",
    "axes[0, 0].plot(x, sigmoid, 'b-', linewidth=2)\n",
    "axes[0, 0].set_title('Sigmoid: œÉ(x) = 1/(1+e‚ÅªÀ£)', fontweight='bold', fontsize=12)\n",
    "axes[0, 0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0, 0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_ylabel('Output', fontweight='bold')\n",
    "axes[0, 0].text(0.5, 0.9, 'Range: (0, 1)\\nUse: Binary classification', \n",
    "                transform=axes[0, 0].transAxes, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Tanh\n",
    "axes[0, 1].plot(x, tanh, 'g-', linewidth=2)\n",
    "axes[0, 1].set_title('Tanh: tanh(x)', fontweight='bold', fontsize=12)\n",
    "axes[0, 1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0, 1].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_ylabel('Output', fontweight='bold')\n",
    "axes[0, 1].text(0.5, 0.9, 'Range: (-1, 1)\\nUse: Hidden layers', \n",
    "                transform=axes[0, 1].transAxes, bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "\n",
    "# ReLU\n",
    "axes[1, 0].plot(x, relu, 'r-', linewidth=2)\n",
    "axes[1, 0].set_title('ReLU: max(0, x)', fontweight='bold', fontsize=12)\n",
    "axes[1, 0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[1, 0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_xlabel('Input', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Output', fontweight='bold')\n",
    "axes[1, 0].text(0.5, 0.9, 'Range: [0, ‚àû)\\nUse: Most popular!', \n",
    "                transform=axes[1, 0].transAxes, bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.5))\n",
    "\n",
    "# Leaky ReLU\n",
    "axes[1, 1].plot(x, leaky_relu, 'm-', linewidth=2)\n",
    "axes[1, 1].set_title('Leaky ReLU: max(0.01x, x)', fontweight='bold', fontsize=12)\n",
    "axes[1, 1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[1, 1].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_xlabel('Input', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Output', fontweight='bold')\n",
    "axes[1, 1].text(0.5, 0.9, 'Range: (-‚àû, ‚àû)\\nUse: Fixes dying ReLU', \n",
    "                transform=axes[1, 1].transAxes, bbox=dict(boxstyle='round', facecolor='plum', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Activation Function Properties:\")\n",
    "print(\"=\"*60)\n",
    "print(\"Sigmoid: Squashes values to (0,1), good for probabilities\")\n",
    "print(\"Tanh: Squashes values to (-1,1), zero-centered\")\n",
    "print(\"ReLU: Fast, prevents vanishing gradients, most popular\")\n",
    "print(\"Leaky ReLU: Fixes dying ReLU problem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88faecf9",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Building a Neural Network with PyTorch\n",
    "\n",
    "### Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc5731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        \n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # Input to hidden\n",
    "        self.relu = nn.ReLU()                           # Activation\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)  # Hidden to output\n",
    "        self.sigmoid = nn.Sigmoid()                     # Output activation\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        x = self.fc1(x)      # Linear transformation\n",
    "        x = self.relu(x)     # Non-linear activation\n",
    "        x = self.fc2(x)      # Linear transformation\n",
    "        x = self.sigmoid(x)  # Output activation\n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model = SimpleNN(input_size=2, hidden_size=4, output_size=1)\n",
    "print(\"üß† Neural Network Architecture\")\n",
    "print(\"=\"*50)\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.tensor([[0.5, 0.8]], dtype=torch.float32)\n",
    "output = model(test_input)\n",
    "print(f\"\\nTest input: {test_input.numpy()}\")\n",
    "print(f\"Test output: {output.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989f426d",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Training a Neural Network\n",
    "\n",
    "### XOR Problem (Classic Non-Linear Problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cea4f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR dataset (not linearly separable)\n",
    "X_xor = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "y_xor = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "print(\"‚ö° XOR Problem\")\n",
    "print(\"=\"*50)\n",
    "print(\"Input (X) | Output (y)\")\n",
    "print(\"-\"*50)\n",
    "for i in range(len(X_xor)):\n",
    "    print(f\"  {X_xor[i].numpy()}  |    {int(y_xor[i].item())}\")\n",
    "\n",
    "# Visualize XOR problem\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = ['red' if y.item() == 0 else 'blue' for y in y_xor]\n",
    "plt.scatter(X_xor[:, 0], X_xor[:, 1], c=colors, s=200, edgecolors='black', linewidths=2)\n",
    "plt.title('XOR Problem (Not Linearly Separable)', fontweight='bold', fontsize=14)\n",
    "plt.xlabel('X‚ÇÅ', fontweight='bold')\n",
    "plt.ylabel('X‚ÇÇ', fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(['Class 0', 'Class 1'], loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a5180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train model for XOR\n",
    "model_xor = SimpleNN(input_size=2, hidden_size=4, output_size=1)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = optim.Adam(model_xor.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "epochs = 1000\n",
    "losses = []\n",
    "\n",
    "print(\"\\nüîÑ Training Neural Network on XOR...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    outputs = model_xor(X_xor)\n",
    "    loss = criterion(outputs, y_xor)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()  # Clear gradients\n",
    "    loss.backward()        # Compute gradients\n",
    "    optimizer.step()       # Update weights\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Training Complete!\")\n",
    "\n",
    "# Test the trained model\n",
    "with torch.no_grad():\n",
    "    predictions = model_xor(X_xor)\n",
    "    predicted_classes = (predictions >= 0.5).float()\n",
    "\n",
    "print(\"\\nüéØ Results:\")\n",
    "print(\"=\"*50)\n",
    "print(\"Input | True | Predicted | Probability\")\n",
    "print(\"-\"*50)\n",
    "for i in range(len(X_xor)):\n",
    "    print(f\"{X_xor[i].numpy()} | {int(y_xor[i].item())} | {int(predicted_classes[i].item())} | {predictions[i].item():.4f}\")\n",
    "\n",
    "accuracy = (predicted_classes == y_xor).float().mean() * 100\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85a53d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.title('Training Loss Over Time', fontweight='bold', fontsize=14)\n",
    "plt.xlabel('Epoch', fontweight='bold')\n",
    "plt.ylabel('Loss (BCE)', fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773c75a3",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Real-World Example: Defect Classification for Manufacturing Copilot\n",
    "\n",
    "Our goal is to build a neural network that can predict whether a manufactured part is defective based on sensor readings. This is a critical component for our **Manufacturing Copilot**, enabling it to flag quality issues in real-time.\n",
    "\n",
    "We'll use two sensor readings: `temperature_deviation` and `pressure_deviation` from the optimal manufacturing process. A non-linear relationship exists between these deviations and the likelihood of a defect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de26401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic manufacturing data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate non-linear data\n",
    "X, y = make_circles(n_samples=n_samples, noise=0.1, factor=0.5, random_state=42)\n",
    "\n",
    "# Add feature names for context\n",
    "df = pd.DataFrame(X, columns=['temperature_deviation', 'pressure_deviation'])\n",
    "df['is_defective'] = y\n",
    "\n",
    "print(\"üè≠ Manufacturing Dataset\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Samples: {len(df)}\")\n",
    "print(f\"Defective: {df['is_defective'].sum()} ({df['is_defective'].mean():.1%})\")\n",
    "print(f\"Non-defective: {(1-df['is_defective']).sum()}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['green' if label == 0 else 'red' for label in y]\n",
    "plt.scatter(X[:, 0], X[:, 1], c=colors, alpha=0.6, edgecolors='black', linewidths=0.5)\n",
    "plt.title('Manufacturing Data: Temperature vs Pressure Deviations', fontweight='bold', fontsize=14)\n",
    "plt.xlabel('Temperature Deviation', fontweight='bold')\n",
    "plt.ylabel('Pressure Deviation', fontweight='bold')\n",
    "plt.legend(['Non-defective', 'Defective'], loc='upper right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4626140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and preprocess data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "y_test_tensor = torch.FloatTensor(y_test).unsqueeze(1)\n",
    "\n",
    "print(\"‚úÖ Data prepared for training\")\n",
    "print(f\"Training samples: {len(X_train_tensor)}\")\n",
    "print(f\"Test samples: {len(X_test_tensor)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a16c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build deeper neural network\n",
    "class ManufacturingNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ManufacturingNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 4)\n",
    "        self.fc4 = nn.Linear(4, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model_manufacturing = ManufacturingNN()\n",
    "print(\"üß† Manufacturing Defect Classifier\")\n",
    "print(\"=\"*50)\n",
    "print(model_manufacturing)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model_manufacturing.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6d3ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model_manufacturing.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 500\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "print(\"\\nüîÑ Training Manufacturing Defect Classifier...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    model_manufacturing.train()\n",
    "    optimizer.zero_grad()\n",
    "    train_outputs = model_manufacturing(X_train_tensor)\n",
    "    train_loss = criterion(train_outputs, y_train_tensor)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Evaluation\n",
    "    model_manufacturing.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model_manufacturing(X_test_tensor)\n",
    "        test_loss = criterion(test_outputs, y_test_tensor)\n",
    "        \n",
    "        # Calculate accuracies\n",
    "        train_preds = (train_outputs >= 0.5).float()\n",
    "        test_preds = (test_outputs >= 0.5).float()\n",
    "        train_acc = (train_preds == y_train_tensor).float().mean().item()\n",
    "        test_acc = (test_preds == y_test_tensor).float().mean().item()\n",
    "    \n",
    "    train_losses.append(train_loss.item())\n",
    "    test_losses.append(test_loss.item())\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "        print(f\"  Train Loss: {train_loss.item():.4f}, Train Acc: {train_acc:.2%}\")\n",
    "        print(f\"  Test Loss: {test_loss.item():.4f}, Test Acc: {test_acc:.2%}\")\n",
    "\n",
    "print(\"\\n‚úÖ Training Complete!\")\n",
    "print(f\"Final Test Accuracy: {test_accuracies[-1]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213d9e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(train_losses, label='Train Loss', linewidth=2)\n",
    "ax1.plot(test_losses, label='Test Loss', linewidth=2)\n",
    "ax1.set_title('Training and Test Loss', fontweight='bold', fontsize=14)\n",
    "ax1.set_xlabel('Epoch', fontweight='bold')\n",
    "ax1.set_ylabel('Loss (BCE)', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "ax2.plot(train_accuracies, label='Train Accuracy', linewidth=2)\n",
    "ax2.plot(test_accuracies, label='Test Accuracy', linewidth=2)\n",
    "ax2.set_title('Training and Test Accuracy', fontweight='bold', fontsize=14)\n",
    "ax2.set_xlabel('Epoch', fontweight='bold')\n",
    "ax2.set_ylabel('Accuracy', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee540bc",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Visualizing Decision Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad4b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create decision boundary visualization\n",
    "def plot_decision_boundary(model, X, y, scaler):\n",
    "    # Create mesh\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Predict on mesh\n",
    "    mesh_data = np.c_[xx.ravel(), yy.ravel()]\n",
    "    mesh_data_scaled = scaler.transform(mesh_data)\n",
    "    mesh_tensor = torch.FloatTensor(mesh_data_scaled)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        Z = model(mesh_tensor).numpy()\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Contour plot\n",
    "    plt.contourf(xx, yy, Z, levels=20, cmap='RdYlGn', alpha=0.6)\n",
    "    plt.colorbar(label='Probability of Defect')\n",
    "    \n",
    "    # Scatter plot\n",
    "    colors = ['green' if label == 0 else 'red' for label in y]\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=colors, edgecolors='black', linewidths=0.5, s=50)\n",
    "    \n",
    "    plt.title('Neural Network Decision Boundary', fontweight='bold', fontsize=14)\n",
    "    plt.xlabel('Temperature Deviation', fontweight='bold')\n",
    "    plt.ylabel('Pressure Deviation', fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(model_manufacturing, X_test, y_test, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b073f239",
   "metadata": {},
   "source": [
    "## üéâ Summary\n",
    "\n",
    "Congratulations! You've mastered neural network fundamentals!\n",
    "\n",
    "### Key Concepts\n",
    "- ‚úÖ Neural network architecture (layers, weights, biases)\n",
    "- ‚úÖ Activation functions (Sigmoid, Tanh, ReLU)\n",
    "- ‚úÖ Forward propagation\n",
    "- ‚úÖ Loss functions and backpropagation\n",
    "- ‚úÖ Training with PyTorch\n",
    "- ‚úÖ Real-world classification\n",
    "\n",
    "### What You Built\n",
    "1. üîµ Single perceptron from scratch\n",
    "2. üß† Multi-layer neural networks\n",
    "3. ‚ö° XOR problem solver\n",
    "4. üè≠ Manufacturing defect classifier\n",
    "\n",
    "### Next Steps\n",
    "Continue to **Notebook 02: Convolutional Neural Networks** to learn about image processing!\n",
    "\n",
    "<div align=\"center\">\n",
    "<b>Neural networks unlocked! Ready for CNNs! üöÄ</b>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
