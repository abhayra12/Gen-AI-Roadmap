# Week 3-4: Deep Learning & NLP Foundations

**Duration:** 2 weeks  
**Difficulty:** Intermediate  
**Prerequisites:** Week 1-2 completed

---

## üìö Module Overview

Dive deep into neural networks and natural language processing. Master the transformer architecture that powers all modern LLMs.

### Learning Objectives

By the end of this module, you will:
- ‚úÖ Understand neural network fundamentals
- ‚úÖ Build CNNs for computer vision
- ‚úÖ Implement RNNs for sequences
- ‚úÖ Master transformer architecture
- ‚úÖ Understand attention mechanisms
- ‚úÖ Work with HuggingFace ecosystem
- ‚úÖ Deploy pre-trained models

---

## üìì Notebooks

### 01. Neural Networks Fundamentals
**File:** `01_neural_networks.ipynb`  
**Duration:** 3-4 hours  
**Topics:**
- Perceptrons and activation functions
- Forward and backward propagation
- Loss functions and optimizers
- Training neural networks in PyTorch
- Regularization techniques

### 02. CNNs for Computer Vision
**File:** `02_cnns_basics.ipynb`  
**Duration:** 3-4 hours  
**Topics:**
- Convolution and pooling operations
- CNN architectures (LeNet, VGG, ResNet)
- Image classification with PyTorch
- Transfer learning
- Data augmentation

### 03. RNNs & Sequence Models
**File:** `03_rnns_sequences.ipynb`  
**Duration:** 3-4 hours  
**Topics:**
- Recurrent neural networks
- LSTM and GRU architectures
- Sequence-to-sequence models
- Handling variable-length sequences
- Applications in NLP

### 04. Transformer Architecture
**File:** `04_transformer_architecture.ipynb`  
**Duration:** 4-5 hours  
**Topics:**
- The "Attention Is All You Need" paper
- Encoder-decoder architecture
- Multi-head self-attention
- Positional encoding
- Feed-forward networks
- Building a transformer from scratch

### 05. Attention Mechanism Deep Dive
**File:** `05_attention_mechanism.ipynb`  
**Duration:** 3-4 hours  
**Topics:**
- Query, Key, Value matrices
- Scaled dot-product attention
- Multi-head attention visualization
- Attention patterns analysis
- Self-attention vs cross-attention

### 06. Embeddings & Tokenization
**File:** `06_embeddings_tokenization.ipynb`  
**Duration:** 2-3 hours  
**Topics:**
- Word embeddings (Word2Vec, GloVe)
- Subword tokenization (BPE, WordPiece)
- Context window and positional encodings
- Embedding visualization with t-SNE
- Tokenizers in practice

### 07. HuggingFace Ecosystem
**File:** `07_huggingface_intro.ipynb`  
**Duration:** 3-4 hours  
**Topics:**
- HuggingFace Hub exploration
- Using pre-trained models
- Pipelines for common tasks
- AutoModel and AutoTokenizer
- Model inference and evaluation
- Saving and loading models

---

## üìù Homework Assignment

### Mini-Project: Multi-lingual Text Classifier

**Objective:** Build a text classification system using transformers.

**Task:**
1. Choose a classification task (sentiment, topic, language detection)
2. Select appropriate HuggingFace model
3. Fine-tune on custom dataset
4. Evaluate performance
5. Deploy as inference endpoint

**Deliverables:**
- Jupyter notebook with complete pipeline
- Model evaluation report
- Inference script
- README with results

**Rubric:** 100 points (see HOMEWORK.md)

---

## üéØ Checkpoint 2: Transformer Implementation

**Assessment:**
- Implement simplified transformer from scratch
- Explain attention mechanism
- Use HuggingFace for real task
- Pass coding challenge

**Pass Criteria:** 70%+ on all sections

---

## üìö Additional Resources

### Papers
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [BERT Paper](https://arxiv.org/abs/1810.04805)
- [GPT-2 Paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

### Documentation
- [PyTorch Documentation](https://pytorch.org/docs/)
- [HuggingFace Transformers](https://huggingface.co/docs/transformers/)
- [HuggingFace Course](https://huggingface.co/course/)

### Videos
- [3Blue1Brown - Neural Networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
- [Andrej Karpathy - Neural Networks](https://www.youtube.com/watch?v=VMj-3S1tku0)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)

---

## üí° Study Tips

### Week 3 Focus
- Days 1-2: Neural Networks + CNNs
- Days 3-4: RNNs + Start Transformers
- Days 5-7: Transformer architecture

### Week 4 Focus
- Days 1-2: Attention mechanism
- Days 3-4: Embeddings + HuggingFace
- Days 5-6: Homework
- Day 7: Checkpoint assessment

---

## ‚úÖ Completion Checklist

- [ ] Completed all 7 notebooks
- [ ] Built transformer from scratch
- [ ] Used HuggingFace models
- [ ] Completed homework
- [ ] Passed Checkpoint 2
- [ ] Updated progress tracker

---

**Next:** Week 5-6 - LLMs, Prompt Engineering & RAG

<div align="center">
Week 3-4 | Deep Learning & NLP | Transform your understanding! üß†
</div>
