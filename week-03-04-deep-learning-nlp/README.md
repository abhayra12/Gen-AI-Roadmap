# üìñ Week 3-4: Deep Learning & NLP Foundations

**Phase 2: Deep Learning & NLP**  
**Goal:** Dive into neural networks and master the Transformer architecture that powers modern LLMs.

---

## üìö Module Overview

This module transitions from classical ML to the core of modern AI. You will learn the theory behind deep learning and gain hands-on experience with the fundamental architectures for computer vision (CNNs), sequence modeling (RNNs), and, most importantly, the Transformer.

### Learning Objectives
By the end of this module, you will be able to:
- ‚úÖ Build and train neural networks using PyTorch.
- ‚úÖ Implement CNNs for image-based tasks like visual quality inspection.
- ‚úÖ Understand the architecture of RNNs for sequence data.
- ‚úÖ Master the Transformer architecture, including the self-attention mechanism.
- ‚úÖ Build a Transformer from scratch to solidify your understanding.
- ‚úÖ Leverage the HuggingFace ecosystem to use powerful pre-trained models.

---

## üìì Notebooks & Concepts

This module covers the essential deep learning architectures in seven notebooks:

| Order | Notebook                            | Key Concepts                                       |
|-------|-------------------------------------|----------------------------------------------------|
| 1.    | `01_neural_networks.ipynb`          | Backpropagation, PyTorch, Loss Functions           |
| 2.    | `02_cnns_basics.ipynb`              | Convolutions, Pooling, Transfer Learning           |
| 3.    | `03_rnns_sequences.ipynb`           | LSTMs, GRUs, Sequence-to-Sequence Models           |
| 4.    | `04_transformer_architecture.ipynb` | Encoder-Decoder, Positional Encoding               |
| 5.    | `05_attention_mechanism.ipynb`      | Self-Attention (Query, Key, Value), Multi-Head     |
| 6.    | `06_embeddings_tokenization.ipynb`  | Word2Vec, BPE/WordPiece Tokenization               |
| 7.    | `07_huggingface_intro.ipynb`        | `pipeline`, `AutoModel`, `AutoTokenizer`, Hub      |

---

## üìù Homework: Text Classifier with Transformers

**Objective:** Build, fine-tune, and evaluate a Transformer-based text classifier.

**Task:**
Using the HuggingFace ecosystem, you will select a pre-trained Transformer model, fine-tune it on a custom dataset for a classification task (e.g., sentiment analysis of product reviews), and evaluate its performance.

- **Instructions:** See the `HOMEWORK.md` file in this directory for detailed instructions.

---

## üéØ What's Next?

With a strong understanding of Transformers, you are ready for **Week 5-6: LLMs & Retrieval-Augmented Generation (RAG)**. You will learn how to effectively prompt Large Language Models and build systems that can reason over external knowledge.

**Get ready to dive deep!** üß†
