{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e662302",
   "metadata": {},
   "source": [
    "# üéØ Notebook 05: Attention Mechanisms in Depth\n",
    "\n",
    "**Week 3-4: Deep Learning & NLP Foundations**  \n",
    "**Gen AI Masters Program**\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Objectives\n",
    "\n",
    "By the end of this notebook, you will master:\n",
    "1. ‚úÖ Why attention improves sequence models\n",
    "2. ‚úÖ Additive (Bahdanau) vs Multiplicative (Luong) attention\n",
    "3. ‚úÖ Self-attention vs cross-attention\n",
    "4. ‚úÖ Visualizing attention weight distributions\n",
    "5. ‚úÖ Implementing attention-enhanced seq2seq models in PyTorch\n",
    "6. ‚úÖ Applying attention to manufacturing incident reports\n",
    "\n",
    "**Estimated Time:** 3-4 hours\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Why Attention?\n",
    "\n",
    "RNNs compress everything into a single vector. Attention lets the model **focus** on relevant parts dynamically.\n",
    "\n",
    "Use cases:\n",
    "- üß† Neural machine translation\n",
    "- üìù Document summarization\n",
    "- üè≠ Incident triage (our focus)\n",
    "- üîÑ Time-series alignment\n",
    "\n",
    "Let's explore how attention mechanisms work under the hood!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def487ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from typing import Tuple, List\n",
    "from collections import Counter\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'‚úÖ Using device: {device}')\n",
    "print(f'PyTorch version: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bc5ba3",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Additive Attention (Bahdanau)\n",
    "\n",
    "Introduced in 2014 for neural machine translation.\n",
    "\n",
    "### Formula\n",
    "\n",
    "Given decoder hidden state $s_t$ and encoder outputs $h_i$:\n",
    "\n",
    "$$ e_{t,i} = v_a^T \tanh(W_a s_t + U_a h_i) $$\n",
    "$$ lpha_{t,i} = \text{softmax}(e_{t,i}) $$\n",
    "$$ c_t = um_i lpha_{t,i} h_i $$\n",
    "\n",
    "### Intuition\n",
    "- Learnable scoring function\n",
    "- Uses MLP to combine state and encoder features\n",
    "- Works well for small embedding sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce972cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, encoder_hidden_dim: int, decoder_hidden_dim: int, attention_dim: int):\n",
    "        super().__init__()\n",
    "        self.W_a = nn.Linear(decoder_hidden_dim, attention_dim, bias=False)\n",
    "        self.U_a = nn.Linear(encoder_hidden_dim, attention_dim, bias=False)\n",
    "        self.v_a = nn.Linear(attention_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, decoder_hidden: torch.Tensor, encoder_outputs: torch.Tensor, mask=None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # decoder_hidden: (batch, decoder_hidden_dim)\n",
    "        # encoder_outputs: (batch, seq_len, encoder_hidden_dim)\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(1)  # (batch, 1, hidden)\n",
    "        score = torch.tanh(self.W_a(decoder_hidden) + self.U_a(encoder_outputs))\n",
    "        attention = self.v_a(score).squeeze(-1)  # (batch, seq_len)\n",
    "\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn_weights = F.softmax(attention, dim=-1)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "\n",
    "        return context, attn_weights\n",
    "\n",
    "\n",
    "# Demo with random tensors\n",
    "batch_size, seq_len, enc_dim, dec_dim = 2, 5, 16, 16\n",
    "encoder_outputs = torch.randn(batch_size, seq_len, enc_dim)\n",
    "decoder_hidden = torch.randn(batch_size, dec_dim)\n",
    "mask = torch.ones(batch_size, seq_len)\n",
    "\n",
    "attn = BahdanauAttention(enc_dim, dec_dim, attention_dim=32)\n",
    "context, weights = attn(decoder_hidden, encoder_outputs, mask)\n",
    "\n",
    "print('üß† Bahdanau Attention Demo')\n",
    "print('='*60)\n",
    "print(f'Context shape: {context.shape}')\n",
    "print(f'Weights shape: {weights.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60baac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(weights.detach().numpy(), annot=True, cmap='YlOrRd', cbar=True)\n",
    "plt.title('Additive Attention Weights (Sampled)', fontweight='bold')\n",
    "plt.xlabel('Encoder Time Steps')\n",
    "plt.ylabel('Batch Index')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('‚úÖ Additive attention learns alignment scores via a small neural network!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a910722",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Multiplicative Attention (Luong)\n",
    "\n",
    "Introduced in 2015; more efficient for larger hidden sizes.\n",
    "\n",
    "### Variants\n",
    "- **Dot**: $e_{t,i} = s_t^T h_i$\n",
    "- **General**: $e_{t,i} = s_t^T W_a h_i$\n",
    "- **Concat**: similar to additive but uses a different formulation\n",
    "\n",
    "### Advantages\n",
    "- Faster because it uses matrix multiplications\n",
    "- Works well when encoder/decoder dimensions match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b342c91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(nn.Module):\n",
    "    def __init__(self, encoder_hidden_dim: int, decoder_hidden_dim: int, mode: str = 'general'):\n",
    "        super().__init__()\n",
    "        assert mode in ['dot', 'general'], 'Unsupported attention mode'\n",
    "        self.mode = mode\n",
    "        if mode == 'general':\n",
    "            self.W_a = nn.Linear(encoder_hidden_dim, decoder_hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, decoder_hidden: torch.Tensor, encoder_outputs: torch.Tensor, mask=None):\n",
    "        # decoder_hidden: (batch, hidden)\n",
    "        # encoder_outputs: (batch, seq_len, hidden)\n",
    "        if self.mode == 'general':\n",
    "            encoder_outputs = self.W_a(encoder_outputs)\n",
    "\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(2)  # (batch, hidden, 1)\n",
    "        scores = torch.bmm(encoder_outputs, decoder_hidden).squeeze(-1)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "\n",
    "        return context, attn_weights\n",
    "\n",
    "\n",
    "# Demo\n",
    "attn_luong = LuongAttention(enc_dim, dec_dim, mode='general')\n",
    "context_luong, weights_luong = attn_luong(decoder_hidden, encoder_outputs, mask)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(weights_luong.detach().numpy(), annot=True, cmap='BuGn', cbar=True)\n",
    "plt.title('Multiplicative Attention Weights (Sampled)', fontweight='bold')\n",
    "plt.xlabel('Encoder Time Steps')\n",
    "plt.ylabel('Batch Index')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('‚ö° Multiplicative attention relies on efficient matrix products!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef5e2f8",
   "metadata": {},
   "source": [
    "### Comparing Additive vs Multiplicative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eed024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare runtime\n",
    "import time\n",
    "\n",
    "def benchmark_attention(module, iterations=1000):\n",
    "    start = time.time()\n",
    "    for _ in range(iterations):\n",
    "        _ = module(decoder_hidden, encoder_outputs, mask)\n",
    "    end = time.time()\n",
    "    return end - start\n",
    "\n",
    "iterations = 500\n",
    "bahdanau_time = benchmark_attention(attn, iterations)\n",
    "luong_time = benchmark_attention(attn_luong, iterations)\n",
    "\n",
    "print('‚è±Ô∏è Attention Runtime Comparison')\n",
    "print('='*60)\n",
    "print(f'Bahdanau (Additive):    {bahdanau_time:.4f} seconds')\n",
    "print(f'Luong (Multiplicative): {luong_time:.4f} seconds')\n",
    "print(f'‚Üí Luong is {bahdanau_time/luong_time:.2f}x faster in this configuration')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539314c2",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Self-Attention vs Cross-Attention\n",
    "\n",
    "| Type | Query From | Key/Value From | Use Case |\n",
    "|------|------------|----------------|----------|\n",
    "| **Self-attention** | Same sequence | Same sequence | Encoder layers, capturing contextual dependencies |\n",
    "| **Cross-attention** | Decoder state | Encoder outputs | Decoder focuses on encoder tokens |\n",
    "\n",
    "Cross-attention is critical in seq2seq tasks like translating sensor logs into recommended actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3400dfb",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Manufacturing Case Study: Incident Explanation\n",
    "\n",
    "Goal: Generate a brief explanation (sequence-to-sequence) for a maintenance alert.\n",
    "\n",
    "### Dataset\n",
    "We'll create synthetic pairs of** sensor observations ‚Üí explanation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c72f4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_sequences = [\n",
    "    'temperature spike with coolant loss',\n",
    "    'vibration rise near motor bearings',\n",
    "    'hydraulic pressure drop and leakage',\n",
    "    'conveyor belt speed oscillation detected',\n",
    "    'voltage surge hitting robotic arm',\n",
    "    'persistent sensor outage on line three',\n",
    "    'steam valve stuck partially open',\n",
    "    'gearbox overheating under heavy load',\n",
    "    'unexpected torque variance in assembly',\n",
    "    'coolant contamination alert triggered'\n",
    "]\n",
    "\n",
    "explanations = [\n",
    "    'reduce load and inspect coolant loop',\n",
    "    'check bearing lubrication and alignment',\n",
    "    'isolate leak and restore fluid pressure',\n",
    "    'calibrate drive rollers and tension system',\n",
    "    'disconnect arm and test power regulators',\n",
    "    'swap redundant sensor and trace wiring',\n",
    "    'cycle valve manually and service actuator',\n",
    "    'pause line and evaluate gear lubrication',\n",
    "    'tighten fasteners and recalibrate wrench',\n",
    "    'flush coolant system and replace filters'\n",
    "]\n",
    "\n",
    "df_seq2seq = pd.DataFrame({'sensor': sensor_sequences, 'explanation': explanations})\n",
    "print(df_seq2seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323b02fd",
   "metadata": {},
   "source": [
    "### Tokenization & Vocabulary Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afe58d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.lower().replace('-', ' ').split()\n",
    "\n",
    "\n",
    "SRC_PAD, SRC_UNK, SRC_SOS, SRC_EOS = '<pad>', '<unk>', '<sos>', '<eos>'\n",
    "TRG_PAD, TRG_UNK, TRG_SOS, TRG_EOS = '<pad>', '<unk>', '<sos>', '<eos>'\n",
    "\n",
    "# Build source vocab\n",
    "source_tokens = [token for sent in sensor_sequences for token in tokenize(sent)]\n",
    "source_vocab = {SRC_PAD: 0, SRC_UNK: 1, SRC_SOS: 2, SRC_EOS: 3}\n",
    "for token, _ in Counter(source_tokens).most_common():\n",
    "    source_vocab[token] = len(source_vocab)\n",
    "\n",
    "# Build target vocab\n",
    "target_tokens = [token for sent in explanations for token in tokenize(sent)]\n",
    "target_vocab = {TRG_PAD: 0, TRG_UNK: 1, TRG_SOS: 2, TRG_EOS: 3}\n",
    "for token, _ in Counter(target_tokens).most_common():\n",
    "    target_vocab[token] = len(target_vocab)\n",
    "\n",
    "print(f'Source vocab size: {len(source_vocab)} | Target vocab size: {len(target_vocab)}')\n",
    "\n",
    "inv_target_vocab = {idx: token for token, idx in target_vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97968db",
   "metadata": {},
   "source": [
    "### Encoding Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea20061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SRC_LEN = 12\n",
    "MAX_TRG_LEN = 12\n",
    "\n",
    "\n",
    "def encode_sequence(text, vocab, max_len, sos_token=None, eos_token=None):\n",
    "    tokens = tokenize(text)\n",
    "    token_ids = []\n",
    "    if sos_token: token_ids.append(vocab[sos_token])\n",
    "    token_ids.extend([vocab.get(tok, vocab[list(vocab.keys())[1]]) for tok in tokens])\n",
    "    if eos_token: token_ids.append(vocab[eos_token])\n",
    "\n",
    "    if len(token_ids) < max_len:\n",
    "        token_ids += [vocab[list(vocab.keys())[0]]] * (max_len - len(token_ids))\n",
    "    else:\n",
    "        token_ids = token_ids[:max_len]\n",
    "\n",
    "    return token_ids\n",
    "\n",
    "\n",
    "encoded_src = [encode_sequence(sent, source_vocab, MAX_SRC_LEN, sos_token=SRC_SOS, eos_token=SRC_EOS) for sent in sensor_sequences]\n",
    "encoded_trg = [encode_sequence(sent, target_vocab, MAX_TRG_LEN, sos_token=TRG_SOS, eos_token=TRG_EOS) for sent in explanations]\n",
    "\n",
    "print(np.array(encoded_src).shape, np.array(encoded_trg).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fe94c7",
   "metadata": {},
   "source": [
    "### Seq2Seq Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57927be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncidentExplanationDataset(Dataset):\n",
    "    def __init__(self, src_sequences, trg_sequences):\n",
    "        self.src = torch.tensor(src_sequences, dtype=torch.long)\n",
    "        self.trg = torch.tensor(trg_sequences, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.src[idx], self.trg[idx]\n",
    "\n",
    "\n",
    "dataset = IncidentExplanationDataset(encoded_src, encoded_trg)\n",
    "loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "for src_batch, trg_batch in loader:\n",
    "    print('Source batch shape:', src_batch.shape)\n",
    "    print('Target batch shape:', trg_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75966294",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Encoder-Decoder with Bahdanau Attention\n",
    "\n",
    "We'll build a compact seq2seq model using GRU encoder/decoder with additive attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9794542",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, hidden = self.gru(embedded)\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class DecoderWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, attention_dim, num_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim + hidden_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.attention = BahdanauAttention(hidden_dim, hidden_dim, attention_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim * 2, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_token, hidden, encoder_outputs, mask=None):\n",
    "        embedded = self.dropout(self.embedding(input_token.unsqueeze(1)))  # (batch, 1, embed)\n",
    "        context, attn_weights = self.attention(hidden[-1], encoder_outputs, mask)\n",
    "        gru_input = torch.cat([embedded, context.unsqueeze(1)], dim=2)\n",
    "        output, hidden = self.gru(gru_input, hidden)\n",
    "        output = torch.cat([output.squeeze(1), context], dim=-1)\n",
    "        prediction = self.fc_out(output)\n",
    "        return prediction, hidden, attn_weights\n",
    "\n",
    "\n",
    "class Seq2SeqAttention(nn.Module):\n",
    "    def __init__(self, encoder, decoder, pad_idx):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def create_mask(self, src):\n",
    "        return (src != self.pad_idx)\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size, trg_len = trg.shape\n",
    "        vocab_size = self.decoder.fc_out.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, trg_len, vocab_size).to(src.device)\n",
    "\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        mask = self.create_mask(src)\n",
    "\n",
    "        input_token = trg[:, 0]  # <sos>\n",
    "\n",
    "        attention_history = []\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, attn_weights = self.decoder(input_token, hidden, encoder_outputs, mask)\n",
    "            outputs[:, t] = output\n",
    "            attention_history.append(attn_weights.detach().cpu())\n",
    "\n",
    "            teacher_force = np.random.rand() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(dim=1)\n",
    "            input_token = trg[:, t] if teacher_force else top1\n",
    "\n",
    "        return outputs, attention_history\n",
    "\n",
    "\n",
    "# Instantiate model\n",
    "ENC_EMBED_DIM = 64\n",
    "DEC_EMBED_DIM = 64\n",
    "HIDDEN_DIM = 128\n",
    "ATTN_DIM = 64\n",
    "\n",
    "encoder = Encoder(len(source_vocab), ENC_EMBED_DIM, HIDDEN_DIM)\n",
    "decoder = DecoderWithAttention(len(target_vocab), DEC_EMBED_DIM, HIDDEN_DIM, ATTN_DIM)\n",
    "seq2seq_model = Seq2SeqAttention(encoder, decoder, pad_idx=source_vocab[SRC_PAD]).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=target_vocab[TRG_PAD])\n",
    "optimizer = optim.Adam(seq2seq_model.parameters(), lr=1e-3)\n",
    "\n",
    "print(seq2seq_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9e456c",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16bf6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seq2seq(model, dataloader, optimizer, criterion, epochs=200):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_loss = 0\n",
    "        for src_batch, trg_batch in dataloader:\n",
    "            src_batch = src_batch.to(device)\n",
    "            trg_batch = trg_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output, _ = model(src_batch, trg_batch, teacher_forcing_ratio=0.75)\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:].reshape(-1, output_dim)\n",
    "            trg = trg_batch[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "        if epoch % 40 == 0 or epoch == 1:\n",
    "            print(f'Epoch {epoch:03d} | Loss: {avg_loss:.4f}')\n",
    "    return losses\n",
    "\n",
    "\n",
    "print('üöÄ Training seq2seq model with additive attention...')\n",
    "losses = train_seq2seq(seq2seq_model, loader, optimizer, criterion, epochs=200)\n",
    "print('‚úÖ Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a7e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(losses, color='blue', linewidth=2)\n",
    "plt.title('Seq2Seq Attention Training Loss', fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9823d2",
   "metadata": {},
   "source": [
    "### Evaluation & Attention Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04f3811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, src_sentence: str):\n",
    "    model.eval()\n",
    "    src_encoded = encode_sequence(src_sentence, source_vocab, MAX_SRC_LEN, sos_token=SRC_SOS, eos_token=SRC_EOS)\n",
    "    src_tensor = torch.tensor(src_encoded, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    trg_indices = [target_vocab[TRG_SOS]]\n",
    "    max_len = MAX_TRG_LEN\n",
    "    attention_maps = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor)\n",
    "        mask = model.create_mask(src_tensor)\n",
    "        input_token = torch.tensor([target_vocab[TRG_SOS]], device=device)\n",
    "\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, attn_weights = model.decoder(input_token, hidden, encoder_outputs, mask)\n",
    "            top1 = output.argmax(1).item()\n",
    "            trg_indices.append(top1)\n",
    "            attention_maps.append(attn_weights.squeeze().cpu().numpy())\n",
    "            if top1 == target_vocab[TRG_EOS]:\n",
    "                break\n",
    "            input_token = torch.tensor([top1], device=device)\n",
    "\n",
    "    trg_tokens = [inv_target_vocab.get(idx, TRG_UNK) for idx in trg_indices[1:-1]]\n",
    "    return trg_tokens, np.array(attention_maps)\n",
    "\n",
    "\n",
    "example_sentence = 'hydraulic pressure drop and leakage'\n",
    "prediction, attention_map = translate_sentence(seq2seq_model, example_sentence)\n",
    "\n",
    "print('üìù Input :', example_sentence)\n",
    "print('üí° Output:', ' '.join(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53557aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize alignment\n",
    "src_tokens = tokenize(example_sentence)\n",
    "src_tokens = [SRC_SOS] + src_tokens + [SRC_EOS]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(attention_map[:len(prediction), :len(src_tokens)], cmap='magma', annot=True, fmt='.2f')\n",
    "plt.xlabel('Source Tokens', fontweight='bold')\n",
    "plt.ylabel('Generated Tokens', fontweight='bold')\n",
    "plt.xticks(np.arange(len(src_tokens)) + 0.5, src_tokens, rotation=45, ha='right')\n",
    "plt.yticks(np.arange(len(prediction)) + 0.5, prediction, rotation=0)\n",
    "plt.title('Attention Alignment: Maintenance Explanation', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721d774c",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Attention Variants Recap\n",
    "\n",
    "| Variant | Highlights | Pros | Cons |\n",
    "|---------|-----------|------|------|\n",
    "| **Bahdanau (Additive)** | MLP scoring | Flexible, good for small dims | Slightly slower |\n",
    "| **Luong (Multiplicative)** | Dot-product scoring | Fast, efficient | Requires matching dims |\n",
    "| **Self-Attention** | Intra-sequence focus | Captures global context | Quadratic cost |\n",
    "| **Cross-Attention** | Decoder ‚Üî Encoder | Enables seq2seq alignment | Needs encoder states |\n",
    "| **Multi-Head** | Multiple subspaces | Diverse patterns | Higher compute |\n",
    "\n",
    "Attention mechanisms underpin modern Transformers and large language models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d79f11",
   "metadata": {},
   "source": [
    "## üéâ Summary\n",
    "\n",
    "Great job mastering attention!\n",
    "\n",
    "### Key Takeaways\n",
    "- ‚úÖ Attention alleviates RNN bottlenecks\n",
    "- ‚úÖ Additive vs multiplicative trade-offs\n",
    "- ‚úÖ Self vs cross-attention roles\n",
    "- ‚úÖ Sequencing attention for manufacturing logs\n",
    "- ‚úÖ Visualizing alignments for explainability\n",
    "\n",
    "### What You Built\n",
    "1. üßÆ Bahdanau and Luong attention modules\n",
    "2. ‚öñÔ∏è Runtime benchmarking for attention variants\n",
    "3. üè≠ Synthetic incident explanation dataset\n",
    "4. üîÑ Seq2seq model with additive attention\n",
    "5. üîç Attention heatmap highlighting critical tokens\n",
    "\n",
    "### Manufacturing Insights\n",
    "- Attention highlights influential observations in maintenance logs\n",
    "- Alignment heatmaps support root-cause analysis\n",
    "- Sequence explanations accelerate technician decisions\n",
    "\n",
    "### Next Steps\n",
    "Proceed to **Notebook 06: Embeddings** to understand vector representations that power attention models.\n",
    "\n",
    "<div align=\"center\">\n",
    "<b>Attention unlocked! Time to dive into embeddings. üß≤üöÄ</b>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
