{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e662302",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Notebook 05: Attention Mechanisms - A Deep Dive\n",
    "\n",
    "**Week 3-4: Deep Learning & NLP Foundations**  \n",
    "**Gen AI Masters Program**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Objectives\n",
    "\n",
    "In the last few notebooks, we've seen the power of LSTMs and Transformers. A key ingredient in the success of the Transformer is **attention**. But attention is not just one thing; it's a general and powerful concept that can be used in many different ways to allow a model to dynamically **focus on specific parts of its input** when performing a task.\n",
    "\n",
    "Instead of forcing a model to compress an entire input sequence (like a long sentence) into a single fixed-size vectorâ€”a major bottleneckâ€”an attention mechanism computes a set of **attention weights**. These weights determine how much importance should be paid to each part of the input for a given step.\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1.  **Understand the Core Intuition**: Grasp the high-level concept of attention as a \"relevance-scoring\" and \"weighted-sum\" mechanism.\n",
    "2.  **Implement Scaled Dot-Product Attention**: Implement the most common type of attention from scratchâ€”the same one used in the Transformerâ€”and understand the roles of the **Query (Q)**, **Key (K)**, and **Value (V)** vectors.\n",
    "3.  **Build a Seq2Seq Model with Attention**: Construct a simple sequence-to-sequence (Seq2Seq) model with an attention layer to see how it works in a practical translation-like task, solving the bottleneck problem of older architectures.\n",
    "4.  **Visualize Attention Weights**: Create heatmaps to visualize the attention weights, providing a powerful and interpretable view into the model's decision-making process.\n",
    "\n",
    "**Estimated Time:** 2-3 hours\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Why is Attention So Important?\n",
    "\n",
    "1.  **Solving the Bottleneck Problem**: In older encoder-decoder models (like those using LSTMs), the encoder had to compress the entire meaning of a long input sentence into a single \"context vector.\" This was a huge bottleneck. Attention allows the decoder to \"look back\" at the entire input sequence at every step of the output generation, focusing on the most relevant parts.\n",
    "2.  **Interpretability**: Attention weights are highly interpretable. By visualizing them, we can see what the model is \"looking at\" when it makes a prediction. For example, in machine translation, we can see which source words the model focuses on when generating a specific target word.\n",
    "3.  **Performance Boost**: Attention mechanisms have consistently led to state-of-the-art results in a wide range of tasks, from NLP to computer vision. They are a fundamental building block of most modern deep learning architectures.\n",
    "\n",
    "This notebook will demystify the attention mechanism and give you a solid, hands-on understanding of this fundamental concept. Let's begin! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bc3d6e",
   "metadata": {},
   "source": [
    "## ðŸš€ Agenda\n",
    "\n",
    "1.  **Setting the Stage**: We'll import the necessary libraries and configure our environment.\n",
    "2.  **Scaled Dot-Product Attention from Scratch**: We'll implement the most famous attention mechanism, breaking down the formula and the roles of Query, Key, and Value.\n",
    "3.  **Attention in a Seq2Seq Model**: We'll build a simple Encoder-Decoder model that uses an attention layer to translate a sequence of numbers into a sorted sequence. This will make the mechanism's function crystal clear.\n",
    "    *   **Encoder**: A GRU to process the input sequence.\n",
    "    *   **Attention Layer**: To compute the context vector.\n",
    "    *   **Decoder**: A GRU that uses the context vector to generate the output.\n",
    "4.  **Visualizing Attention**: We'll create a heatmap of the attention weights to see exactly where the model \"looks\" during the decoding process.\n",
    "5.  **Conceptual Overview**: We'll conclude with a brief summary of different attention variants like self-attention vs. cross-attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def487ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Set up the Environment ---\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Set the default device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"âœ… Using device: {device.upper()}\")\n",
    "\n",
    "# --- Plotting Style ---\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "sns.set_palette(\"colorblind\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bc5ba3",
   "metadata": {},
   "source": [
    "## ðŸ§  Part 2: Scaled Dot-Product Attention from Scratch\n",
    "\n",
    "This is the heart of the Transformer and the most common form of attention used today. The formula looks intimidating at first, but we'll break it down into simple steps.\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\n",
    "\n",
    "Let's understand the components, which are just matrices derived from the input embeddings:\n",
    "\n",
    "*   **Query (Q):** A matrix of vectors, where each vector represents the current word/position we are focusing on. It's asking a question, like \"What parts of the input are relevant to *me*?\"\n",
    "*   **Key (K):** A matrix of vectors, one for each word in the input sequence. You can think of these as \"labels\" or \"identifiers\" for the words. They are the \"database keys\" that the Query will be compared against.\n",
    "*   **Value (V):** A matrix of vectors, also one for each word in the input. These vectors contain the actual information or content of the words.\n",
    "\n",
    "**The Process:**\n",
    "\n",
    "1.  **Compute Scores (`QK^T`):** We take the matrix product of the Queries with the transposed Keys. This efficiently computes the dot product between every query vector and every key vector. If a query is similar to a key, their dot product will be large. This gives us a matrix of **raw attention scores**, indicating the relevance of each word to every other word.\n",
    "2.  **Scale (`/ sqrt(d_k)`):** We scale the scores by the square root of the dimension of the key vectors (`d_k`). This is a crucial step for stabilizing the gradients during training, preventing them from becoming too small, especially when `d_k` is large.\n",
    "3.  **Apply Mask (Optional):** In some cases (like in a Transformer decoder), we need to prevent a position from attending to future positions. A mask is applied to set these illegal scores to a very small number (`-1e9`).\n",
    "4.  **Apply Softmax:** We apply a softmax function along the last dimension of the scaled scores. This turns the scores for each query into a probability distribution (they all sum to 1). These are our final **attention weights**.\n",
    "5.  **Weighted Sum (`* V`):** We perform a matrix multiplication between the attention weights and the Value matrix. This produces the final output: a new representation for each word, where words that received high attention contribute more to the result.\n",
    "\n",
    "Let's implement this in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce972cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes Scaled Dot-Product Attention as described in \"Attention Is All You Need\".\n",
    "    This is the core building block of Multi-Head Attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            temperature (float): The scaling factor (sqrt(d_k)).\n",
    "            attn_dropout (float): Dropout rate for the attention weights.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            q (torch.Tensor): Query tensor. Shape: (batch_size, n_head, len_q, d_k)\n",
    "            k (torch.Tensor): Key tensor. Shape: (batch_size, n_head, len_k, d_k)\n",
    "            v (torch.Tensor): Value tensor. Shape: (batch_size, n_head, len_v, d_v)\n",
    "                              Note: len_k and len_v must be the same.\n",
    "            mask (torch.Tensor, optional): A boolean mask to prevent attention to certain positions.\n",
    "                                           Shape: (batch_size, 1, 1, len_k) or similar. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            output (torch.Tensor): The context vector after applying attention.\n",
    "                                   Shape: (batch_size, n_head, len_q, d_v)\n",
    "            attn (torch.Tensor): The attention weights.\n",
    "                                 Shape: (batch_size, n_head, len_q, len_k)\n",
    "        \"\"\"\n",
    "        # 1. & 2. Compute Scores and Scale: Q * K^T / sqrt(d_k)\n",
    "        # Matmul shapes: (..., len_q, d_k) @ (..., d_k, len_k) -> (..., len_q, len_k)\n",
    "        attn = torch.matmul(q / self.temperature, k.transpose(2, 3))\n",
    "\n",
    "        # 3. Apply Mask (if provided)\n",
    "        if mask is not None:\n",
    "            # The mask is typically a tensor where `True` or `1` indicates a position to be masked.\n",
    "            # We set these positions to a very large negative number before the softmax.\n",
    "            attn = attn.masked_fill(mask == 0, -1e9) # Use a large negative value\n",
    "\n",
    "        # 4. Apply Softmax to get attention weights\n",
    "        # Softmax is applied on the last dimension (len_k) to get probabilities.\n",
    "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
    "\n",
    "        # 5. Weighted Sum: Attention Weights * V\n",
    "        # Matmul shapes: (..., len_q, len_k) @ (..., len_k, d_v) -> (..., len_q, d_v)\n",
    "        output = torch.matmul(attn, v)\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "# --- Let's test it with some example tensors ---\n",
    "\n",
    "# Parameters\n",
    "batch_size = 2\n",
    "n_head = 8      # Number of attention heads\n",
    "len_q = 4       # Length of query sequence\n",
    "len_k = 6       # Length of key/value sequence\n",
    "d_k = 64        # Dimension of keys/queries\n",
    "d_v = 64        # Dimension of values\n",
    "\n",
    "# Scaling factor\n",
    "temperature = np.sqrt(d_k)\n",
    "\n",
    "# Create random tensors for Q, K, V\n",
    "q = torch.randn(batch_size, n_head, len_q, d_k)\n",
    "k = torch.randn(batch_size, n_head, len_k, d_k)\n",
    "v = torch.randn(batch_size, n_head, len_k, d_v) # len_k and len_v are the same\n",
    "\n",
    "# Create an instance of our attention module\n",
    "attention_layer = ScaledDotProductAttention(temperature=temperature)\n",
    "\n",
    "# Get the output\n",
    "output, attn_weights = attention_layer(q, k, v)\n",
    "\n",
    "# Print the shapes to verify\n",
    "print(\"--- Tensor Shapes ---\")\n",
    "print(\"Query (q):          \", q.shape)\n",
    "print(\"Key (k):            \", k.shape)\n",
    "print(\"Value (v):          \", v.shape)\n",
    "print(\"\\nOutput from Attention:\", output.shape)\n",
    "print(\"Attention Weights:  \", attn_weights.shape)\n",
    "\n",
    "# --- Verification ---\n",
    "# The output for each head should have the shape (len_q, d_v)\n",
    "# The attention weights for each head should have the shape (len_q, len_k)\n",
    "assert output.shape == (batch_size, n_head, len_q, d_v)\n",
    "assert attn_weights.shape == (batch_size, n_head, len_q, len_k)\n",
    "\n",
    "print(\"\\nâœ… Shapes are correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a910722",
   "metadata": {},
   "source": [
    "## ðŸ§  Part 3: Multi-Head Attention (MHA)\n",
    "\n",
    "Scaled Dot-Product Attention is powerful, but Multi-Head Attention takes it a step further. Instead of calculating attention just once, MHA runs the process multiple times in parallel and concatenates the results.\n",
    "\n",
    "**Why is this useful?**\n",
    "\n",
    "It allows the model to jointly attend to information from different \"representation subspaces\" at different positions. A single attention head might learn to focus on, for example, subject-verb relationships, while another head might focus on adjective-noun pairings. By having multiple heads, the model can capture a richer variety of linguistic patterns simultaneously.\n",
    "\n",
    "**The Process:**\n",
    "\n",
    "1.  **Initial Projections:** The input Queries, Keys, and Values are not fed directly into the attention mechanism. Instead, for *each head*, we create a separate linear projection (a `nn.Linear` layer) of Q, K, and V. This is what creates the different \"representation subspaces.\"\n",
    "2.  **Parallel Attention:** We perform Scaled Dot-Product Attention on each of these projected sets of Q, K, and V in parallel. This results in `h` (number of heads) different output matrices.\n",
    "3.  **Concatenate & Final Projection:** The `h` output matrices are concatenated together. This combined matrix is then passed through one final linear layer (`nn.Linear`) to produce the ultimate output of the MHA block.\n",
    "\n",
    "This entire process is encapsulated in the `MultiHeadAttention` module below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b342c91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Multi-Head Attention as described in \"Attention Is All You Need\".\n",
    "    This module contains multiple parallel Scaled Dot-Product Attention layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_head (int): The number of attention heads.\n",
    "            d_model (int): The dimensionality of the input and output.\n",
    "            d_k (int): The dimensionality of the queries and keys.\n",
    "            d_v (int): The dimensionality of the values.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        # 1. Initial Projections for Q, K, V for all heads\n",
    "        # We create one large linear layer for each of Q, K, V.\n",
    "        # This is more efficient than creating n_head separate linear layers.\n",
    "        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False)\n",
    "        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False)\n",
    "        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)\n",
    "\n",
    "        # 3. Final Projection Layer\n",
    "        self.fc = nn.Linear(n_head * d_v, d_model, bias=False)\n",
    "\n",
    "        # 2. Parallel Attention Layer\n",
    "        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for Multi-Head Attention.\n",
    "\n",
    "        Args:\n",
    "            q, k, v (torch.Tensor): Input tensors. Shape: (batch_size, seq_len, d_model)\n",
    "            mask (torch.Tensor, optional): Mask for attention. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            output (torch.Tensor): Output tensor. Shape: (batch_size, seq_len, d_model)\n",
    "            attn (torch.Tensor): Attention weights. Shape: (batch_size, n_head, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
    "        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n",
    "\n",
    "        residual = q # Save the original query for the residual connection\n",
    "\n",
    "        # --- 1. Initial Projections & Reshape ---\n",
    "        # Pass inputs through linear layers and split into n_head.\n",
    "        # view() reshapes the tensor to (batch_size, seq_len, n_head, d_k/d_v)\n",
    "        # transpose(1, 2) swaps seq_len and n_head to get the desired shape for attention:\n",
    "        # (batch_size, n_head, seq_len, d_k/d_v)\n",
    "        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k).transpose(1, 2)\n",
    "        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k).transpose(1, 2)\n",
    "        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v).transpose(1, 2)\n",
    "\n",
    "        # If a mask is provided, expand it to match the n_head dimension.\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)   # Shape: (batch_size, 1, 1, len_k)\n",
    "\n",
    "        # --- 2. Parallel Attention ---\n",
    "        # q, k, v are now shape (batch_size, n_head, seq_len, d_k/d_v)\n",
    "        output, attn = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "        # --- 3. Concatenate & Final Projection ---\n",
    "        # Transpose back to (batch_size, len_q, n_head, d_v)\n",
    "        # contiguous() is needed to create a contiguous block of memory.\n",
    "        # reshape() or view() can then combine the last two dimensions.\n",
    "        output = output.transpose(1, 2).contiguous().view(sz_b, len_q, -1)\n",
    "        output = self.dropout(self.fc(output))\n",
    "\n",
    "        # --- Add & Norm (Residual Connection) ---\n",
    "        output += residual\n",
    "        output = self.layer_norm(output)\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "# --- Let's test it ---\n",
    "# Parameters\n",
    "d_model = 512\n",
    "n_head = 8\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "# Create an instance of the MHA module\n",
    "mha = MultiHeadAttention(n_head, d_model, d_k, d_v)\n",
    "\n",
    "# Create random input tensors\n",
    "q = torch.randn(batch_size, seq_len, d_model)\n",
    "k = torch.randn(batch_size, seq_len, d_model)\n",
    "v = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Get the output\n",
    "output, attn_weights = mha(q, k, v)\n",
    "\n",
    "# Print shapes to verify\n",
    "print(\"--- MHA Tensor Shapes ---\")\n",
    "print(\"Input (q, k, v):    \", q.shape)\n",
    "print(\"\\nOutput of MHA:      \", output.shape)\n",
    "print(\"Attention Weights:  \", attn_weights.shape)\n",
    "\n",
    "# --- Verification ---\n",
    "assert output.shape == (batch_size, seq_len, d_model)\n",
    "assert attn_weights.shape == (batch_size, n_head, seq_len, seq_len)\n",
    "print(\"\\nâœ… Shapes are correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d79f11",
   "metadata": {},
   "source": [
    "## \udfaf Part 4: Summary and Next Steps\n",
    "\n",
    "In this notebook, we have demystified the attention mechanism, the cornerstone of modern NLP models like the Transformer.\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "*   **Attention is a Mechanism for Focusing:** It allows a model to weigh the importance of different parts of the input sequence when producing an output, rather than relying on a single fixed-size context vector from an RNN.\n",
    "*   **Scaled Dot-Product Attention:** This is the most common implementation. It uses Query, Key, and Value matrices to calculate attention scores, which are then scaled and passed through a softmax to create a probability distribution (the attention weights). These weights are then used to create a weighted sum of the Value vectors.\n",
    "*   **Multi-Head Attention (MHA):** This is the standard way to use attention in Transformers. It runs multiple Scaled Dot-Product Attention layers in parallel, allowing the model to capture different types of relationships and features from the input data simultaneously. The results are then concatenated and linearly projected to produce the final output.\n",
    "*   **Residual Connections and Layer Normalization:** These are crucial components that are used alongside MHA to ensure stable training and effective information flow through deep networks.\n",
    "\n",
    "### What We Built:\n",
    "\n",
    "1.  A `ScaledDotProductAttention` module from scratch, understanding the core formula.\n",
    "2.  A `MultiHeadAttention` module that wraps the scaled dot-product attention and adds the necessary linear projections and parallel head logic.\n",
    "\n",
    "### Where to Go From Here:\n",
    "\n",
    "*   **Transformer Encoder:** The next logical step is to see how Multi-Head Attention is used inside a full Transformer Encoder block. This involves combining MHA with a Position-wise Feed-Forward Network.\n",
    "*   **Positional Encodings:** We haven't addressed *how* the model knows the order of the words. This is solved by adding positional encodings to the input embeddings before they are fed into the attention layers.\n",
    "*   **Full Transformer Model:** Combine multiple Encoder blocks to build a full Transformer for a task like sentiment analysis or machine translation.\n",
    "\n",
    "This notebook provides the fundamental building blocks. By understanding how Q, K, and V interact, you are well on your way to mastering the Transformer architecture."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
