{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dac41464",
   "metadata": {},
   "source": [
    "# üìù Notebook 03: Recurrent Neural Networks (RNNs) & LSTMs\n",
    "\n",
    "**Week 3-4: Deep Learning & NLP Foundations**  \n",
    "**Gen AI Masters Program**\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Objectives\n",
    "\n",
    "Welcome to the world of sequential data! So far, we've worked with data where each input is independent, like images or tabular data. But what about data where the order is crucial?\n",
    "-   **Text & Language**: The order of words defines meaning. \"Man bites dog\" is very different from \"Dog bites man.\"\n",
    "-   **Time Series**: Stock prices, weather patterns, and sensor readings are all sequences where past values influence future ones.\n",
    "-   **Audio & Video**: These are fundamentally sequences of sound waves or image frames.\n",
    "\n",
    "This is where **Recurrent Neural Networks (RNNs)** shine. Unlike the networks we've seen before, RNNs have a \"memory\" in the form of a **hidden state** that allows them to persist information across time steps, making them ideal for processing sequences.\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1.  **Understand the RNN Architecture**: Grasp the concept of a recurrent loop and how the hidden state acts as the network's memory.\n",
    "2.  **Identify the Vanishing/Exploding Gradient Problem**: Learn about the key challenge that makes it difficult for simple RNNs to learn long-range dependencies.\n",
    "3.  **Master the Long Short-Term Memory (LSTM) Cell**: Dive into the powerful LSTM architecture, a specialized RNN that uses a system of \"gates\" (Forget, Input, and Output) to effectively regulate information flow and overcome the vanishing gradient problem.\n",
    "4.  **Build an LSTM for Sentiment Analysis**: Apply your knowledge to a real-world Natural Language Processing (NLP) task. We will build and train an LSTM model from scratch using PyTorch to classify movie reviews from the IMDb dataset as positive or negative.\n",
    "5.  **Implement a Text Processing Pipeline**: Learn the essential steps to prepare text data for a neural network, including tokenization, building a vocabulary, numericalization, and padding.\n",
    "\n",
    "**Estimated Time:** 3-4 hours\n",
    "\n",
    "---\n",
    "\n",
    "## üìö What are RNNs and LSTMs?\n",
    "\n",
    "An RNN processes a sequence by iterating through its elements one by one. At each step, it takes the current input element and its hidden state from the previous step, and uses them to compute the new hidden state. This recurrent formula allows information to propagate through the sequence.\n",
    "\n",
    "However, simple RNNs struggle to \"remember\" information from many steps back due to the vanishing gradient problem. **Long Short-Term Memory (LSTM)** networks were created to solve this. LSTMs have a more complex internal structure, including a separate **cell state** and three gates that meticulously control what information is stored, updated, and read from the memory. This makes them exceptionally good at capturing long-range dependencies in data.\n",
    "\n",
    "Let's dive in and build one for our sentiment analysis task! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c318c271",
   "metadata": {},
   "source": [
    "## üöÄ Agenda\n",
    "\n",
    "Our journey into the world of recurrent networks will be structured as follows:\n",
    "\n",
    "1.  **Setting the Stage**: We'll import the necessary libraries, including `torch` and `torchtext`, and configure our device for GPU acceleration.\n",
    "2.  **Data Preparation for NLP**: We'll dive into the essential pipeline for processing text data. This is a critical skill for any NLP task.\n",
    "    *   **Loading Data**: Use `torchtext` to load the IMDb movie review dataset.\n",
    "    *   **Tokenization**: Break down sentences into individual words (tokens).\n",
    "    *   **Building a Vocabulary**: Create a numerical mapping for all unique words.\n",
    "    *   **Numericalization & Padding**: Convert sentences into integer sequences of a fixed length to be fed into the model.\n",
    "3.  **Building the LSTM Model**: We'll construct our sentiment analysis model layer by layer in PyTorch.\n",
    "    *   **Embedding Layer**: Learn dense vector representations for words.\n",
    "    *   **LSTM Layer**: The core of our network for processing the sequence of word embeddings.\n",
    "    *   **Fully Connected Layer**: The final classifier that makes the sentiment prediction.\n",
    "4.  **Training the Model**: We'll define the loss function (`BCEWithLogitsLoss`) and optimizer (`Adam`), and write a complete training and evaluation loop to train the model on our data.\n",
    "5.  **Using the Model for Inference**: We'll load our trained model and use it to predict the sentiment of new, unseen movie reviews.\n",
    "6.  **Conceptual Extensions**: We'll briefly discuss powerful variations like **GRUs** and **Bidirectional LSTMs** to provide context for further learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291be556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Set up the Environment ---\n",
    "\n",
    "# For this notebook, we'll be using `torchtext` to handle the IMDb dataset.\n",
    "# `torchtext` provides convenient tools for text processing in PyTorch.\n",
    "# The `portalocker` library is a dependency for `torchtext` data utilities,\n",
    "# ensuring that data loading is handled correctly.\n",
    "!pip install torchtext portalocker --quiet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Import torchtext components\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Set the default device\n",
    "# Training on a GPU is significantly faster for deep learning models.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"‚úÖ Using device: {device.upper()}\")\n",
    "\n",
    "# --- Plotting Style ---\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "sns.set_palette(\"colorblind\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1997a470",
   "metadata": {},
   "source": [
    "## üí¨ Part 2: Preparing the IMDb Dataset\n",
    "\n",
    "Before we can train a model, we need to convert our raw text data into a structured, numerical format that the network can understand. This preprocessing pipeline is a fundamental part of almost any NLP project.\n",
    "\n",
    "Here are the key steps we'll take:\n",
    "\n",
    "1.  **Load the Data**: We'll use `torchtext` to stream the IMDb dataset, which consists of 25,000 training reviews and 25,000 testing reviews. Each review is labeled as either positive (`2`) or negative (`1`). We will map these to `1` and `0`.\n",
    "\n",
    "2.  **Tokenization**: We'll break down each review (which is a long string) into a list of individual words or \"tokens.\" This process, called tokenization, is the first step in making sense of the text. We'll use a basic English tokenizer that splits text by spaces and punctuation.\n",
    "\n",
    "3.  **Build a Vocabulary**: We need to create a \"vocabulary,\" which is a dictionary that maps every unique word in our dataset to a unique integer. This allows us to represent our sentences as sequences of numbers. To keep things manageable, we'll limit the vocabulary to the `10,000` most frequent words. Words not in this vocabulary will be mapped to a special \"unknown\" token (`<unk>`).\n",
    "\n",
    "4.  **Numericalization & Padding**:\n",
    "    *   **Numericalization**: We'll use the vocabulary to convert our tokenized sentences into numerical sequences.\n",
    "    *   **Padding/Truncation**: RNNs require the sequences within a single batch to have the same length. We will enforce a `MAX_LEN` for all sequences. Reviews longer than this will be truncated, and shorter ones will be \"padded\" with a special padding token (`<pad>`) until they reach the desired length.\n",
    "\n",
    "Let's execute this pipeline. This is often the most time-consuming part of an NLP project, but `torchtext` makes it surprisingly straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6918b5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2.1. Load Data and Define Tokenizer ---\n",
    "print(\"Loading IMDb dataset and defining tokenizer...\")\n",
    "# `IMDB` returns iterators for the training and test datasets.\n",
    "# An iterator is a memory-efficient way to access data one item at a time.\n",
    "train_iter, test_iter = IMDB(split=('train', 'test'))\n",
    "# We'll use a basic English tokenizer from torchtext that splits sentences by spaces and punctuation.\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# --- 2.2. Build the Vocabulary ---\n",
    "# The vocabulary maps words to integer indices. We build it from the training data.\n",
    "VOCAB_SIZE = 10000  # We will only consider the top 10,000 most frequent words.\n",
    "UNK_TOKEN = \"<unk>\"   # Special token for unknown words (words not in our vocabulary).\n",
    "PAD_TOKEN = \"<pad>\"   # Special token for padding sequences to a fixed length.\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    \"\"\"\n",
    "    A helper generator function to yield tokens from the raw dataset iterator.\n",
    "    This is passed to the vocabulary builder.\n",
    "    \"\"\"\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# Reset the training iterator, as we will have consumed it in the function above.\n",
    "train_iter, _ = IMDB(split=('train', 'test'))\n",
    "\n",
    "print(f\"Building vocabulary with top {VOCAB_SIZE} words...\")\n",
    "# `build_vocab_from_iterator` handles the process of counting word frequencies\n",
    "# and creating the integer mapping.\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(train_iter),\n",
    "    specials=[UNK_TOKEN, PAD_TOKEN], # Add our special tokens to the vocabulary.\n",
    "    max_tokens=VOCAB_SIZE,           # Limit the vocabulary size.\n",
    "    special_first=True               # Ensure special tokens get indices 0 and 1.\n",
    ")\n",
    "# Set the default index for unknown words. If the vocabulary encounters a word\n",
    "# it hasn't seen, it will be mapped to the index of our UNK_TOKEN.\n",
    "vocab.set_default_index(vocab[UNK_TOKEN])\n",
    "\n",
    "pad_index = vocab[PAD_TOKEN] # Get the integer index for the padding token.\n",
    "\n",
    "print(f\"‚úÖ Vocabulary created. Size: {len(vocab)}\")\n",
    "print(f\"   - Index for '<unk>': {vocab[UNK_TOKEN]}\")\n",
    "print(f\"   - Index for '<pad>': {vocab[PAD_TOKEN]}\")\n",
    "print(f\"   - Example mapping: 'hello' -> {vocab['hello']}\")\n",
    "\n",
    "\n",
    "# --- 2.3. Define the Data Processing and Batching Pipeline ---\n",
    "MAX_LEN = 250    # Maximum sequence length. Longer reviews will be truncated, shorter ones padded.\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def create_dataloader(data_iter, is_train=True):\n",
    "    \"\"\"\n",
    "    A function to convert the raw data iterator into a batched and padded DataLoader.\n",
    "    \"\"\"\n",
    "    all_texts, all_labels = [], []\n",
    "    \n",
    "    print(f\"Processing data... (This may take a moment)\")\n",
    "    for label, text in tqdm(data_iter, desc=\"Numericalizing and Padding\"):\n",
    "        # Convert text to a list of integer indices using the vocabulary.\n",
    "        numericalized_text = vocab(tokenizer(text))\n",
    "        \n",
    "        # Truncate the sequence if it's longer than MAX_LEN.\n",
    "        if len(numericalized_text) > MAX_LEN:\n",
    "            numericalized_text = numericalized_text[:MAX_LEN]\n",
    "            \n",
    "        all_texts.append(torch.tensor(numericalized_text, dtype=torch.int64))\n",
    "        \n",
    "        # The original labels are 1 (neg) and 2 (pos). We map them to 0 and 1.\n",
    "        all_labels.append(label - 1)\n",
    "\n",
    "    # `pad_sequence` takes a list of tensors of different lengths and stacks them\n",
    "    # into a single tensor, padding them all to the length of the longest sequence\n",
    "    # in the batch (or to a max length if specified).\n",
    "    # `batch_first=True` makes the output shape [batch_size, sequence_length].\n",
    "    padded_texts = pad_sequence(all_texts, batch_first=True, padding_value=pad_index)\n",
    "    \n",
    "    # Create a standard PyTorch TensorDataset and DataLoader.\n",
    "    dataset = TensorDataset(padded_texts, torch.tensor(all_labels, dtype=torch.float32))\n",
    "    return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=is_train)\n",
    "\n",
    "# --- Create the DataLoaders ---\n",
    "# We need to get fresh iterators since they get consumed during processing.\n",
    "train_iter, test_iter = IMDB(split=('train', 'test'))\n",
    "\n",
    "train_dataloader = create_dataloader(train_iter, is_train=True)\n",
    "test_dataloader = create_dataloader(test_iter, is_train=False)\n",
    "\n",
    "print(\"\\n‚úÖ Data preparation complete.\")\n",
    "print(f\"   - Number of training batches: {len(train_dataloader)}\")\n",
    "print(f\"   - Number of testing batches:  {len(test_dataloader)}\")\n",
    "\n",
    "# --- Inspect a Single Batch ---\n",
    "texts, labels = next(iter(train_dataloader))\n",
    "print(f\"\\nShape of a single batch of texts: {texts.shape}\")\n",
    "print(f\"Shape of a single batch of labels: {labels.shape}\")\n",
    "print(f\"Data type of texts: {texts.dtype}\")\n",
    "print(f\"Data type of labels: {labels.dtype}\")\n",
    "print(\"\\nThe first review in the batch (as integers):\")\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a19b3bb",
   "metadata": {},
   "source": [
    "## üß† Part 3: Building the LSTM Model\n",
    "\n",
    "Now for the exciting part! We'll construct our sentiment analysis model using an **LSTM**. Our model will be a `nn.Module` subclass, and its architecture will consist of three main layers:\n",
    "\n",
    "1.  **Embedding Layer (`nn.Embedding`):**\n",
    "    *   This is the crucial first layer in any NLP model. Its job is to transform the integer indices (our numericalized words) into dense vectors of a fixed size (`embedding_dim`).\n",
    "    *   These vectors are called **word embeddings**. Unlike sparse one-hot encodings, embeddings are dense and can capture semantic relationships between words (e.g., the vectors for \"king\" and \"queen\" might be very close to each other in the embedding space).\n",
    "    *   Most importantly, these embedding vectors are **learnable parameters**. The network will learn the optimal representation for each word in our vocabulary to best solve the sentiment analysis task. It starts with random vectors and adjusts them during training via backpropagation.\n",
    "\n",
    "2.  **LSTM Layer (`nn.LSTM`):**\n",
    "    *   This is the recurrent core of our model. It will process the sequence of embedding vectors one by one.\n",
    "    *   It maintains a **hidden state** and a **cell state**, which are updated at each time step, allowing it to remember information from earlier in the sequence.\n",
    "    *   We'll configure it with a few key parameters:\n",
    "        *   `input_size`: The size of each input vector (our `embedding_dim`).\n",
    "        *   `hidden_size`: The dimension of the hidden and cell states.\n",
    "        *   `num_layers`: We can stack multiple LSTM layers on top of each other to create a deeper model, which can learn more complex patterns.\n",
    "        *   `batch_first=True`: This tells the layer to expect input tensors with the shape `[batch_size, seq_len, feature_dim]`, which matches our `DataLoader`'s output.\n",
    "        *   `dropout`: We'll add dropout between the LSTM layers (if `num_layers > 1`) to help prevent overfitting.\n",
    "\n",
    "3.  **Fully Connected Layer (`nn.Linear`):**\n",
    "    *   The LSTM layer outputs a sequence of hidden states, one for each time step. For our classification task, we are primarily interested in the **final hidden state** of the last layer, as it represents a summary of the entire review.\n",
    "    *   We'll pass this final hidden state to a linear layer, which will map it from the `hidden_dim` to a single output value (`output_dim=1`). This single value is our raw prediction, or \"logit.\"\n",
    "\n",
    "The output logit will then be passed to our loss function (`BCEWithLogitsLoss`), which will internally apply a sigmoid function to get a probability and calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7e6015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Define the LSTM Model ---\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    An LSTM-based neural network for sentiment classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
    "        \"\"\"\n",
    "        Initializes the layers of the model.\n",
    "        \n",
    "        Args:\n",
    "            vocab_size (int): The size of the vocabulary.\n",
    "            embedding_dim (int): The dimension of the word embeddings.\n",
    "            hidden_dim (int): The size of the LSTM's hidden state.\n",
    "            output_dim (int): The dimension of the output (1 for binary classification).\n",
    "            n_layers (int): The number of stacked LSTM layers.\n",
    "            dropout (float): The dropout probability for regularization.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Embedding Layer\n",
    "        # This layer maps integer word indices to dense, learnable vectors.\n",
    "        # `padding_idx` tells the layer to ignore the padding token during training\n",
    "        # and not update its embedding.\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_index)\n",
    "        \n",
    "        # 2. LSTM Layer\n",
    "        # This is the recurrent core that processes the sequence of embeddings.\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout if n_layers > 1 else 0, # Dropout is only applied between LSTM layers.\n",
    "            batch_first=True  # Crucial: ensures input format is [batch, seq_len, features].\n",
    "        )\n",
    "        \n",
    "        # 3. Dropout Layer\n",
    "        # A general dropout layer applied to the LSTM's output for further regularization.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 4. Fully Connected (Linear) Layer\n",
    "        # This layer maps the final LSTM hidden state to the desired output dimension.\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            text (Tensor): A batch of text sequences, shape [batch_size, seq_len].\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The model's raw output (logits), shape [batch_size].\n",
    "        \"\"\"\n",
    "        # text shape: [batch_size, seq_len]\n",
    "        \n",
    "        # Pass text through the embedding layer.\n",
    "        embedded = self.embedding(text)\n",
    "        # embedded shape: [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # Pass the embeddings through the LSTM.\n",
    "        # `lstm_out` contains the hidden state for every time step.\n",
    "        # `hidden` is a tuple containing the final hidden state and cell state of the last time step.\n",
    "        # lstm_out shape: [batch_size, seq_len, hidden_dim]\n",
    "        # hidden shape: ([n_layers, batch_size, hidden_dim], [n_layers, batch_size, hidden_dim])\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # We are interested in the final hidden state of the *last* layer, as it summarizes the whole sequence.\n",
    "        # `hidden` has shape [n_layers, batch_size, hidden_dim], so we take the last layer's state with `hidden[-1]`.\n",
    "        final_hidden_state = self.dropout(hidden[-1])\n",
    "        # final_hidden_state shape: [batch_size, hidden_dim]\n",
    "        \n",
    "        # Pass the final hidden state through the fully connected layer to get the final prediction.\n",
    "        output = self.fc(final_hidden_state)\n",
    "        # output shape: [batch_size, output_dim]\n",
    "        \n",
    "        # We squeeze the output to remove the last dimension, resulting in a shape of [batch_size].\n",
    "        return output.squeeze(1)\n",
    "\n",
    "# --- Instantiate the Model ---\n",
    "# Define the hyperparameters for our model. These values are a good starting point.\n",
    "EMBEDDING_DIM = 100   # The size of the dense word vectors.\n",
    "HIDDEN_DIM = 256      # The number of features in the LSTM's hidden state.\n",
    "OUTPUT_DIM = 1        # The output dimension (1 for binary classification: a single logit).\n",
    "N_LAYERS = 2          # The number of stacked LSTM layers.\n",
    "DROPOUT = 0.5         # The dropout rate for regularization.\n",
    "\n",
    "# Create an instance of our LSTM model.\n",
    "model = SentimentLSTM(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(device) # Move the model to the configured device (GPU or CPU).\n",
    "\n",
    "print(\"‚úÖ LSTM Model created and moved to device.\")\n",
    "\n",
    "# --- Count Model Parameters ---\n",
    "def count_parameters(model):\n",
    "    \"\"\"A helper function to count the number of trainable parameters in a model.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'\\nThe model has {count_parameters(model):,} trainable parameters.')\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bdc5a9",
   "metadata": {},
   "source": [
    "## üöÄ Part 4: Training the Model\n",
    "\n",
    "With our data prepared and our model built, it's time to start the training process. This involves defining our optimization strategy and then creating a loop to feed data to the model and update its weights.\n",
    "\n",
    "Here's our game plan:\n",
    "\n",
    "1.  **Define the Optimizer:** We'll use the **Adam** optimizer, a robust and popular choice for training deep learning models. It will be responsible for updating our model's weights based on the calculated gradients, using an adaptive learning rate.\n",
    "\n",
    "2.  **Choose the Loss Function:** Since our output is a single logit for a binary classification problem, the perfect loss function is **`BCEWithLogitsLoss`**. This function is highly recommended because it combines a `Sigmoid` activation and the Binary Cross-Entropy loss in a single, numerically stable function. It takes the raw, un-squashed outputs (logits) from our model, which is exactly what our `SentimentLSTM` provides.\n",
    "\n",
    "3.  **Create the Training Loop:** We'll loop through our training data for a set number of **epochs**. In each epoch, we will:\n",
    "    *   Iterate through the batches of data in our `train_dataloader`.\n",
    "    *   For each batch, perform the core training steps:\n",
    "        *   Set the model to training mode (`model.train()`).\n",
    "        *   Clear any old gradients from the last step (`optimizer.zero_grad()`).\n",
    "        *   Move the data batch to the correct device (GPU/CPU).\n",
    "        *   Make a **forward pass**: feed the batch to the model to get predictions (logits).\n",
    "        *   Calculate the **loss**: compare the model's predictions with the true labels using our criterion.\n",
    "        *   Perform a **backward pass**: compute the gradients of the loss with respect to all model parameters (`loss.backward()`).\n",
    "        *   **Update the weights**: instruct the optimizer to take a step, updating the weights based on the computed gradients (`optimizer.step()`).\n",
    "        *   Calculate the accuracy for the batch to monitor performance as we train.\n",
    "\n",
    "4.  **Create the Evaluation Loop:** After each epoch of training, we'll evaluate our model's performance on the unseen test data. This is crucial for checking how well the model is generalizing and for spotting overfitting. The steps are similar to the training loop but without any gradient computation or weight updates.\n",
    "    *   Set the model to evaluation mode (`model.eval()`).\n",
    "    *   Wrap the entire loop in a `with torch.no_grad():` block to disable gradient calculation, which saves memory and computation.\n",
    "    *   Calculate the average loss and accuracy over the entire test set.\n",
    "\n",
    "Let's write the code to bring this to life."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0fb264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.1. Define Optimizer, Loss Function, and Accuracy Metric ---\n",
    "\n",
    "# We'll use the Adam optimizer, a popular and effective default choice.\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# We use BCEWithLogitsLoss, which combines a Sigmoid layer and BCELoss in one.\n",
    "# This is numerically more stable than using a plain Sigmoid followed by BCELoss.\n",
    "# It expects the raw logits from our model as input.\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Calculates accuracy for binary classification.\n",
    "    \n",
    "    Args:\n",
    "        preds (Tensor): The raw logits from the model.\n",
    "        y (Tensor): The true labels (0 or 1).\n",
    "        \n",
    "    Returns:\n",
    "        float: The accuracy as a percentage.\n",
    "    \"\"\"\n",
    "    # Apply sigmoid to get probabilities and round to get predictions (0 or 1).\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    # Check if predictions are equal to the true labels.\n",
    "    correct = (rounded_preds == y).float()\n",
    "    # Calculate the mean accuracy.\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "# --- 4.2. Define Training and Evaluation Functions ---\n",
    "\n",
    "def train_one_epoch(model, iterator, optimizer, criterion):\n",
    "    \"\"\"Defines a single training epoch.\"\"\"\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()  # Set the model to training mode.\n",
    "    \n",
    "    for batch_texts, batch_labels in tqdm(iterator, desc=\"Training\"):\n",
    "        # Move data to the correct device.\n",
    "        batch_texts, batch_labels = batch_texts.to(device), batch_labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Clear old gradients.\n",
    "        \n",
    "        # Forward pass.\n",
    "        predictions = model(batch_texts)\n",
    "        \n",
    "        # Calculate loss and accuracy.\n",
    "        loss = criterion(predictions, batch_labels)\n",
    "        acc = binary_accuracy(predictions, batch_labels)\n",
    "        \n",
    "        # Backward pass and optimization.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate_one_epoch(model, iterator, criterion):\n",
    "    \"\"\"Defines the evaluation loop.\"\"\"\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()  # Set the model to evaluation mode.\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculations for efficiency.\n",
    "        for batch_texts, batch_labels in tqdm(iterator, desc=\"Evaluating\"):\n",
    "            batch_texts, batch_labels = batch_texts.to(device), batch_labels.to(device)\n",
    "            \n",
    "            predictions = model(batch_texts)\n",
    "            \n",
    "            loss = criterion(predictions, batch_labels)\n",
    "            acc = binary_accuracy(predictions, batch_labels)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "# --- 4.3. Run the Training Loop ---\n",
    "N_EPOCHS = 5\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "print(\"üöÄ Starting training...\")\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    # Train the model for one epoch.\n",
    "    train_loss, train_acc = train_one_epoch(model, train_dataloader, optimizer, criterion)\n",
    "    \n",
    "    # Evaluate the model on the validation set.\n",
    "    valid_loss, valid_acc = evaluate_one_epoch(model, test_dataloader, criterion)\n",
    "    \n",
    "    # Store history.\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(valid_loss)\n",
    "    history['val_acc'].append(valid_acc)\n",
    "    \n",
    "    # Check if this is the best model we've seen so far based on validation loss.\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        # Save the model's state dictionary. This saves the learned weights.\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "    \n",
    "    print(f'\\nEpoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "print(\"\\n‚úÖ Training complete.\")\n",
    "print(f\"Best validation loss: {best_valid_loss:.3f}\")\n",
    "print(\"The best model has been saved to 'best_model.pt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81681ad",
   "metadata": {},
   "source": [
    "## üé¨ Part 5: Using the Model for Inference\n",
    "\n",
    "Now that we have a trained model, let's use it for its intended purpose: predicting the sentiment of new, unseen sentences. We'll create a function that encapsulates the entire inference pipeline, from raw text to a sentiment prediction.\n",
    "\n",
    "The steps for inference must mirror the preprocessing steps used for training:\n",
    "1.  **Load the Best Model:** We'll load the weights from `best_model.pt`, which we saved during training as it had the lowest validation loss.\n",
    "2.  **Set to Evaluation Mode:** It's crucial to call `model.eval()` to turn off dropout and other training-specific layers.\n",
    "3.  **Preprocessing the Input Sentence:** The new sentence must be processed in the exact same way as the training data. This includes:\n",
    "    *   **Tokenizing** the sentence into a list of words.\n",
    "    *   **Numericalizing** the tokens using our existing vocabulary to get a list of integer indices.\n",
    "    *   **Converting** the list of indices to a PyTorch tensor.\n",
    "    *   **Adding a \"batch\" dimension**, as the model was trained on batches of sentences and therefore expects a batch dimension (e.g., `[1, seq_len]`).\n",
    "4.  **Prediction:**\n",
    "    *   Pass the processed tensor through the model within a `torch.no_grad()` block.\n",
    "    *   Apply the **sigmoid function** to the output logit to get a probability score between 0 and 1.\n",
    "    *   Interpret the score to classify the sentiment as \"Positive\" or \"Negative\" based on a 0.5 threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad8d703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.1. Load the Best Performing Model ---\n",
    "print(\"Loading the best model from 'best_model.pt'...\")\n",
    "# We create a new instance of the model and then load the saved weights into it.\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "model.to(device) # Ensure the model is on the correct device.\n",
    "print(\"‚úÖ Model loaded successfully.\")\n",
    "\n",
    "# --- 5.2. Create the Prediction Function ---\n",
    "def predict_sentiment(sentence):\n",
    "    \"\"\"\n",
    "    Predicts the sentiment of a given sentence using the trained model.\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): The input sentence (e.g., a movie review).\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing the predicted sentiment (\"Positive\" or \"Negative\")\n",
    "               and the raw probability score.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode (disables dropout, etc.).\n",
    "    \n",
    "    # 1. Tokenize and Numericalize the input sentence.\n",
    "    tokenized = tokenizer(sentence)\n",
    "    indexed = vocab(tokenized)\n",
    "    \n",
    "    # 2. Convert to a Tensor and Add the Batch Dimension.\n",
    "    # The model expects a batch of data, so we use `unsqueeze(0)` to add a\n",
    "    # batch dimension of size 1, changing the shape from [seq_len] to [1, seq_len].\n",
    "    tensor = torch.LongTensor(indexed).unsqueeze(0).to(device)\n",
    "    \n",
    "    # 3. Get the Prediction from the Model.\n",
    "    # We use `torch.no_grad()` to ensure no gradients are calculated.\n",
    "    with torch.no_grad():\n",
    "        prediction = model(tensor)\n",
    "    \n",
    "    # 4. Convert the Raw Logit to a Probability.\n",
    "    # The model outputs a raw logit, which we pass through a sigmoid function\n",
    "    # to get a probability score between 0 and 1.\n",
    "    probability = torch.sigmoid(prediction).item()\n",
    "    \n",
    "    # 5. Classify and Return the Result.\n",
    "    sentiment = \"Positive\" if probability > 0.5 else \"Negative\"\n",
    "    return sentiment, probability\n",
    "\n",
    "# --- 5.3. Test with Example Sentences ---\n",
    "print(\"\\n--- Testing with a positive review ---\")\n",
    "positive_review = \"This movie was fantastic! The acting was superb and the plot was gripping. I would watch it again in a heartbeat.\"\n",
    "sentiment, prob = predict_sentiment(positive_review)\n",
    "print(f\"Sentence:  '{positive_review}'\")\n",
    "print(f\"Sentiment: {sentiment}\")\n",
    "print(f\"Probability (Positive): {prob:.4f}\\n\")\n",
    "\n",
    "print(\"--- Testing with a negative review ---\")\n",
    "negative_review = \"I was really disappointed with this film. It was boring, the story made no sense, and I almost fell asleep.\"\n",
    "sentiment, prob = predict_sentiment(negative_review)\n",
    "print(f\"Sentence:  '{negative_review}'\")\n",
    "print(f\"Sentiment: {sentiment}\")\n",
    "print(f\"Probability (Positive): {prob:.4f}\\n\")\n",
    "\n",
    "print(\"--- Testing with a neutral/ambiguous review ---\")\n",
    "ambiguous_review = \"The movie was okay, I guess. Some parts were good, others not so much. I'm not sure if I would recommend it.\"\n",
    "sentiment, prob = predict_sentiment(ambiguous_review)\n",
    "print(f\"Sentence:  '{ambiguous_review}'\")\n",
    "print(f\"Sentiment: {sentiment}\")\n",
    "print(f\"Probability (Positive): {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44932dba",
   "metadata": {},
   "source": [
    "## üí° Part 6: Conceptual Extensions - GRUs and Bidirectional RNNs\n",
    "\n",
    "While our LSTM model is very powerful, it's helpful to be aware of a few common and powerful variations in the world of recurrent networks.\n",
    "\n",
    "### 1. Gated Recurrent Unit (GRU)\n",
    "\n",
    "The **GRU** is a newer and slightly simpler alternative to the LSTM. It was designed to solve the same vanishing gradient problem but with a more streamlined architecture.\n",
    "\n",
    "-   **Fewer Gates**: A GRU has only two gates (a **Reset Gate** and an **Update Gate**), whereas an LSTM has three (Forget, Input, Output).\n",
    "-   **Combined Cell and Hidden State**: It doesn't have a separate cell state like the LSTM; it only uses the hidden state to transfer information.\n",
    "\n",
    "**Advantages**:\n",
    "-   **Fewer Parameters**: Because it's simpler, a GRU has fewer weights to train. This can make it **faster** and more computationally efficient.\n",
    "-   **Similar Performance**: For many tasks, GRUs perform just as well as LSTMs, and their simplicity can make them easier to work with. They are a very popular choice in modern NLP.\n",
    "\n",
    "### 2. Bidirectional RNNs\n",
    "\n",
    "When processing a sequence, our standard LSTM only looks at the past and present‚Äîthe hidden state at time `t` only contains information from time steps `1, 2, ..., t`.\n",
    "\n",
    "But what if future context is also important? Consider the sentence:\n",
    "> \"The man who was previously sitting next to the window **stood** up.\"\n",
    "\n",
    "To understand the verb \"stood,\" knowing what comes *after* it is just as important as what came before.\n",
    "\n",
    "A **Bidirectional RNN** addresses this by processing the sequence in two directions at once:\n",
    "1.  A **forward RNN** reads the sequence from left to right.\n",
    "2.  A **backward RNN** reads the sequence from right to left.\n",
    "\n",
    "The hidden states from both directions are then concatenated at each time step. This gives the model a richer, more complete understanding of the context surrounding every word in the sequence.\n",
    "\n",
    "**When to use it?** Bidirectional models are the standard for many NLP tasks like sentiment analysis, named entity recognition, and question answering, where the full context of a word is crucial for making a correct prediction. You can easily make a PyTorch LSTM or GRU bidirectional by setting the `bidirectional=True` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d33a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU Model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "        super(GRUModel, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # GRU layer\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, hn = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Compare LSTM vs GRU\n",
    "lstm_model = LSTMModel(input_size=4, hidden_size=64, num_layers=2, output_size=1)\n",
    "gru_model = GRUModel(input_size=4, hidden_size=64, num_layers=2, output_size=1)\n",
    "\n",
    "lstm_params = sum(p.numel() for p in lstm_model.parameters())\n",
    "gru_params = sum(p.numel() for p in gru_model.parameters())\n",
    "\n",
    "print(\"‚öñÔ∏è LSTM vs GRU Comparison\")\n",
    "print(\"=\"*60)\n",
    "print(f\"LSTM parameters: {lstm_params:,}\")\n",
    "print(f\"GRU parameters:  {gru_params:,}\")\n",
    "print(f\"\\nGRU has {lstm_params - gru_params:,} fewer parameters ({100*(1-gru_params/lstm_params):.1f}% reduction)\")\n",
    "print(\"\\n‚úÖ GRU is more efficient!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab540cb",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Bidirectional RNNs\n",
    "\n",
    "### Why Bidirectional?\n",
    "\n",
    "Sometimes **future context** helps understand the past!\n",
    "\n",
    "Example: \"The animal didn't cross the street because it was too **tired**\"\n",
    "- Need to see \"tired\" to understand \"it\" refers to the animal\n",
    "\n",
    "Bidirectional RNNs process sequences in **both directions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fe9f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional LSTM\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True  # Key difference!\n",
    "        )\n",
    "        \n",
    "        # FC layer (hidden_size * 2 because bidirectional)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # num_directions = 2 for bidirectional\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Create bidirectional model\n",
    "bi_lstm = BiLSTM(input_size=4, hidden_size=64, num_layers=2, output_size=1)\n",
    "\n",
    "print(\"‚ÜîÔ∏è Bidirectional LSTM\")\n",
    "print(\"=\"*60)\n",
    "print(bi_lstm)\n",
    "print(f\"\\nParameters: {sum(p.numel() for p in bi_lstm.parameters()):,}\")\n",
    "print(\"\\n‚úÖ BiLSTM uses both past AND future context!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb3e085",
   "metadata": {},
   "source": [
    "## üéâ Summary\n",
    "\n",
    "Congratulations! You've mastered RNNs and LSTMs!\n",
    "\n",
    "### Key Concepts\n",
    "- ‚úÖ RNN architecture and hidden states\n",
    "- ‚úÖ Vanishing gradient problem\n",
    "- ‚úÖ LSTM gates (forget, input, output)\n",
    "- ‚úÖ GRU (simplified LSTM)\n",
    "- ‚úÖ Bidirectional RNNs\n",
    "- ‚úÖ Sequence prediction\n",
    "\n",
    "### What You Built\n",
    "1. üîÑ Simple RNN from scratch\n",
    "2. üß† LSTM temperature predictor\n",
    "3. ‚ö° GRU model\n",
    "4. ‚ÜîÔ∏è Bidirectional LSTM\n",
    "\n",
    "### RNN Applications\n",
    "- üè≠ **Manufacturing**: Equipment monitoring, predictive maintenance\n",
    "- üìà **Finance**: Stock prediction, fraud detection\n",
    "- üéµ **Audio**: Speech recognition, music generation\n",
    "- üìù **Text**: Language modeling, translation\n",
    "\n",
    "### Comparison Table\n",
    "\n",
    "| Model | Parameters | Speed | Long-term Memory | Use Case |\n",
    "|-------|-----------|-------|------------------|----------|\n",
    "| Simple RNN | Low | Fast | Poor | Short sequences |\n",
    "| LSTM | High | Slow | Excellent | Long sequences |\n",
    "| GRU | Medium | Medium | Very Good | Balanced |\n",
    "| BiLSTM | Highest | Slowest | Best | Full context needed |\n",
    "\n",
    "### Next Steps\n",
    "Continue to **Notebook 04: Transformers** to learn the architecture that revolutionized NLP!\n",
    "\n",
    "<div align=\"center\">\n",
    "<b>RNNs & LSTMs mastered! Ready for Transformers! üöÄ</b>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
