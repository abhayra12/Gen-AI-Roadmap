{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dac41464",
   "metadata": {},
   "source": [
    "# üîÑ Notebook 03: RNNs and LSTMs\n",
    "\n",
    "**Week 3-4: Deep Learning & NLP Foundations**  \n",
    "**Gen AI Masters Program**\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Objectives\n",
    "\n",
    "By the end of this notebook, you will master:\n",
    "1. ‚úÖ Recurrent Neural Networks (RNN) architecture\n",
    "2. ‚úÖ Sequence modeling and time series\n",
    "3. ‚úÖ Vanishing gradient problem\n",
    "4. ‚úÖ Long Short-Term Memory (LSTM) networks\n",
    "5. ‚úÖ Gated Recurrent Units (GRU)\n",
    "6. ‚úÖ Bidirectional RNNs\n",
    "7. ‚úÖ Real-world sequence prediction\n",
    "\n",
    "**Estimated Time:** 3-4 hours\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Why RNNs?\n",
    "\n",
    "CNNs work great for images, but what about **sequential data**?\n",
    "- üìà Time series (sensor readings, stock prices)\n",
    "- üìù Text (sentences, documents)\n",
    "- üéµ Audio (speech, music)\n",
    "- üè≠ **Manufacturing logs** (our focus!)\n",
    "\n",
    "**Key Insight**: RNNs have **memory** - they remember previous inputs!\n",
    "\n",
    "Let's dive in! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291be556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1997a470",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Understanding RNN Architecture\n",
    "\n",
    "### How RNNs Work\n",
    "\n",
    "Unlike feedforward networks, RNNs process sequences **step by step**:\n",
    "\n",
    "```\n",
    "h(t) = tanh(W_hh * h(t-1) + W_xh * x(t) + b)\n",
    "y(t) = W_hy * h(t) + b\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `h(t)`: Hidden state at time t\n",
    "- `x(t)`: Input at time t\n",
    "- `y(t)`: Output at time t\n",
    "\n",
    "The **hidden state** carries information forward!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6918b5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple RNN from scratch\n",
    "class SimpleRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Simple RNN implementation\n",
    "        \n",
    "        Args:\n",
    "            input_size: Dimension of input features\n",
    "            hidden_size: Dimension of hidden state\n",
    "            output_size: Dimension of output\n",
    "        \"\"\"\n",
    "        # Initialize weights\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # W_xh: input to hidden\n",
    "        self.W_xh = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        \n",
    "        # W_hh: hidden to hidden (recurrent)\n",
    "        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        \n",
    "        # W_hy: hidden to output\n",
    "        self.W_hy = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        \n",
    "        # Biases\n",
    "        self.b_h = np.zeros((1, hidden_size))\n",
    "        self.b_y = np.zeros((1, output_size))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass through RNN\n",
    "        \n",
    "        Args:\n",
    "            inputs: Sequence of inputs (seq_len, input_size)\n",
    "        \n",
    "        Returns:\n",
    "            outputs: Sequence of outputs\n",
    "            hidden_states: All hidden states\n",
    "        \"\"\"\n",
    "        h = np.zeros((1, self.hidden_size))  # Initial hidden state\n",
    "        outputs = []\n",
    "        hidden_states = [h]\n",
    "        \n",
    "        for x in inputs:\n",
    "            x = x.reshape(1, -1)\n",
    "            \n",
    "            # Update hidden state\n",
    "            h = np.tanh(np.dot(x, self.W_xh) + np.dot(h, self.W_hh) + self.b_h)\n",
    "            \n",
    "            # Compute output\n",
    "            y = np.dot(h, self.W_hy) + self.b_y\n",
    "            \n",
    "            outputs.append(y)\n",
    "            hidden_states.append(h)\n",
    "        \n",
    "        return np.array(outputs), np.array(hidden_states)\n",
    "\n",
    "# Test SimpleRNN\n",
    "rnn = SimpleRNN(input_size=3, hidden_size=5, output_size=2)\n",
    "\n",
    "# Create a simple sequence\n",
    "sequence = np.array([\n",
    "    [1.0, 0.5, 0.3],  # t=0\n",
    "    [0.8, 0.6, 0.4],  # t=1\n",
    "    [0.9, 0.7, 0.2],  # t=2\n",
    "    [0.7, 0.8, 0.5],  # t=3\n",
    "])\n",
    "\n",
    "outputs, hidden_states = rnn.forward(sequence)\n",
    "\n",
    "print(\"üîÑ Simple RNN Test\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Input sequence shape: {sequence.shape}\")\n",
    "print(f\"Output shape: {outputs.shape}\")\n",
    "print(f\"Hidden states shape: {hidden_states.shape}\")\n",
    "print(f\"\\nFirst output: {outputs[0]}\")\n",
    "print(f\"Last output: {outputs[-1]}\")\n",
    "print(\"\\n‚úÖ RNN processes sequences step-by-step!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a19b3bb",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ The Vanishing Gradient Problem\n",
    "\n",
    "### Why Simple RNNs Fail\n",
    "\n",
    "RNNs struggle with **long sequences** because:\n",
    "- Gradients shrink exponentially (vanishing)\n",
    "- Or explode exponentially (exploding)\n",
    "- Can't learn long-term dependencies\n",
    "\n",
    "**Solution**: LSTM & GRU with gating mechanisms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7e6015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate vanishing gradient\n",
    "def compute_gradient_flow(sequence_length):\n",
    "    \"\"\"\n",
    "    Simulate gradient magnitude through time\n",
    "    \"\"\"\n",
    "    W = 0.5  # Weight < 1 causes vanishing\n",
    "    gradient = 1.0\n",
    "    gradients = [gradient]\n",
    "    \n",
    "    for t in range(sequence_length):\n",
    "        gradient *= W  # Gradient flows backward\n",
    "        gradients.append(gradient)\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "# Compute for different sequence lengths\n",
    "seq_lengths = [10, 20, 30, 40, 50]\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "for seq_len in seq_lengths:\n",
    "    grads = compute_gradient_flow(seq_len)\n",
    "    plt.plot(range(len(grads)), grads, marker='o', label=f'Seq Len = {seq_len}', linewidth=2)\n",
    "\n",
    "plt.axhline(y=0.01, color='r', linestyle='--', label='Vanishing Threshold', linewidth=2)\n",
    "plt.xlabel('Time Steps Backward', fontweight='bold', fontsize=12)\n",
    "plt.ylabel('Gradient Magnitude', fontweight='bold', fontsize=12)\n",
    "plt.title('Vanishing Gradient Problem in RNNs (W=0.5)', fontweight='bold', fontsize=14)\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚ö†Ô∏è Vanishing Gradient Problem:\")\n",
    "print(\"   ‚Ä¢ Gradients shrink exponentially\")\n",
    "print(\"   ‚Ä¢ Can't learn from distant past\")\n",
    "print(\"   ‚Ä¢ Need LSTM/GRU to solve this!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bdc5a9",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ LSTM Networks\n",
    "\n",
    "### LSTM Architecture\n",
    "\n",
    "LSTM uses **gates** to control information flow:\n",
    "\n",
    "1. **Forget Gate**: What to forget from cell state\n",
    "2. **Input Gate**: What new info to add\n",
    "3. **Output Gate**: What to output\n",
    "\n",
    "```\n",
    "f(t) = œÉ(W_f * [h(t-1), x(t)] + b_f)  # Forget gate\n",
    "i(t) = œÉ(W_i * [h(t-1), x(t)] + b_i)  # Input gate\n",
    "CÃÉ(t) = tanh(W_C * [h(t-1), x(t)] + b_C)  # Candidate\n",
    "C(t) = f(t) * C(t-1) + i(t) * CÃÉ(t)  # Cell state\n",
    "o(t) = œÉ(W_o * [h(t-1), x(t)] + b_o)  # Output gate\n",
    "h(t) = o(t) * tanh(C(t))  # Hidden state\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0fb264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM with PyTorch\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "        \"\"\"\n",
    "        LSTM model for sequence prediction\n",
    "        \n",
    "        Args:\n",
    "            input_size: Number of input features\n",
    "            hidden_size: Number of LSTM units\n",
    "            num_layers: Number of LSTM layers\n",
    "            output_size: Number of output features\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Take the output from the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Create LSTM model\n",
    "lstm_model = LSTMModel(\n",
    "    input_size=10,\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    output_size=1\n",
    ")\n",
    "\n",
    "print(\"üß† LSTM Model Architecture\")\n",
    "print(\"=\"*60)\n",
    "print(lstm_model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in lstm_model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randn(32, 20, 10)  # (batch, seq_len, features)\n",
    "output = lstm_model(test_input)\n",
    "print(f\"\\nInput shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"\\n‚úÖ LSTM handles variable-length sequences!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81681ad",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Real-World Example: Predictive Maintenance for Manufacturing Copilot\n",
    "\n",
    "### Use Case\n",
    "Our **Manufacturing Copilot** needs to predict potential equipment failures. We will build an LSTM model to predict a machine's `temperature` based on a sequence of its recent sensor readings (`temperature`, `vibration`, `pressure`, `rpm`). An accurate temperature prediction can help the copilot issue warnings before a machine overheats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad8d703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic manufacturing sensor data\n",
    "def generate_sensor_data(n_samples=1000):\n",
    "    \"\"\"\n",
    "    Generate synthetic time series sensor data\n",
    "    \"\"\"\n",
    "    time = np.arange(n_samples)\n",
    "    \n",
    "    # Base temperature with daily cycle\n",
    "    base_temp = 70 + 10 * np.sin(2 * np.pi * time / 100)\n",
    "    \n",
    "    # Add trend (equipment degradation)\n",
    "    trend = 0.01 * time\n",
    "    \n",
    "    # Add noise\n",
    "    noise = np.random.normal(0, 2, n_samples)\n",
    "    \n",
    "    # Occasional spikes (anomalies)\n",
    "    spikes = np.zeros(n_samples)\n",
    "    spike_indices = np.random.choice(n_samples, size=20, replace=False)\n",
    "    spikes[spike_indices] = np.random.uniform(5, 15, 20)\n",
    "    \n",
    "    # Final temperature\n",
    "    temperature = base_temp + trend + noise + spikes\n",
    "    \n",
    "    # Additional features\n",
    "    vibration = 0.5 * temperature + np.random.normal(0, 5, n_samples)\n",
    "    pressure = 100 + 0.3 * temperature + np.random.normal(0, 3, n_samples)\n",
    "    rpm = 1500 + 2 * temperature + np.random.normal(0, 50, n_samples)\n",
    "    \n",
    "    data = pd.DataFrame({\n",
    "        'time': time,\n",
    "        'temperature': temperature,\n",
    "        'vibration': vibration,\n",
    "        'pressure': pressure,\n",
    "        'rpm': rpm\n",
    "    })\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Generate data\n",
    "sensor_data = generate_sensor_data(n_samples=1000)\n",
    "\n",
    "print(\"üè≠ Manufacturing Sensor Data\")\n",
    "print(\"=\"*60)\n",
    "print(sensor_data.head(10))\n",
    "print(f\"\\nShape: {sensor_data.shape}\")\n",
    "print(f\"\\nStatistics:\")\n",
    "print(sensor_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c41d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sensor data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Temperature\n",
    "axes[0, 0].plot(sensor_data['time'], sensor_data['temperature'], linewidth=1.5, color='red')\n",
    "axes[0, 0].set_title('Temperature Over Time', fontweight='bold', fontsize=12)\n",
    "axes[0, 0].set_xlabel('Time Steps')\n",
    "axes[0, 0].set_ylabel('Temperature (¬∞F)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Vibration\n",
    "axes[0, 1].plot(sensor_data['time'], sensor_data['vibration'], linewidth=1.5, color='blue')\n",
    "axes[0, 1].set_title('Vibration Over Time', fontweight='bold', fontsize=12)\n",
    "axes[0, 1].set_xlabel('Time Steps')\n",
    "axes[0, 1].set_ylabel('Vibration (Hz)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Pressure\n",
    "axes[1, 0].plot(sensor_data['time'], sensor_data['pressure'], linewidth=1.5, color='green')\n",
    "axes[1, 0].set_title('Pressure Over Time', fontweight='bold', fontsize=12)\n",
    "axes[1, 0].set_xlabel('Time Steps')\n",
    "axes[1, 0].set_ylabel('Pressure (PSI)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# RPM\n",
    "axes[1, 1].plot(sensor_data['time'], sensor_data['rpm'], linewidth=1.5, color='purple')\n",
    "axes[1, 1].set_title('RPM Over Time', fontweight='bold', fontsize=12)\n",
    "axes[1, 1].set_xlabel('Time Steps')\n",
    "axes[1, 1].set_ylabel('RPM')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Manufacturing Equipment Sensor Readings', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f8faca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for LSTM\n",
    "def create_sequences(data, seq_length):\n",
    "    \"\"\"\n",
    "    Create sequences for LSTM training\n",
    "    \n",
    "    Args:\n",
    "        data: Input features (DataFrame or array)\n",
    "        seq_length: Length of input sequences\n",
    "    \n",
    "    Returns:\n",
    "        X: Input sequences\n",
    "        y: Target values\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(data[i+seq_length, 0])  # Predict temperature\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Normalize data\n",
    "features = ['temperature', 'vibration', 'pressure', 'rpm']\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(sensor_data[features])\n",
    "\n",
    "# Create sequences\n",
    "seq_length = 20  # Use 20 time steps to predict next value\n",
    "X, y = create_sequences(scaled_data, seq_length)\n",
    "\n",
    "print(\"üìä Sequence Data Preparation\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Sequence length: {seq_length}\")\n",
    "print(f\"Number of features: {len(features)}\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False  # Don't shuffle time series!\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.FloatTensor(y_test)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Data ready for LSTM training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a563c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train LSTM model\n",
    "model = LSTMModel(\n",
    "    input_size=4,  # 4 features\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    output_size=1,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "train_losses = []\n",
    "\n",
    "print(\"üîÑ Training LSTM Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs.squeeze(), batch_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.6f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ff4419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(range(1, epochs+1), train_losses, linewidth=2, color='blue', marker='o', markersize=4)\n",
    "plt.title('LSTM Training Loss', fontweight='bold', fontsize=14)\n",
    "plt.xlabel('Epoch', fontweight='bold')\n",
    "plt.ylabel('MSE Loss', fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336ad365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_device = X_test_tensor.to(device)\n",
    "    predictions = model(X_test_device).cpu().numpy().squeeze()\n",
    "    actuals = y_test\n",
    "\n",
    "# Calculate metrics\n",
    "mse = np.mean((predictions - actuals) ** 2)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = np.mean(np.abs(predictions - actuals))\n",
    "\n",
    "print(\"üìä Model Performance\")\n",
    "print(\"=\"*60)\n",
    "print(f\"MSE:  {mse:.6f}\")\n",
    "print(f\"RMSE: {rmse:.6f}\")\n",
    "print(f\"MAE:  {mae:.6f}\")\n",
    "\n",
    "# Plot predictions vs actuals\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(range(len(actuals)), actuals, label='Actual Temperature', linewidth=2, alpha=0.7)\n",
    "plt.plot(range(len(predictions)), predictions, label='Predicted Temperature', linewidth=2, alpha=0.7)\n",
    "plt.title('LSTM Temperature Prediction', fontweight='bold', fontsize=14)\n",
    "plt.xlabel('Time Steps', fontweight='bold')\n",
    "plt.ylabel('Normalized Temperature', fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Zoom in on a portion\n",
    "zoom_start, zoom_end = 0, 100\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(range(zoom_start, zoom_end), actuals[zoom_start:zoom_end], \n",
    "         label='Actual', linewidth=2, marker='o', markersize=4, alpha=0.7)\n",
    "plt.plot(range(zoom_start, zoom_end), predictions[zoom_start:zoom_end], \n",
    "         label='Predicted', linewidth=2, marker='s', markersize=4, alpha=0.7)\n",
    "plt.title('LSTM Predictions (Zoomed View)', fontweight='bold', fontsize=14)\n",
    "plt.xlabel('Time Steps', fontweight='bold')\n",
    "plt.ylabel('Normalized Temperature', fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ LSTM successfully predicts equipment temperature!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44932dba",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ GRU: Simplified LSTM\n",
    "\n",
    "### GRU Architecture\n",
    "\n",
    "GRU simplifies LSTM with fewer gates:\n",
    "- **Reset Gate**: How much past to forget\n",
    "- **Update Gate**: How much past to keep\n",
    "\n",
    "**Advantages**:\n",
    "- Fewer parameters (faster training)\n",
    "- Often similar performance to LSTM\n",
    "- Easier to tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d33a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU Model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "        super(GRUModel, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # GRU layer\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, hn = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Compare LSTM vs GRU\n",
    "lstm_model = LSTMModel(input_size=4, hidden_size=64, num_layers=2, output_size=1)\n",
    "gru_model = GRUModel(input_size=4, hidden_size=64, num_layers=2, output_size=1)\n",
    "\n",
    "lstm_params = sum(p.numel() for p in lstm_model.parameters())\n",
    "gru_params = sum(p.numel() for p in gru_model.parameters())\n",
    "\n",
    "print(\"‚öñÔ∏è LSTM vs GRU Comparison\")\n",
    "print(\"=\"*60)\n",
    "print(f\"LSTM parameters: {lstm_params:,}\")\n",
    "print(f\"GRU parameters:  {gru_params:,}\")\n",
    "print(f\"\\nGRU has {lstm_params - gru_params:,} fewer parameters ({100*(1-gru_params/lstm_params):.1f}% reduction)\")\n",
    "print(\"\\n‚úÖ GRU is more efficient!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab540cb",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Bidirectional RNNs\n",
    "\n",
    "### Why Bidirectional?\n",
    "\n",
    "Sometimes **future context** helps understand the past!\n",
    "\n",
    "Example: \"The animal didn't cross the street because it was too **tired**\"\n",
    "- Need to see \"tired\" to understand \"it\" refers to the animal\n",
    "\n",
    "Bidirectional RNNs process sequences in **both directions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fe9f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional LSTM\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True  # Key difference!\n",
    "        )\n",
    "        \n",
    "        # FC layer (hidden_size * 2 because bidirectional)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # num_directions = 2 for bidirectional\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Create bidirectional model\n",
    "bi_lstm = BiLSTM(input_size=4, hidden_size=64, num_layers=2, output_size=1)\n",
    "\n",
    "print(\"‚ÜîÔ∏è Bidirectional LSTM\")\n",
    "print(\"=\"*60)\n",
    "print(bi_lstm)\n",
    "print(f\"\\nParameters: {sum(p.numel() for p in bi_lstm.parameters()):,}\")\n",
    "print(\"\\n‚úÖ BiLSTM uses both past AND future context!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb3e085",
   "metadata": {},
   "source": [
    "## üéâ Summary\n",
    "\n",
    "Congratulations! You've mastered RNNs and LSTMs!\n",
    "\n",
    "### Key Concepts\n",
    "- ‚úÖ RNN architecture and hidden states\n",
    "- ‚úÖ Vanishing gradient problem\n",
    "- ‚úÖ LSTM gates (forget, input, output)\n",
    "- ‚úÖ GRU (simplified LSTM)\n",
    "- ‚úÖ Bidirectional RNNs\n",
    "- ‚úÖ Sequence prediction\n",
    "\n",
    "### What You Built\n",
    "1. üîÑ Simple RNN from scratch\n",
    "2. üß† LSTM temperature predictor\n",
    "3. ‚ö° GRU model\n",
    "4. ‚ÜîÔ∏è Bidirectional LSTM\n",
    "\n",
    "### RNN Applications\n",
    "- üè≠ **Manufacturing**: Equipment monitoring, predictive maintenance\n",
    "- üìà **Finance**: Stock prediction, fraud detection\n",
    "- üéµ **Audio**: Speech recognition, music generation\n",
    "- üìù **Text**: Language modeling, translation\n",
    "\n",
    "### Comparison Table\n",
    "\n",
    "| Model | Parameters | Speed | Long-term Memory | Use Case |\n",
    "|-------|-----------|-------|------------------|----------|\n",
    "| Simple RNN | Low | Fast | Poor | Short sequences |\n",
    "| LSTM | High | Slow | Excellent | Long sequences |\n",
    "| GRU | Medium | Medium | Very Good | Balanced |\n",
    "| BiLSTM | Highest | Slowest | Best | Full context needed |\n",
    "\n",
    "### Next Steps\n",
    "Continue to **Notebook 04: Transformers** to learn the architecture that revolutionized NLP!\n",
    "\n",
    "<div align=\"center\">\n",
    "<b>RNNs & LSTMs mastered! Ready for Transformers! üöÄ</b>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
