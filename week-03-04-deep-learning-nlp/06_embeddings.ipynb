{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d61f7a39",
   "metadata": {},
   "source": [
    "# \udde0 Notebook 06: Word Embeddings - The Building Blocks of NLP\n",
    "\n",
    "**Week 3-4: Deep Learning & NLP Foundations**  \n",
    "**Gen AI Masters Program**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objectives\n",
    "\n",
    "Welcome to the world of word embeddings! Before a neural network can understand text, we need to convert words into numbers. But how we do this is critically important. This notebook explores the evolution of representing text, from simple but flawed methods to the powerful vector representations that underpin modern NLP.\n",
    "\n",
    "By the end of this session, you will understand:\n",
    "\n",
    "1.  **The \"Why\":** The limitations of basic text representations like one-hot encoding.\n",
    "2.  **The \"How\":** The theory behind distributional semantics‚Äîthe idea that \"a word is characterized by the company it keeps.\"\n",
    "3.  **The Classics:** How to build foundational embedding models like **Word2Vec** from scratch.\n",
    "4.  **The Powerhouses:** How to leverage pre-trained embeddings like **GloVe** and contextual embeddings from **Transformers (BERT)**.\n",
    "5.  **The Application:** How to visualize and interpret these embeddings to reveal semantic relationships in text data.\n",
    "\n",
    "**Estimated Time:** 3 hours\n",
    "\n",
    "---\n",
    "\n",
    "## \udd14 Why are Embeddings So Important?\n",
    "\n",
    "At their core, **embeddings are numerical representations of words (or sentences) in a low-dimensional vector space.** A good embedding captures the semantic meaning of a word, placing it in the vector space such that similar words are close to each other.\n",
    "\n",
    "For example, the vectors for \"cat\" and \"kitten\" should be very close, while the vectors for \"cat\" and \"car\" should be far apart. This allows neural networks to learn patterns and relationships based on meaning, not just arbitrary token IDs. They are the fundamental input layer for nearly all deep learning models that work with text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb294b64",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üìú Agenda\n",
    "\n",
    "1.  **Part 1: The Problem with Numbers - One-Hot Encoding**\n",
    "    *   Representing words as sparse, high-dimensional vectors.\n",
    "    *   Demonstrating their key limitation: no notion of similarity.\n",
    "2.  **Part 2: A Better Way - Distributional Semantics & Co-occurrence Matrices**\n",
    "    *   Building a co-occurrence matrix from a corpus.\n",
    "    *   Using dimensionality reduction (PCA) to create dense embeddings.\n",
    "3.  **Part 3: Learning Embeddings - Word2Vec (Skip-Gram) from Scratch**\n",
    "    *   Implementing the Skip-Gram with Negative Sampling algorithm in PyTorch.\n",
    "    *   Training our own embeddings on a custom corpus.\n",
    "    *   Visualizing the learned vector space with t-SNE.\n",
    "4.  **Part 4: Using Pre-trained Embeddings - GloVe**\n",
    "    *   Loading and using powerful, pre-trained word vectors from `torchtext`.\n",
    "5.  **Part 5: The State of the Art - Contextual Embeddings with Transformers**\n",
    "    *   Understanding the difference between static (Word2Vec, GloVe) and contextual embeddings (BERT).\n",
    "    *   Using a pre-trained Transformer to generate sentence embeddings that understand context.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748bb749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Basic Setup and Imports ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import math\n",
    "from collections import Counter\n",
    "import re # For tokenization\n",
    "\n",
    "# --- Plotting and Device Configuration ---\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set the device (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'‚úÖ Using device: {device}')\n",
    "print(f'PyTorch version: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a0abf3",
   "metadata": {},
   "source": [
    "## üß† Part 1: One-Hot Encoding - A Flawed Start\n",
    "\n",
    "The most basic way to convert a word into a vector is **one-hot encoding**.\n",
    "\n",
    "Imagine we have a vocabulary of size `V`. For any given word, we create a vector of length `V` that is all zeros, except for a single `1` at the index corresponding to that word.\n",
    "\n",
    "**Example Vocabulary:** `['temperature', 'vibration', 'pressure']` (Size V=3)\n",
    "*   `temperature` -> `[1, 0, 0]`\n",
    "*   `vibration`   -> `[0, 1, 0]`\n",
    "*   `pressure`    -> `[0, 0, 1]`\n",
    "\n",
    "**Key Characteristics:**\n",
    "*   **Sparse:** The vectors are mostly zeros.\n",
    "*   **High-Dimensional:** The vector length equals the vocabulary size, which can be huge (e.g., >50,000).\n",
    "*   **Orthogonal:** The dot product of any two different one-hot vectors is always 0. This is their biggest flaw‚Äîit implies that there is **no shared similarity** between any two words. The model has no way of knowing that \"vibration\" is more similar to \"pressure\" than it is to \"apple\".\n",
    "\n",
    "Let's see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97aebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a small, relevant vocabulary\n",
    "vocabulary = ['temperature', 'vibration', 'pressure', 'coolant', 'bearing', 'leakage']\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "# Create a mapping from word to its index\n",
    "word_to_idx = {word: i for i, word in enumerate(vocabulary)}\n",
    "print(f\"Vocabulary mapping: {word_to_idx}\\n\")\n",
    "\n",
    "def one_hot_encode(word, word_to_idx):\n",
    "    \"\"\"Creates a one-hot vector for a given word.\"\"\"\n",
    "    # Create a vector of zeros with length equal to the vocabulary size\n",
    "    one_hot_vector = np.zeros(len(word_to_idx))\n",
    "    # Get the index of the word\n",
    "    idx = word_to_idx[word]\n",
    "    # Set the element at that index to 1\n",
    "    one_hot_vector[idx] = 1\n",
    "    return one_hot_vector\n",
    "\n",
    "# --- Demonstrate the flaw ---\n",
    "# Encode two words\n",
    "hot_temp = one_hot_encode('temperature', word_to_idx)\n",
    "hot_vibration = one_hot_encode('vibration', word_to_idx)\n",
    "\n",
    "print(f\"One-hot vector for 'temperature': {hot_temp}\")\n",
    "print(f\"One-hot vector for 'vibration':   {hot_vibration}\")\n",
    "\n",
    "# Calculate the dot product. For one-hot vectors, this is a proxy for similarity.\n",
    "# A dot product of 0 means the vectors are orthogonal (completely unrelated).\n",
    "similarity_score = np.dot(hot_temp, hot_vibration)\n",
    "print(f\"\\nDot product (similarity) between 'temperature' and 'vibration': {similarity_score}\")\n",
    "print(\"‚ùå This is meaningless! The model can't learn that these concepts might be related.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcf0257",
   "metadata": {},
   "source": [
    "## üß† Part 2: Distributional Semantics & Co-occurrence Matrices\n",
    "\n",
    "To overcome the limitations of one-hot encoding, we turn to the **Distributional Hypothesis**: \"a word is characterized by the company it keeps.\"\n",
    "\n",
    "This means we can infer a word's meaning by looking at the words that frequently appear near it. A simple way to capture this is with a **co-occurrence matrix**.\n",
    "\n",
    "**How it works:**\n",
    "1.  **Define a Context Window:** Choose a window size (e.g., 2 words to the left and 2 to the right).\n",
    "2.  **Slide and Count:** Slide this window across your entire text corpus. For each word in the center of the window, count which other words appear inside its window.\n",
    "3.  **Build the Matrix:** Create a square matrix of size `V x V` (where V is your vocabulary size). The entry `(word1, word2)` in the matrix stores the number of times `word2` appeared in the context window of `word1`.\n",
    "\n",
    "The rows of this matrix can be considered our first attempt at dense embeddings! They are no longer orthogonal; words that appear in similar contexts will have similar-looking vectors.\n",
    "\n",
    "Let's build one from a small corpus of synthetic maintenance reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f72013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A small corpus of text data\n",
    "corpus = [\n",
    "    'High temperature spike detected in the main furnace chamber.',\n",
    "    'Vibration analysis shows an increase near the primary bearing housing.',\n",
    "    'A sudden pressure drop indicates a coolant circulation issue.',\n",
    "    'Coolant leakage was observed directly below the hydraulic press.',\n",
    "    'The main bearing is overheating, which threatens a production halt.',\n",
    "    'A sensor outage is causing a data gap in the plant historian.',\n",
    "    'The hydraulic pump is showing signs of cavitation, producing audible noise.',\n",
    "    'The lubrication schedule was missed for the gearbox assembly.',\n",
    "    'Conveyor belt torque variation is impacting the line speed.',\n",
    "    'An unexpected voltage surge has tripped the main safety relay.'\n",
    "]\n",
    "\n",
    "def tokenize_corpus(corpus):\n",
    "    \"\"\"A simple tokenizer that converts to lowercase and removes punctuation.\"\"\"\n",
    "    tokenized_docs = []\n",
    "    for doc in corpus:\n",
    "        # Remove punctuation and split by space\n",
    "        tokens = re.sub(r'[^\\w\\s]', '', doc.lower()).split()\n",
    "        tokenized_docs.append(tokens)\n",
    "    return tokenized_docs\n",
    "\n",
    "# --- Build the Vocabulary and Co-occurrence Matrix ---\n",
    "tokenized_corpus = tokenize_corpus(corpus)\n",
    "# Create a vocabulary of all unique words\n",
    "vocab = sorted(list(set(word for doc in tokenized_corpus for word in doc)))\n",
    "vocab_size = len(vocab)\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "# Initialize a V x V matrix of zeros\n",
    "co_occurrence_matrix = np.zeros((vocab_size, vocab_size), dtype=np.float32)\n",
    "\n",
    "# Define the context window size\n",
    "window_size = 2\n",
    "\n",
    "# Populate the matrix\n",
    "for doc in tokenized_corpus:\n",
    "    for i, target_word in enumerate(doc):\n",
    "        target_idx = word_to_idx[target_word]\n",
    "        # Define the start and end of the context window\n",
    "        start = max(i - window_size, 0)\n",
    "        end = min(i + window_size + 1, len(doc))\n",
    "        \n",
    "        # Iterate through the context window\n",
    "        for j in range(start, end):\n",
    "            if i == j:\n",
    "                continue # Don't count the target word itself\n",
    "            context_word = doc[j]\n",
    "            context_idx = word_to_idx[context_word]\n",
    "            # Increment the count for the co-occurring pair\n",
    "            co_occurrence_matrix[target_idx, context_idx] += 1\n",
    "\n",
    "# --- Visualize the Matrix ---\n",
    "# For better visualization, let's wrap it in a pandas DataFrame\n",
    "co_occurrence_df = pd.DataFrame(co_occurrence_matrix, index=vocab, columns=vocab)\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(co_occurrence_df, cmap='viridis', annot=False)\n",
    "plt.title('Word Co-occurrence Matrix (Window Size = 2)', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Context Words')\n",
    "plt.ylabel('Target Words')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Example: Find co-occurrences for the word 'bearing'\n",
    "print(\"Co-occurrence vector for 'bearing':\")\n",
    "print(co_occurrence_df.loc['bearing'][co_occurrence_df.loc['bearing'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e761a9b3",
   "metadata": {},
   "source": [
    "### From Co-occurrence to Embeddings: Dimensionality Reduction with PCA\n",
    "\n",
    "The rows of our co-occurrence matrix are dense vectors, but they are still very high-dimensional (equal to the vocabulary size). This makes them computationally expensive and prone to noise.\n",
    "\n",
    "A common technique to make these vectors more manageable and robust is to reduce their dimensionality using methods like **Principal Component Analysis (PCA)**. PCA finds the directions of maximum variance in the data and projects the data onto a new, lower-dimensional subspace.\n",
    "\n",
    "These lower-dimensional vectors are our first real \"embeddings\"! Let's project our `V x V` matrix down to `V x 2` so we can visualize the word relationships in a 2D plot. Words that appeared in similar contexts should now cluster together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bea33f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PCA to reduce dimensions to 2\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "\n",
    "# Fit PCA on the co-occurrence matrix and transform it\n",
    "# The rows of the matrix are our high-dimensional vectors\n",
    "embeddings_2d = pca.fit_transform(co_occurrence_matrix)\n",
    "\n",
    "# --- Visualize the 2D Embeddings ---\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], color='skyblue', s=50)\n",
    "\n",
    "# Annotate each point with its corresponding word\n",
    "for i, word in enumerate(vocab):\n",
    "    plt.text(\n",
    "        embeddings_2d[i, 0] + 0.03, # Add a small offset for readability\n",
    "        embeddings_2d[i, 1] + 0.03,\n",
    "        word,\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "plt.title('2D Word Embeddings via PCA on Co-occurrence Matrix', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.axhline(0, color='grey', linewidth=0.5)\n",
    "plt.axvline(0, color='grey', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Interpretation ---\n",
    "# Look for clusters! For example, 'bearing' and 'housing' might be close,\n",
    "# as might 'coolant', 'leakage', and 'pressure'. This shows that even\n",
    "# this simple method can start to capture semantic relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5791e87",
   "metadata": {},
   "source": [
    "## üß† Part 3: Learning Embeddings - Word2Vec (Skip-Gram) from Scratch\n",
    "\n",
    "While co-occurrence matrices are intuitive, they have drawbacks:\n",
    "*   They can become very large and sparse for big vocabularies.\n",
    "*   The resulting embeddings are not always optimal for downstream tasks.\n",
    "\n",
    "A more powerful approach is to **learn** the embeddings directly by training a neural network. **Word2Vec** is a family of models that does exactly this. We will implement the **Skip-Gram** variant.\n",
    "\n",
    "### The Skip-Gram Model\n",
    "\n",
    "The core idea of Skip-Gram is the inverse of our co-occurrence counting: instead of counting which words appear next to a target word, we train a model to **predict the context words given a target word**.\n",
    "\n",
    "**The Process:**\n",
    "1.  **Generate Training Data:** We slide a window over our corpus and create `(target, context)` word pairs. For a window size of 2, the sentence \"pressure drop indicates a coolant issue\" would yield pairs like `(indicates, pressure)`, `(indicates, drop)`, `(indicates, a)`, `(indicates, coolant)`.\n",
    "2.  **Define the Model:** We create a simple neural network with one hidden layer. The weights of this hidden layer will become our word embeddings!\n",
    "    *   The input is a one-hot encoded vector of the target word.\n",
    "    *   The output is a probability distribution over the entire vocabulary, representing the likelihood of each word being a context word.\n",
    "3.  **The \"Negative Sampling\" Trick:** Training a model to predict over the entire vocabulary (which could be 50,000+ words) is extremely slow. **Negative Sampling** provides a clever and efficient approximation. Instead of updating weights for all words, for each `(target, context)` pair (a \"positive\" sample), we randomly select a few \"negative\" samples‚Äîwords that are *not* in the target's context. The model's task then becomes much simpler: predict `1` for the true context word and `0` for the random negative words.\n",
    "\n",
    "Let's build it step-by-step. First, we'll create a PyTorch `Dataset` to generate our `(target, context)` pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1761f91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset to generate (target, context) pairs for Skip-Gram training.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenized_docs, word_to_idx, window_size=2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tokenized_docs (list of list of str): The corpus, already tokenized.\n",
    "            word_to_idx (dict): Mapping from words to their integer indices.\n",
    "            window_size (int): The size of the context window (words to the left and right).\n",
    "        \"\"\"\n",
    "        self.pairs = []\n",
    "        # Iterate through each document in the corpus\n",
    "        for doc in tokenized_docs:\n",
    "            # Convert the document's words to their corresponding indices\n",
    "            indexed_doc = [word_to_idx[word] for word in doc if word in word_to_idx]\n",
    "            \n",
    "            # Iterate through each word in the indexed document to treat it as a target\n",
    "            for i, target_word_idx in enumerate(indexed_doc):\n",
    "                # Define the start and end of the context window\n",
    "                start = max(i - window_size, 0)\n",
    "                end = min(i + window_size + 1, len(indexed_doc))\n",
    "                \n",
    "                # Iterate through the context window\n",
    "                for j in range(start, end):\n",
    "                    if i == j:\n",
    "                        continue # The target word is not its own context\n",
    "                    context_word_idx = indexed_doc[j]\n",
    "                    # Add the (target, context) pair to our list\n",
    "                    self.pairs.append((target_word_idx, context_word_idx))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of (target, context) pairs.\"\"\"\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves a single (target, context) pair by its index.\n",
    "        The DataLoader will use this to create batches.\n",
    "        \"\"\"\n",
    "        target, context = self.pairs[idx]\n",
    "        return torch.tensor(target, dtype=torch.long), torch.tensor(context, dtype=torch.long)\n",
    "\n",
    "# --- Create the Dataset and DataLoader ---\n",
    "# We'll use the same tokenized corpus and vocabulary from Part 2\n",
    "skipgram_dataset = SkipGramDataset(tokenized_corpus, word_to_idx, window_size=2)\n",
    "# The DataLoader will handle batching and shuffling for us during training\n",
    "skipgram_loader = DataLoader(skipgram_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "print(f\"‚úÖ Created Skip-Gram dataset with {len(skipgram_dataset)} (target, context) pairs.\")\n",
    "\n",
    "# Let's inspect a few pairs\n",
    "print(\"\\n--- Sample Pairs ---\")\n",
    "for i in range(5):\n",
    "    target_idx, context_idx = skipgram_dataset[i]\n",
    "    target_word = list(word_to_idx.keys())[target_idx]\n",
    "    context_word = list(word_to_idx.keys())[context_idx]\n",
    "    print(f\"Pair {i+1}: (target='{target_word}', context='{context_word}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca32ad5",
   "metadata": {},
   "source": [
    "### The Skip-Gram Model Architecture\n",
    "\n",
    "Our model is surprisingly simple. It consists of two embedding layers:\n",
    "\n",
    "1.  **Target Embeddings:** This is an embedding matrix where each row corresponds to the vector representation of a word when it's the *target* word. This is the matrix we'll ultimately use as our word embeddings.\n",
    "2.  **Context Embeddings:** A second embedding matrix where each row corresponds to a word's vector when it's a *context* word.\n",
    "\n",
    "**Forward Pass Logic (with Negative Sampling):**\n",
    "\n",
    "For a given `(target, positive_context)` pair:\n",
    "1.  Look up the embedding for the `target` word from the `target_embeddings` matrix.\n",
    "2.  Look up the embedding for the `positive_context` word from the `context_embeddings` matrix.\n",
    "3.  Compute the dot product of these two vectors. A high dot product means they are similar. We want to maximize this, so we pass it through a sigmoid function and aim for a target of `1`.\n",
    "4.  Randomly sample a few `negative_context` words from the vocabulary.\n",
    "5.  Look up their embeddings from the `context_embeddings` matrix.\n",
    "6.  Compute the dot product between the `target` embedding and each of the `negative_context` embeddings. We want to minimize these, so we pass them through a sigmoid and aim for a target of `0`.\n",
    "7.  The total loss is the sum of the losses from the positive and negative samples.\n",
    "\n",
    "This process efficiently trains the model to push the vectors of true context pairs closer together while pushing the vectors of random pairs apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99250b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch implementation of the Skip-Gram model with Negative Sampling.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size (int): The total number of unique words in the vocabulary.\n",
    "            embed_dim (int): The desired dimensionality of the word embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # The embedding layer for target words (the one we'll keep)\n",
    "        self.target_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        # The embedding layer for context words\n",
    "        self.context_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, target, context):\n",
    "        \"\"\"\n",
    "        This forward pass is designed for calculating the loss.\n",
    "        It computes the dot product between target and context embeddings.\n",
    "        \"\"\"\n",
    "        # Get the embedding vectors for the batch of target and context words\n",
    "        target_embeds = self.target_embeddings(target)   # Shape: (batch_size, embed_dim)\n",
    "        context_embeds = self.context_embeddings(context) # Shape: (batch_size, embed_dim)\n",
    "        \n",
    "        # Compute the dot product between target and context vectors\n",
    "        # We use element-wise multiplication and sum along the dimension of the embeddings\n",
    "        scores = torch.sum(target_embeds * context_embeds, dim=1)\n",
    "        return scores\n",
    "\n",
    "def train_skipgram(model, loader, optimizer, epochs=100, num_negative_samples=5):\n",
    "    \"\"\"\n",
    "    The main training loop for the Skip-Gram model using negative sampling.\n",
    "    \"\"\"\n",
    "    # For negative sampling, we need a \"noise distribution\" to sample from.\n",
    "    # Words that appear more frequently should be sampled more often.\n",
    "    # The formula uses a power of 0.75 to smooth the distribution slightly.\n",
    "    word_counts = Counter(word for doc in tokenized_corpus for word in doc)\n",
    "    freqs = [word_counts[word] for word in vocab]\n",
    "    freqs_tensor = torch.tensor(freqs, dtype=torch.float32)\n",
    "    noise_dist = freqs_tensor.pow(0.75)\n",
    "    noise_dist /= torch.sum(noise_dist)\n",
    "\n",
    "    loss_history = []\n",
    "    model.train() # Set the model to training mode\n",
    "    \n",
    "    print(\"üîÑ Starting Word2Vec (Skip-Gram) training...\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        total_loss = 0\n",
    "        for target_words, context_words in loader:\n",
    "            # Move data to the selected device\n",
    "            target_words = target_words.to(device)\n",
    "            context_words = context_words.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # --- 1. Positive Loss ---\n",
    "            # Get the scores for the true (positive) context pairs\n",
    "            positive_scores = model(target_words, context_words)\n",
    "            # We want the model to predict 1 for these, so we use log-sigmoid\n",
    "            positive_loss = F.logsigmoid(positive_scores).mean()\n",
    "\n",
    "            # --- 2. Negative Loss ---\n",
    "            # Sample random negative words from the noise distribution\n",
    "            batch_size = target_words.shape[0]\n",
    "            negative_words = torch.multinomial(\n",
    "                noise_dist, batch_size * num_negative_samples, replacement=True\n",
    "            ).to(device)\n",
    "            \n",
    "            # Get the scores for the negative samples\n",
    "            # We need to reshape the target words to match the number of negative samples\n",
    "            target_for_neg = target_words.repeat_interleave(num_negative_samples)\n",
    "            negative_scores = model(target_for_neg, negative_words)\n",
    "            # We want the model to predict 0 for these, so we use log-sigmoid on the negative scores\n",
    "            negative_loss = F.logsigmoid(-negative_scores).mean()\n",
    "\n",
    "            # --- 3. Total Loss ---\n",
    "            # The goal is to maximize positive scores and minimize negative scores.\n",
    "            # Maximizing log(sigmoid(x)) + log(sigmoid(-y)) is equivalent to minimizing -(...).\n",
    "            loss = -(positive_loss + negative_loss)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(loader)\n",
    "        loss_history.append(avg_loss)\n",
    "        if epoch % 20 == 0 or epoch == 1:\n",
    "            print(f\"Epoch {epoch:03d}/{epochs} | Average Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "    print(\"‚úÖ Training complete!\")\n",
    "    return loss_history\n",
    "\n",
    "# --- Hyperparameters and Training ---\n",
    "EMBED_DIM = 50 # The dimensionality of our learned embeddings\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 200\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "skipgram_model = SkipGramModel(vocab_size, EMBED_DIM).to(device)\n",
    "optimizer = optim.Adam(skipgram_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Run the training\n",
    "loss_history = train_skipgram(skipgram_model, skipgram_loader, optimizer, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5588ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training loss over epochs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(loss_history, color='deepskyblue', linewidth=2)\n",
    "plt.title('Word2Vec (Skip-Gram) Training Loss', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Negative Sampling Loss')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9a5775",
   "metadata": {},
   "source": [
    "### Visualizing the Learned Embeddings with t-SNE\n",
    "\n",
    "Now for the exciting part! After training, the `target_embeddings` layer in our model contains the learned word vectors. Just like we did with PCA, we can use a dimensionality reduction technique to visualize them.\n",
    "\n",
    "**t-SNE (t-Distributed Stochastic Neighbor Embedding)** is a more sophisticated technique than PCA, particularly well-suited for visualizing high-dimensional datasets in low dimensions (like 2D or 3D). It works by preserving local similarities, so points that are close in the high-dimensional space are mapped to points that are close in the low-dimensional space.\n",
    "\n",
    "Let's extract the weights from our model and plot them. We should see meaningful clusters emerge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e675d73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the learned embedding vectors from the model\n",
    "# We use .detach() to remove them from the computation graph\n",
    "learned_embeddings = skipgram_model.target_embeddings.weight.detach().cpu().numpy()\n",
    "\n",
    "# Use t-SNE to reduce the 50-dimensional embeddings to 2 dimensions\n",
    "# Perplexity is a key hyperparameter; it's roughly the number of close neighbors each point has.\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=10)\n",
    "embeddings_2d_tsne = tsne.fit_transform(learned_embeddings)\n",
    "\n",
    "# --- Visualize the 2D t-SNE Embeddings ---\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(embeddings_2d_tsne[:, 0], embeddings_2d_tsne[:, 1], color='coral', s=50)\n",
    "\n",
    "# Annotate each point with its corresponding word\n",
    "for i, word in enumerate(vocab):\n",
    "    plt.text(\n",
    "        embeddings_2d_tsne[i, 0] + 0.05,\n",
    "        embeddings_2d_tsne[i, 1] + 0.05,\n",
    "        word,\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "plt.title('2D t-SNE Visualization of Learned Word2Vec Embeddings', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Interpretation ---\n",
    "# Compare this plot to the PCA plot. The clusters should be more distinct.\n",
    "# 'hydraulic', 'pump', 'press' might form a group.\n",
    "# 'bearing', 'housing', 'overheating' might form another.\n",
    "# This demonstrates that the neural network has learned meaningful semantic relationships from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0adc148",
   "metadata": {},
   "source": [
    "### Querying for Semantic Similarity\n",
    "\n",
    "The ultimate test of our embeddings is to see if they can find words with similar meanings. We can do this by calculating the **cosine similarity** between the vector of a query word and all other vectors in the vocabulary.\n",
    "\n",
    "**Cosine Similarity** measures the cosine of the angle between two vectors.\n",
    "*   A value of `1` means the vectors point in the exact same direction (maximum similarity).\n",
    "*   A value of `0` means they are orthogonal.\n",
    "*   A value of `-1` means they point in opposite directions.\n",
    "\n",
    "It's the standard way to measure similarity between embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9b8809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar(query_word, embeddings, word_to_idx, top_k=5):\n",
    "    \"\"\"\n",
    "    Finds the most similar words to a query word based on cosine similarity.\n",
    "    \"\"\"\n",
    "    if query_word not in word_to_idx:\n",
    "        print(f\"Error: '{query_word}' not in vocabulary.\")\n",
    "        return\n",
    "\n",
    "    # Get the embedding vector for the query word\n",
    "    query_vector = embeddings[word_to_idx[query_word]]\n",
    "    \n",
    "    # Calculate cosine similarity between the query vector and all other vectors\n",
    "    # Cosine Similarity = (A . B) / (||A|| * ||B||)\n",
    "    dot_products = np.dot(embeddings, query_vector)\n",
    "    norms = np.linalg.norm(embeddings, axis=1) * np.linalg.norm(query_vector)\n",
    "    similarities = dot_products / norms\n",
    "    \n",
    "    # Get the indices of the top_k most similar words (excluding the query word itself)\n",
    "    # We use argsort to get the indices that would sort the array in ascending order,\n",
    "    # then we take the last few indices for descending order.\n",
    "    top_indices = np.argsort(similarities)[-top_k-1:-1][::-1]\n",
    "    \n",
    "    print(f\"üîé Words most similar to '{query_word}':\")\n",
    "    for i in top_indices:\n",
    "        word = list(word_to_idx.keys())[i]\n",
    "        similarity = similarities[i]\n",
    "        print(f\"  - {word:15s} (Similarity: {similarity:.3f})\")\n",
    "\n",
    "# --- Perform Similarity Queries ---\n",
    "find_most_similar('vibration', learned_embeddings, word_to_idx)\n",
    "print(\"-\" * 30)\n",
    "find_most_similar('coolant', learned_embeddings, word_to_idx)\n",
    "print(\"-\" * 30)\n",
    "find_most_similar('hydraulic', learned_embeddings, word_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8751ba",
   "metadata": {},
   "source": [
    "## üß† Part 4: Using Pre-trained Embeddings (GloVe)\n",
    "\n",
    "Training your own embeddings is great for domain-specific tasks, but it requires a large corpus of text. For general-purpose language understanding, it's often better to start with **pre-trained embeddings** that have been trained on massive datasets (like the entire web).\n",
    "\n",
    "**GloVe (Global Vectors for Word Representation)** is another popular model for learning embeddings. While Word2Vec is a \"predictive\" model, GloVe is a \"count-based\" model that is trained directly on the statistics of a global co-occurrence matrix.\n",
    "\n",
    "The `torchtext` library makes it easy to download and use pre-trained GloVe vectors. We can load them and see how they represent our vocabulary, even without training on our specific corpus.\n",
    "\n",
    "**Note:** This requires an internet connection to download the GloVe vectors the first time you run it. The files are several hundred megabytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c5028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is optional and requires an internet connection.\n",
    "# If you have issues, you can skip to the next section.\n",
    "try:\n",
    "    from torchtext.vocab import GloVe\n",
    "    \n",
    "    # Load the pre-trained GloVe embeddings.\n",
    "    # '6B' means it was trained on 6 billion tokens.\n",
    "    # 'dim=100' means we'll get 100-dimensional vectors.\n",
    "    glove = GloVe(name='6B', dim=100)\n",
    "    print(\"‚úÖ Successfully loaded pre-trained GloVe (6B, 100d) vectors.\")\n",
    "\n",
    "    # Let's get the GloVe vector for a word\n",
    "    word = 'pressure'\n",
    "    glove_vector = glove[word]\n",
    "    print(f\"\\nGloVe vector for '{word}':\")\n",
    "    print(glove_vector)\n",
    "    print(f\"Vector dimension: {glove_vector.shape[0]}\")\n",
    "\n",
    "    # Find words similar to 'pressure' using GloVe\n",
    "    def find_glove_similar(query_word, glove_model, top_k=5):\n",
    "        if query_word not in glove_model.stoi:\n",
    "            print(f\"'{query_word}' not in GloVe vocabulary.\")\n",
    "            return\n",
    "        \n",
    "        query_vector = glove_model[query_word]\n",
    "        \n",
    "        # Calculate cosine similarity against the entire GloVe vocabulary\n",
    "        # This can be slow, so we'll just demonstrate the principle\n",
    "        similarities = {}\n",
    "        for word, idx in glove_model.stoi.items():\n",
    "            vec = glove_model.vectors[idx]\n",
    "            # Using torch.cosine_similarity for efficiency\n",
    "            sim = F.cosine_similarity(query_vector.unsqueeze(0), vec.unsqueeze(0)).item()\n",
    "            similarities[word] = sim\n",
    "            \n",
    "        # Sort and get top_k\n",
    "        sorted_sims = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\nüîé Words most similar to '{query_word}' in GloVe:\")\n",
    "        for word, sim in sorted_sims[1:top_k+1]:\n",
    "            print(f\"  - {word:15s} (Similarity: {sim:.3f})\")\n",
    "\n",
    "    find_glove_similar('pressure', glove)\n",
    "\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è torchtext not found. Skipping GloVe section.\")\n",
    "    print(\"Install it with: pip install torchtext\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not download GloVe vectors. Skipping section. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9c7fd7",
   "metadata": {},
   "source": [
    "## üß† Part 5: Contextual Embeddings with Transformers (BERT)\n",
    "\n",
    "Word2Vec and GloVe produce **static embeddings**. This means that a word like \"pressure\" has the *exact same* vector regardless of its context.\n",
    "*   \"The tire **pressure** is low.\"\n",
    "*   \"He felt the **pressure** of the deadline.\"\n",
    "\n",
    "Clearly, the meaning is different in each sentence. This is a major limitation that static embeddings cannot overcome.\n",
    "\n",
    "**Contextual embeddings**, generated by models like **BERT (Bidirectional Encoder Representations from Transformers)**, solve this problem. BERT processes the entire sentence at once, and the embedding it generates for a word is a function of all the other words in that sentence.\n",
    "\n",
    "This allows the model to create dynamic, context-aware word vectors. We'll use the popular `transformers` library from HuggingFace to easily load a pre-trained BERT model and see this in action.\n",
    "\n",
    "**Note:** This section requires the `transformers` library (`pip install transformers`) and an internet connection to download the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addc6328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is optional and requires the transformers library and an internet connection.\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "    # Load a pre-trained tokenizer and model.\n",
    "    # 'distilbert-base-uncased' is a smaller, faster version of BERT.\n",
    "    model_name = 'distilbert-base-uncased'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(device)\n",
    "    print(f\"‚úÖ Successfully loaded pre-trained '{model_name}' model.\")\n",
    "\n",
    "    # --- Compare embeddings for the same word in different contexts ---\n",
    "    sentence1 = \"The tire pressure is low.\"\n",
    "    sentence2 = \"He felt the pressure of the deadline.\"\n",
    "\n",
    "    def get_word_embedding(sentence, word, tokenizer, model):\n",
    "        \"\"\"\n",
    "        Gets the contextual embedding for a specific word in a sentence.\n",
    "        \"\"\"\n",
    "        # Tokenize the sentence\n",
    "        inputs = tokenizer(sentence, return_tensors='pt').to(device)\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "        \n",
    "        # Find the index of our target word\n",
    "        try:\n",
    "            word_index = tokens.index(word)\n",
    "        except ValueError:\n",
    "            print(f\"Word '{word}' not found as a single token in the sentence.\")\n",
    "            return None\n",
    "            \n",
    "        # Get the model's output (the hidden states)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        # Extract the embedding for our target word\n",
    "        word_embedding = last_hidden_states[0, word_index, :].cpu()\n",
    "        return word_embedding\n",
    "\n",
    "    # Get the two different embeddings for \"pressure\"\n",
    "    embedding1 = get_word_embedding(sentence1, 'pressure', tokenizer, model)\n",
    "    embedding2 = get_word_embedding(sentence2, 'pressure', tokenizer, model)\n",
    "\n",
    "    if embedding1 is not None and embedding2 is not None:\n",
    "        # Calculate the cosine similarity between the two vectors\n",
    "        similarity = F.cosine_similarity(embedding1.unsqueeze(0), embedding2.unsqueeze(0)).item()\n",
    "\n",
    "        print(f\"\\n--- Contextual Embedding Comparison for the word 'pressure' ---\")\n",
    "        print(f\"Sentence 1: '{sentence1}'\")\n",
    "        print(f\"Sentence 2: '{sentence2}'\")\n",
    "        print(f\"\\nCosine Similarity between the two embeddings: {similarity:.3f}\")\n",
    "\n",
    "        if similarity < 0.9:\n",
    "            print(\"\\n‚úÖ The embeddings are different! BERT has captured the different contexts.\")\n",
    "        else:\n",
    "            print(\"\\nü§î The embeddings are very similar. This can happen with some models/sentences.\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è transformers library not found. Skipping BERT section.\")\n",
    "    print(\"Install it with: pip install transformers\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not download/run BERT model. Skipping section. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2a9097",
   "metadata": {},
   "source": [
    "## üéâ Summary & Key Takeaways\n",
    "\n",
    "Excellent work! You've journeyed from the most basic form of word representation to the state-of-the-art, building a foundational understanding of how modern NLP models process language.\n",
    "\n",
    "### The Evolution of Embeddings:\n",
    "\n",
    "1.  **One-Hot Encoding:**\n",
    "    *   **Concept:** A sparse vector with a single `1`.\n",
    "    *   **Flaw:** Assumes all words are completely unrelated (orthogonal vectors). Useless for capturing meaning.\n",
    "\n",
    "2.  **Co-occurrence Matrices:**\n",
    "    *   **Concept:** Count how often words appear near each other. The rows of this matrix are simple embeddings.\n",
    "    *   **Improvement:** Captures the distributional hypothesis (\"you are known by the company you keep\"). Similar words have similar vectors.\n",
    "    *   **Flaw:** High-dimensional, sparse, and doesn't scale well.\n",
    "\n",
    "3.  **Learned Static Embeddings (Word2Vec, GloVe):**\n",
    "    *   **Concept:** Train a neural network to *learn* low-dimensional, dense embeddings.\n",
    "    *   **Improvement:** Computationally efficient, dense, and captures complex semantic relationships (e.g., analogies like \"king - man + woman = queen\").\n",
    "    *   **Flaw:** Static‚Äîone word has only one vector, regardless of context.\n",
    "\n",
    "4.  **Contextual Embeddings (BERT, Transformers):**\n",
    "    *   **Concept:** Generate embeddings for a word based on the entire sentence it appears in.\n",
    "    *   **Improvement:** Dynamic and context-aware. Solves the problem of words with multiple meanings (polysemy).\n",
    "    *   **Current State-of-the-Art:** The foundation for all modern Large Language Models.\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "Now that you understand how to turn text into meaningful numbers, you're ready to build powerful NLP models. In the next notebook, we'll dive into the **HuggingFace Transformers library**, the go-to toolkit for working with pre-trained models like BERT for tasks like sentiment analysis, question answering, and more.\n",
    "\n",
    "<div align=\"center\">\n",
    "<b>You've mastered the building blocks. Now, let's build something powerful. üöÄ</b>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
