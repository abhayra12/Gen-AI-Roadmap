{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d61f7a39",
   "metadata": {},
   "source": [
    "# üß≤ Notebook 06: Embeddings & Vector Semantics\n",
    "\n",
    "**Week 3-4: Deep Learning & NLP Foundations**  \n",
    "**Gen AI Masters Program**\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Objectives\n",
    "\n",
    "By the end of this notebook, you will master:\n",
    "1. ‚úÖ One-hot vs dense embeddings\n",
    "2. ‚úÖ Co-occurrence matrices and dimensionality reduction\n",
    "3. ‚úÖ Training Word2Vec (skip-gram) from scratch in PyTorch\n",
    "4. ‚úÖ Visualizing manufacturing vocabulary in embedding space\n",
    "5. ‚úÖ Using pre-trained GloVe embeddings\n",
    "6. ‚úÖ Extracting contextual embeddings with Transformers\n",
    "\n",
    "**Estimated Time:** 4 hours\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Why Embeddings Matter\n",
    "\n",
    "Embeddings map discrete tokens to continuous vectors that capture:\n",
    "- Semantic similarity (\"temperature\" ‚Üî \"heat\")\n",
    "- Relationships (\"pump\" ‚Üî \"hydraulic\")\n",
    "- Manufacturing context (\"vibration\" ‚Üî \"bearing\")\n",
    "\n",
    "Embeddings power attention, transformers, and downstream generative models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748bb749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Tuple\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'‚úÖ Using device: {device}')\n",
    "print(f'PyTorch version: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a0abf3",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ One-Hot vs Dense Representations\n",
    "\n",
    "### One-Hot Encoding\n",
    "- Vector of vocabulary size\n",
    "- Exactly one \"1\"; rest are zeros\n",
    "- No notion of similarity\n",
    "\n",
    "### Dense Embeddings\n",
    "- Low-dimensional vectors (e.g., 100)\n",
    "- Learned from data\n",
    "- Capture semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97aebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "manufacturing_vocab = ['temperature', 'vibration', 'pressure', 'coolant', 'bearing', 'leakage']\n",
    "vocab_to_idx = {word: idx for idx, word in enumerate(manufacturing_vocab)}\n",
    "\n",
    "def one_hot(word):\n",
    "        vec = np.zeros(len(manufacturing_vocab))\n",
    "        vec[vocab_to_idx[word]] = 1\n",
    "        return vec\n",
    "\n",
    "hot_temp = one_hot('temperature')\n",
    "hot_vibration = one_hot('vibration')\n",
    "\n",
    "print('One-hot for temperature:', hot_temp)\n",
    "print('Dot product (temperature ¬∑ vibration):', np.dot(hot_temp, hot_vibration))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85ea7de",
   "metadata": {},
   "source": [
    "### Learned Embeddings\n",
    "We'll create a toy embedding matrix and visualize distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24981c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 4\n",
    "embeddings = nn.Embedding(len(manufacturing_vocab), embedding_dim)\n",
    "torch.nn.init.xavier_uniform_(embeddings.weight)\n",
    "\n",
    "temp_vec = embeddings(torch.tensor(vocab_to_idx['temperature'])).detach().numpy()\n",
    "vibration_vec = embeddings(torch.tensor(vocab_to_idx['vibration'])).detach().numpy()\n",
    "print('Embedding temperature:', np.round(temp_vec, 3))\n",
    "print('Embedding vibration   :', np.round(vibration_vec, 3))\n",
    "print('Cosine similarity     :', np.dot(temp_vec, vibration_vec) / (np.linalg.norm(temp_vec)*np.linalg.norm(vibration_vec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcf0257",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Co-occurrence Matrix\n",
    "\n",
    "We'll build a co-occurrence matrix from synthetic maintenance reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f72013",
   "metadata": {},
   "outputs": [],
   "source": [
    "maintenance_corpus = [\n",
    "    'temperature spike detected in furnace chamber',\n",
    "    'vibration increase near main bearing housing',\n",
    "    'pressure drop indicates coolant circulation issue',\n",
    "    'coolant leakage observed below hydraulic press',\n",
    "    'bearing overheating threatens production halt',\n",
    "    'sensor outage causing data gap in historian',\n",
    "    'hydraulic pump cavitation producing audible noise',\n",
    "    'lubrication schedule missed for gear assembly',\n",
    "    'conveyor torque variation impacts line speed',\n",
    "    'unexpected voltage surge tripped safety relay'\n",
    "]\n",
    "\n",
    "window_size = 2\n",
    "tokenized_docs = [doc.lower().split() for doc in maintenance_corpus]\n",
    "vocab = sorted({word for doc in tokenized_docs for word in doc})\n",
    "idx = {word: i for i, word in enumerate(vocab)}\n",
    "co_matrix = np.zeros((len(vocab), len(vocab)), dtype=np.float32)\n",
    "\n",
    "for doc in tokenized_docs:\n",
    "        for i, word in enumerate(doc):\n",
    "            word_idx = idx[word]\n",
    "            start = max(i - window_size, 0)\n",
    "            end = min(i + window_size + 1, len(doc))\n",
    "            for j in range(start, end):\n",
    "                if i != j:\n",
    "                    context_idx = idx[doc[j]]\n",
    "                    co_matrix[word_idx, context_idx] += 1\n",
    "\n",
    "co_df = pd.DataFrame(co_matrix, index=vocab, columns=vocab)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(co_df, cmap='Blues')\n",
    "plt.title('Co-occurrence Matrix (Window=2)', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e761a9b3",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bea33f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "co_embeddings = pca.fit_transform(co_matrix)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for word, (x, y) in zip(vocab, co_embeddings):\n",
    "        plt.scatter(x, y, color='steelblue')\n",
    "        plt.text(x + 0.02, y + 0.02, word, fontsize=10)\n",
    "plt.title('PCA Projection of Co-occurrence Embeddings')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5791e87",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Training Word2Vec (Skip-Gram) from Scratch\n",
    "\n",
    "We'll train on the maintenance corpus to learn domain-specific embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1761f91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, tokenized_docs: List[List[str]], vocab_to_idx: dict, window_size: int = 2):\n",
    "        self.pairs = []\n",
    "        for doc in tokenized_docs:\n",
    "            indexed = [vocab_to_idx[word] for word in doc]\n",
    "            for i, center in enumerate(indexed):\n",
    "                start = max(i - window_size, 0)\n",
    "                end = min(i + window_size + 1, len(indexed))\n",
    "                for j in range(start, end):\n",
    "                    if i != j:\n",
    "                        context = indexed[j]\n",
    "                        self.pairs.append((center, context))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.pairs[idx][0]), torch.tensor(self.pairs[idx][1])\n",
    "\n",
    "\n",
    "skipgram_dataset = SkipGramDataset(tokenized_docs, idx, window_size=2)\n",
    "skipgram_loader = DataLoader(skipgram_dataset, batch_size=64, shuffle=True)\n",
    "print(f'Total skip-gram pairs: {len(skipgram_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca32ad5",
   "metadata": {},
   "source": [
    "### Skip-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99250b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.target_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.context_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, center_words, context_words):\n",
    "        center_vecs = self.target_embeddings(center_words)\n",
    "        context_vecs = self.context_embeddings(context_words)\n",
    "        scores = torch.sum(center_vecs * context_vecs, dim=1)\n",
    "        return scores\n",
    "\n",
    "\n",
    "EMBED_DIM = 100\n",
    "model = SkipGramModel(len(vocab), EMBED_DIM).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Negative sampling: create noise distribution\n",
    "word_freqs = np.array([vocab.count(word) for word in vocab], dtype=np.float32)\n",
    "word_freqs = word_freqs / word_freqs.sum()\n",
    "noise_distribution = torch.tensor(word_freqs ** 0.75 / np.sum(word_freqs ** 0.75))\n",
    "\n",
    "def negative_sampling_loss(model, center_words, pos_context, neg_sample_size=5):\n",
    "    center_embeds = model.target_embeddings(center_words)\n",
    "    pos_embeds = model.context_embeddings(pos_context)\n",
    "\n",
    "    pos_scores = torch.sum(center_embeds * pos_embeds, dim=1)\n",
    "    pos_loss = F.logsigmoid(pos_scores)\n",
    "\n",
    "    batch_size = center_words.size(0)\n",
    "    neg_words = torch.multinomial(noise_distribution, batch_size * neg_sample_size, replacement=True).to(device)\n",
    "    neg_words = neg_words.view(batch_size, neg_sample_size)\n",
    "    neg_embeds = model.context_embeddings(neg_words)\n",
    "\n",
    "    neg_scores = torch.bmm(neg_embeds, center_embeds.unsqueeze(2)).squeeze()\n",
    "    neg_loss = F.logsigmoid(-neg_scores).sum(dim=1)\n",
    "\n",
    "    return -(pos_loss + neg_loss).mean()\n",
    "\n",
    "\n",
    "EPOCHS = 200\n",
    "loss_history = []\n",
    "print('üîÑ Training Word2Vec skip-gram embeddings...')\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_loss = 0\n",
    "    for centers, contexts in skipgram_loader:\n",
    "        centers = centers.to(device)\n",
    "        contexts = contexts.to(device)\n",
    "\n",
    "        loss = negative_sampling_loss(model, centers, contexts, neg_sample_size=10)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * centers.size(0)\n",
    "\n",
    "    avg_loss = epoch_loss / len(skipgram_dataset)\n",
    "    loss_history.append(avg_loss)\n",
    "    if epoch % 40 == 0 or epoch == 1:\n",
    "        print(f'Epoch {epoch:03d} | Loss: {avg_loss:.4f}')\n",
    "\n",
    "print('‚úÖ Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5588ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(loss_history, color='blue', linewidth=2)\n",
    "plt.title('Skip-Gram Training Loss', fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Negative Sampling Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9a5775",
   "metadata": {},
   "source": [
    "### Visualizing Learned Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e675d73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    word_vectors = model.target_embeddings.weight.cpu().numpy()\n",
    "\n",
    "two_d = TSNE(n_components=2, random_state=42, perplexity=5).fit_transform(word_vectors)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for word, (x, y) in zip(vocab, two_d):\n",
    "    plt.scatter(x, y, color='darkorange')\n",
    "    plt.text(x + 0.05, y + 0.05, word, fontsize=10)\n",
    "plt.title('t-SNE Visualization of Manufacturing Word2Vec Embeddings')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0adc148",
   "metadata": {},
   "source": [
    "### Similarity Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9b8809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word: str, top_k: int = 5):\n",
    "    if word not in idx:\n",
    "        raise ValueError('Word not in vocabulary')\n",
    "    vector = word_vectors[idx[word]]\n",
    "    similarities = []\n",
    "    for other_word in vocab:\n",
    "        sim = np.dot(vector, word_vectors[idx[other_word]]) / (np.linalg.norm(vector) * np.linalg.norm(word_vectors[idx[other_word]]) + 1e-9)\n",
    "        similarities.append((other_word, sim))\n",
    "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "    return similarities[1:top_k+1]\n",
    "\n",
    "print('Top neighbors for \n",
    ":')\n",
    "for word, score in most_similar('vibration'):\n",
    "    print(f'  {word:15s} -> {score:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8751ba",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Using Pre-trained GloVe Embeddings\n",
    "\n",
    "GloVe (Global Vectors) captures global co-occurrence statistics.\n",
    "\n",
    "We'll demonstrate how to load embeddings using `torchtext` (requires internet to download at runtime)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c5028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Uncomment to download pre-trained GloVe vectors (requires internet)\n",
    "# from torchtext.vocab import GloVe\n",
    "# glove = GloVe(name='6B', dim=100)  # 100-dimensional embeddings\n",
    "#\n",
    "# def glove_vector(word: str):\n",
    "#     return glove[word]\n",
    "#\n",
    "# manufacturing_terms = ['sensor', 'factory', 'hydraulic', 'temperature', 'safety', 'robot']\n",
    "# for term in manufacturing_terms:\n",
    "#     vec = glove_vector(term)\n",
    "#     print(f'{term:12s} | vector norm: {vec.norm().item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7adc12a",
   "metadata": {},
   "source": [
    "### Projecting GloVe Embeddings (If Available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30964d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Visualize subset of GloVe vectors\n",
    "# subset = torch.stack([glove[word] for word in manufacturing_terms])\n",
    "# tsne_glove = TSNE(n_components=2, random_state=42).fit_transform(subset.numpy())\n",
    "#\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# for (x, y), word in zip(tsne_glove, manufacturing_terms):\n",
    "#     plt.scatter(x, y, color='teal')\n",
    "#     plt.text(x + 0.02, y + 0.02, word, fontsize=12)\n",
    "# plt.title('t-SNE of GloVe Embeddings (Manufacturing Terms)')\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9c7fd7",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Contextual Embeddings with Transformers\n",
    "\n",
    "Static embeddings assign one vector per word. Contextual embeddings (BERT, RoBERTa, DistilBERT) change with sentence context.\n",
    "\n",
    "We'll generate sentence embeddings for maintenance logs using HuggingFace Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addc6328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Requires transformers library and internet to download the model\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "#\n",
    "# model_name = 'distilbert-base-uncased'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# bert_model = AutoModel.from_pretrained(model_name).to(device)\n",
    "#\n",
    "# maintenance_sentences = [\n",
    "#     'Hydraulic pump failure triggered an emergency shutdown',\n",
    "#     'Sensor drift detected during calibration routine',\n",
    "#     'Operators reported excessive vibration near cutter assembly',\n",
    "#     'Thermal runaway prevented by automatic coolant injection',\n",
    "#     'Preventive maintenance completed without incidents'\n",
    "# ]\n",
    "#\n",
    "# def encode_sentences(sentences: List[str]):\n",
    "#     encoded = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = bert_model(**encoded)\n",
    "#     # Use CLS token (first token) as sentence embedding\n",
    "#     cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "#     return cls_embeddings.cpu().numpy()\n",
    "#\n",
    "# contextual_vectors = encode_sentences(maintenance_sentences)\n",
    "# tsne_contextual = TSNE(n_components=2, random_state=42).fit_transform(contextual_vectors)\n",
    "#\n",
    "# plt.figure(figsize=(9, 6))\n",
    "# for (x, y), sentence in zip(tsne_contextual, maintenance_sentences):\n",
    "#     plt.scatter(x, y, color='crimson')\n",
    "#     plt.text(x + 0.02, y + 0.02, sentence[:30] + '...', fontsize=9)\n",
    "# plt.title('Contextual Embeddings (DistilBERT CLS)')\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dc5f96",
   "metadata": {},
   "source": [
    "### Semantic Similarity with Contextual Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2e3159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Compute cosine similarity between sentence embeddings\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# sim_matrix = cosine_similarity(contextual_vectors)\n",
    "#\n",
    "# plt.figure(figsize=(6, 5))\n",
    "# sns.heatmap(sim_matrix, annot=True, cmap='Purples', xticklabels=False, yticklabels=False)\n",
    "# plt.title('Contextual Embedding Similarity (DistilBERT)')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d405b32",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Embedding Best Practices\n",
    "\n",
    "1. **Domain adaptation**: Fine-tune embeddings on manufacturing corpora\n",
    "2. **Out-of-vocabulary (OOV)**: Use subword tokenization (BPE, WordPiece)\n",
    "3. **Dimensionality**: Balance expressiveness vs compute (128-768 for transformers)\n",
    "4. **Normalization**: Normalize vectors for cosine comparisons\n",
    "5. **Storage**: Persist embeddings for downstream predictive maintenance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2a9097",
   "metadata": {},
   "source": [
    "## üéâ Summary\n",
    "\n",
    "Awesome work embedding knowledge into vectors!\n",
    "\n",
    "### Key Concepts\n",
    "- ‚úÖ One-hot vs dense embeddings\n",
    "- ‚úÖ Co-occurrence matrices & PCA\n",
    "- ‚úÖ Training Word2Vec (skip-gram with negative sampling)\n",
    "- ‚úÖ Visualizing manufacturing semantics\n",
    "- ‚úÖ Loading pre-trained GloVe vectors\n",
    "- ‚úÖ Extracting contextual embeddings with transformers\n",
    "\n",
    "### What You Built\n",
    "1. üßÆ Co-occurrence matrix for maintenance corpora\n",
    "2. üß† Skip-gram Word2Vec trainer in PyTorch\n",
    "3. üîç t-SNE visualization of manufacturing vocabulary\n",
    "4. üß± Cosine similarity explorer\n",
    "5. üß∞ Optional hooks into GloVe & DistilBERT\n",
    "\n",
    "### Manufacturing Insights\n",
    "- Group similar failure terms for root-cause libraries\n",
    "- Use embeddings to cluster maintenance logs and highlight emerging issues\n",
    "- Initialize downstream classifiers with domain-specific vectors\n",
    "\n",
    "### Next Steps\n",
    "Advance to **Notebook 07: HuggingFace Transformers** to integrate these embeddings into production workflows.\n",
    "\n",
    "<div align=\"center\">\n",
    "<b>Embeddings secured! Onward to HuggingFace pipelines. ü§ùüöÄ</b>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
