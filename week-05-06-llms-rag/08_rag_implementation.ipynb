{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "970a58f2",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Week 5-6 ¬∑ Notebook 08 ¬∑ RAG Implementation with LangChain\n",
    "\n",
    "**Module:** LLMs, Prompt Engineering & RAG  \n",
    "**Project:** Build the Knowledge Core for the Manufacturing Copilot\n",
    "\n",
    "---\n",
    "\n",
    "In the previous notebook, we built a RAG pipeline from scratch to understand the core concepts. Now, we'll use **LangChain**, a powerful library that simplifies building complex LLM applications. LangChain provides ready-to-use components for every step of the RAG process, allowing us to build a more robust and maintainable system for our Manufacturing Copilot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd43ecc7",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. ‚úÖ **Use LangChain Components:** Implement a RAG pipeline using LangChain's `DocumentLoader`, `TextSplitter`, `Embeddings`, and `VectorStore`.\n",
    "2. ‚úÖ **Build a `RetrievalQA` Chain:** Create and run a question-answering chain that automatically handles retrieval and generation.\n",
    "3. ‚úÖ **Customize Prompts in LangChain:** Modify the prompt template within a chain to control the LLM's output.\n",
    "4. ‚úÖ **Return Source Documents:** Configure a RAG chain to cite its sources, a critical feature for trustworthy AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dab3c48",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Setup: Installing and Importing Libraries\n",
    "\n",
    "LangChain integrates with many other tools, so we'll need to install a few packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce443585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q langchain langchain-community sentence-transformers chromadb transformers torch\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54566544",
   "metadata": {},
   "source": [
    "## Step 1: Load and Prepare Documents\n",
    "\n",
    "First, we need some documents for our knowledge base. We'll create a few dummy SOP text files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1388a86d",
   "metadata": {},
   "source": [
    "# Create a directory for our documents\n",
    "os.makedirs(\"sops\", exist_ok=True)\n",
    "\n",
    "sop_docs = {\n",
    "    \"SOP-HYD-001.txt\": \"For hydraulic press maintenance, first perform lockout-tagout. Then, inspect all seals for leaks, verify pressure gauges read within the 500-550 PSI range, and top off hydraulic fluid if necessary. Check fluid levels weekly.\",\n",
    "    \"SOP-CONV-003.txt\": \"To troubleshoot a conveyor belt stoppage, first check for physical obstructions. If clear, verify the motor's thermal overload has not tripped. Finally, inspect belt tension and sensor alignment. Belt tension should be checked monthly.\",\n",
    "    \"SOP-ROBO-002.txt\": \"Preventive maintenance for a robotic arm involves greasing all major joints monthly, recalibrating torque sensors quarterly, and validating vision system alignment weekly.\"\n",
    "}\n",
    "\n",
    "for filename, content in sop_docs.items():\n",
    "    with open(os.path.join(\"sops\", filename), \"w\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "# Load the documents using LangChain's TextLoader\n",
    "loaders = [TextLoader(os.path.join(\"sops\", name)) for name in os.listdir(\"sops\")]\n",
    "documents = []\n",
    "for loader in loaders:\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c027196",
   "metadata": {},
   "source": [
    "### Chunk the Documents\n",
    "\n",
    "Next, we split the loaded documents into smaller chunks. This is crucial for effective retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2956a626",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=150, chunk_overlap=20)\n",
    "chunked_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Split {len(documents)} documents into {len(chunked_docs)} chunks.\")\n",
    "print(\"\\n--- Example Chunk ---\")\n",
    "print(chunked_docs[0].page_content)\n",
    "print(chunked_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffebe81",
   "metadata": {},
   "source": [
    "## Step 2: Create Embeddings and a Vector Store\n",
    "\n",
    "Now we convert the chunks into embeddings and store them in a `Chroma` vector store. This creates our searchable knowledge index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4a9ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a standard Hugging Face model for embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create the Chroma vector store from the chunked documents\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=chunked_docs,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"./chroma_db\" # Persist the DB to disk\n",
    ")\n",
    "\n",
    "print(f\"Vector store created with {vector_store._collection.count()} vectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300d2b6d",
   "metadata": {},
   "source": [
    "## Step 3: Set Up the LLM and RetrievalQA Chain\n",
    "\n",
    "This is where LangChain shines. We'll wrap our LLM in a `HuggingFacePipeline` and then create a `RetrievalQA` chain that connects all the pieces: the LLM, the vector store (as a retriever), and a custom prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e053ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the LLM pipeline\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "llm_pipeline = pipeline(\n",
    "    'text2text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    max_length=200\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
    "\n",
    "# Create a retriever from the vector store\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 2}) # Retrieve top 2 chunks\n",
    "\n",
    "# Define a custom prompt template\n",
    "prompt_template = \"\"\"You are a manufacturing assistant. Use the following context to answer the question. If you don't know the answer, say so.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# Create the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\", # \"stuff\" means all retrieved chunks are stuffed into the prompt\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True, # This is key to getting citations\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "\n",
    "print(\"RetrievalQA chain created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f05329",
   "metadata": {},
   "source": [
    "## Step 4: Run Queries and Get Answers with Sources\n",
    "\n",
    "Now we can ask our chain questions. It will perform the retrieval, prompt augmentation, and generation all in one step, and it will return the source documents it used!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084c4697",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How often should I check hydraulic press fluid levels?\"\n",
    "\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "\n",
    "print(\"--- Question ---\")\n",
    "print(question)\n",
    "print(\"\\n--- Answer ---\")\n",
    "print(result['result'])\n",
    "print(\"\\n--- Source Documents ---\")\n",
    "for doc in result['source_documents']: \n",
    "    print(f\"- Source: {doc.metadata['source']}, Content: '{doc.page_content}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10126f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_2 = \"What is the first step for robot maintenance?\"\n",
    "\n",
    "result_2 = qa_chain.invoke({\"query\": question_2})\n",
    "\n",
    "print(\"--- Question ---\")\n",
    "print(question_2)\n",
    "print(\"\\n--- Answer ---\")\n",
    "print(result_2['result'])\n",
    "print(\"\\n--- Source Documents ---\")\n",
    "for doc in result_2['source_documents']: \n",
    "    print(f\"- Source: {doc.metadata['source']}, Content: '{doc.page_content}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d50d21",
   "metadata": {},
   "source": [
    "## ‚úÖ Next Steps\n",
    "\n",
    "This notebook demonstrated the power and simplicity of using LangChain to build a RAG pipeline. By using high-level abstractions like `RetrievalQA`, we can create a sophisticated, source-citing question-answering system with just a few lines of code.\n",
    "\n",
    "Key takeaways:\n",
    "- **Modularity:** LangChain lets you easily swap out components (different LLMs, vector stores, etc.).\n",
    "- **Simplicity:** High-level chains like `RetrievalQA` abstract away the boilerplate code for retrieval, prompting, and generation.\n",
    "- **Traceability:** The ability to return source documents is crucial for building trust and allowing users to verify answers.\n",
    "\n",
    "In the final notebook of this module, **`09_vector_embeddings.ipynb`**, we will dive deeper into the heart of our RAG system: the vector embeddings themselves. We'll explore what they are, how they are created, and why choosing the right embedding model is so important."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
