{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "970a58f2",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Week 5-6 ¬∑ Notebook 08 ¬∑ Production-Grade RAG with LangChain\n",
    "\n",
    "**Module:** LLMs, Prompt Engineering & RAG  \n",
    "**Project:** Build the Knowledge Core for the Manufacturing Copilot\n",
    "\n",
    "---\n",
    "\n",
    "### From Scratch to Framework: Scaling our RAG System\n",
    "\n",
    "In the previous notebook, we built a complete Retrieval-Augmented Generation (RAG) pipeline from scratch. This was essential for understanding the fundamental mechanics: indexing, retrieval, and generation.\n",
    "\n",
    "However, building for production requires more than just understanding. It demands tools that are robust, scalable, and maintainable. This is where frameworks like **LangChain** come in.\n",
    "\n",
    "LangChain provides a comprehensive toolkit of pre-built, modular components that streamline the development of complex LLM applications. Instead of writing boilerplate code for loading documents, managing vector stores, or chaining prompts, we can use LangChain's high-level APIs to assemble a production-ready RAG pipeline quickly and efficiently.\n",
    "\n",
    "In this notebook, we will rebuild our manufacturing knowledge base using LangChain, transforming our simple proof-of-concept into a powerful, framework-driven application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd43ecc7",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to build a robust, production-style RAG pipeline for our Manufacturing Copilot. You will learn to:\n",
    "\n",
    "1.  **Master the LangChain RAG Toolkit:**\n",
    "    -   **`DocumentLoader`**: Automatically load and parse knowledge from various sources (like our SOP text files).\n",
    "    -   **`TextSplitter`**: Intelligently chunk large documents into smaller, semantically meaningful pieces for effective retrieval.\n",
    "    -   **`HuggingFaceEmbeddings`**: Convert text chunks into vector embeddings using powerful open-source models.\n",
    "    -   **`Chroma` (VectorStore)**: Create a persistent, searchable vector database to serve as the long-term memory for our copilot.\n",
    "\n",
    "2.  **Construct a `RetrievalQA` Chain:**\n",
    "    -   Assemble a complete, end-to-end question-answering system that seamlessly connects the user's query to the vector store and the LLM.\n",
    "\n",
    "3.  **Engineer Prompts within a Chain:**\n",
    "    -   Inject custom instructions into the `RetrievalQA` chain to precisely control the LLM's behavior, ensuring it acts as a helpful manufacturing assistant.\n",
    "\n",
    "4.  **Implement Source Citation for Trustworthiness:**\n",
    "    -   Configure the chain to return the exact source documents it used to generate an answer, a critical feature for building user trust and enabling fact-checking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dab3c48",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 1: Environment Setup\n",
    "\n",
    "First, we need to install the necessary libraries. LangChain is a modular framework, so we install the core `langchain` package along with specific integrations we'll be using:\n",
    "\n",
    "-   `langchain-community`: Provides integrations with community-maintained tools like Hugging Face models and ChromaDB.\n",
    "-   `sentence-transformers`: The library that powers our embedding model.\n",
    "-   `chromadb`: The high-performance vector store we'll use as our knowledge base.\n",
    "-   `transformers` and `torch`: The foundational libraries for running our local LLM.\n",
    "\n",
    "This modular approach allows us to keep our environment lean by only installing what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce443585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "!pip install -q langchain langchain-community sentence-transformers chromadb transformers torch\n",
    "\n",
    "# --- Core LangChain Imports ---\n",
    "# Document processing\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Vector store and embeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# LLM and pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# Chains and prompts\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# --- Standard Library Imports ---\n",
    "import os\n",
    "\n",
    "print(\"Libraries installed and imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54566544",
   "metadata": {},
   "source": [
    "## üìö Step 2: Build the Knowledge Base\n",
    "\n",
    "The first phase in any RAG system is building the knowledge base. With LangChain, this process is broken down into three clean, modular steps: **Load**, **Split**, and **Store**.\n",
    "\n",
    "### 2.1. Create Dummy Data: Standard Operating Procedures (SOPs)\n",
    "\n",
    "First, let's create the raw material for our knowledge base. We'll simulate a set of Standard Operating Procedure (SOP) documents that our Manufacturing Copilot will need to understand. These plain text files represent the \"knowledge\" we want to inject into our LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c027196",
   "metadata": {},
   "source": [
    "### 2.3. Split Documents with `RecursiveCharacterTextSplitter`\n",
    "\n",
    "LLMs have a limited context window, meaning they can only process a certain amount of text at once. Therefore, we must split our large documents into smaller, more manageable chunks.\n",
    "\n",
    "However, we must be careful not to split the text in the middle of a sentence, which could destroy its meaning. LangChain's `RecursiveCharacterTextSplitter` is a smart solution. It attempts to split text based on a prioritized list of separators (like `\\n\\n` for paragraphs, then `\\n` for lines, then spaces) to keep semantically related pieces of text together as much as possible.\n",
    "\n",
    "-   `chunk_size`: Defines the maximum size of each chunk (in characters).\n",
    "-   `chunk_overlap`: Creates a small overlap between chunks. This helps preserve context for sentences that might otherwise be split across two chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2956a626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the documents\n",
    "loader_paths = [os.path.join(\"sops\", name) for name in os.listdir(\"sops\")]\n",
    "loaders = [TextLoader(path) for path in loader_paths]\n",
    "documents = []\n",
    "for loader in loaders:\n",
    "    documents.extend(loader.load())\n",
    "print(f\"Loaded {len(documents)} documents.\")\n",
    "\n",
    "# 2. Split the documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=50)\n",
    "chunked_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Split {len(documents)} documents into {len(chunked_docs)} chunks.\")\n",
    "print(\"\\n--- Example Chunk ---\")\n",
    "print(\"Content:\")\n",
    "print(chunked_docs[0].page_content)\n",
    "print(\"\\nMetadata:\")\n",
    "print(chunked_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffebe81",
   "metadata": {},
   "source": [
    "### 2.4. Store Chunks in a `Chroma` Vector Store\n",
    "\n",
    "Once our knowledge is loaded and chunked, the final step is to **index** it for fast retrieval. This involves two parts:\n",
    "\n",
    "1.  **Embedding Model (`HuggingFaceEmbeddings`)**: We select an embedding model to convert our text chunks into numerical vectors (embeddings). We'll use `all-MiniLM-L6-v2`, a high-quality, lightweight model perfect for this task. LangChain's `HuggingFaceEmbeddings` wrapper makes it trivial to use any model from the Hugging Face Hub.\n",
    "\n",
    "2.  **Vector Store (`Chroma`)**: We choose a vector store to house our embeddings. `Chroma` is a popular open-source vector database that runs locally, making it perfect for development. It stores the embeddings and provides a powerful `similarity_search` function.\n",
    "\n",
    "LangChain's `Chroma.from_documents` function elegantly handles this entire process: it takes our chunked documents, passes them through the embedding model, and stores the resulting vectors in the Chroma database in a single line of code. We also set `persist_directory` to save our database to disk, so we don't have to rebuild it every time we run the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4a9ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the embedding model we'll use\n",
    "# \"all-MiniLM-L6-v2\" is a popular and efficient model for generating sentence embeddings.\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Define the directory to persist the vector store\n",
    "# Persisting the DB saves computation time, as we don't need to re-create embeddings every time.\n",
    "persist_directory = \"./chroma_db_langchain\"\n",
    "\n",
    "# Create the Chroma vector store\n",
    "# This single command does the following:\n",
    "# 1. Takes the `chunked_docs`.\n",
    "# 2. Uses `embedding_model` to convert each chunk's content into a vector.\n",
    "# 3. Stores the vector and the original document chunk (including metadata) in the Chroma database.\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=chunked_docs,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "print(f\"Successfully created and persisted vector store with {vector_store._collection.count()} vectors.\")\n",
    "print(f\"Database is stored in: {os.path.abspath(persist_directory)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300d2b6d",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 3: Build the RAG Chain\n",
    "\n",
    "With our knowledge base indexed and ready, we can now build the \"live\" part of our RAG system: the question-answering chain. This chain will orchestrate the entire process of receiving a user query, retrieving relevant information, and generating an answer.\n",
    "\n",
    "LangChain's `RetrievalQA` chain is the perfect tool for this. It elegantly connects three key components:\n",
    "1.  **The LLM:** The \"brain\" that will generate the final answer.\n",
    "2.  **The Retriever:** The \"librarian\" that searches the vector store for relevant documents.\n",
    "3.  **The Prompt Template:** The \"instructions\" that guide the LLM on how to behave and how to use the retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e053ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Set up the LLM ---\n",
    "# We'll use the same local Hugging Face pipeline as in the previous notebook.\n",
    "# LangChain's `HuggingFacePipeline` wrapper makes it easy to integrate.\n",
    "print(\"Setting up the LLM...\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Create a text-generation pipeline\n",
    "llm_pipeline = pipeline(\n",
    "    'text2text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    max_length=256,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95\n",
    ")\n",
    "\n",
    "# Wrap the pipeline in LangChain's utility class\n",
    "llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
    "print(\"LLM setup complete.\")\n",
    "\n",
    "# --- 2. Set up the Retriever ---\n",
    "# The retriever's job is to fetch relevant documents from the vector store.\n",
    "# We can configure it to return the top 'k' most similar documents.\n",
    "print(\"\\nSetting up the Retriever...\")\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 2} # Retrieve the top 2 most relevant chunks\n",
    ")\n",
    "print(\"Retriever setup complete.\")\n",
    "\n",
    "# --- 3. Set up the Prompt Template ---\n",
    "# This is the instruction set for our LLM. It defines how the LLM should use\n",
    "# the retrieved context to answer the user's question.\n",
    "print(\"\\nSetting up the Prompt Template...\")\n",
    "prompt_template = \"\"\"\n",
    "You are a helpful and precise manufacturing assistant. Your role is to answer questions about Standard Operating Procedures (SOPs).\n",
    "Use the following retrieved context to answer the question.\n",
    "If you don't know the answer from the context provided, just say that you don't know. Do not make up an answer.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ACCURATE ANSWER:\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "print(\"Prompt Template setup complete.\")\n",
    "\n",
    "\n",
    "# --- 4. Create the RetrievalQA Chain ---\n",
    "# This is the final step where we assemble all our components.\n",
    "print(\"\\nCreating the RetrievalQA chain...\")\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # \"stuff\" is the simplest method: it \"stuffs\" all retrieved chunks into the prompt.\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True, # This is crucial for citing sources.\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "\n",
    "print(\"RetrievalQA chain created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f05329",
   "metadata": {},
   "source": [
    "## ‚úÖ Step 4: Run Queries and Cite Sources\n",
    "\n",
    "Our RAG system is now fully assembled and ready to use.\n",
    "\n",
    "When we call `qa_chain.invoke()`, LangChain orchestrates the entire RAG flow behind the scenes:\n",
    "1.  The user's `question` is sent to the `retriever`.\n",
    "2.  The `retriever` converts the question into an embedding and searches the `Chroma` vector store for the most similar document chunks.\n",
    "3.  The retrieved chunks (the `context`) and the original `question` are inserted into our `PROMPT` template.\n",
    "4.  The completed prompt is sent to the `llm`.\n",
    "5.  The `llm` generates an answer based *only* on the provided context.\n",
    "6.  Because we set `return_source_documents=True`, the final result includes both the generated `result` (the answer) and the `source_documents` used to create it.\n",
    "\n",
    "This ability to cite sources is a cornerstone of building trustworthy and transparent AI systems. It allows users to verify the information and builds confidence in the copilot's responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084c4697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to run queries and print the results nicely\n",
    "def ask(question: str):\n",
    "    \"\"\"\n",
    "    Invokes the RAG chain with a question and prints the answer and sources.\n",
    "    \"\"\"\n",
    "    # The `invoke` method runs the entire chain\n",
    "    result = qa_chain.invoke({\"query\": question})\n",
    "\n",
    "    print(f\"‚ùì Question: {question}\\n\")\n",
    "    print(f\"‚úÖ Answer: {result['result']}\\n\")\n",
    "    print(\"--- üìú Sources ---\")\n",
    "    \n",
    "    # Print the source documents\n",
    "    for doc in result['source_documents']:\n",
    "        source_file = doc.metadata['source']\n",
    "        content = doc.page_content\n",
    "        print(f\"üìÑ Source: {source_file}\\n   Content: '{content}'\\n\")\n",
    "    print(\"--------------------\")\n",
    "\n",
    "# --- Let's ask our first question! ---\n",
    "question_1 = \"How often should I check hydraulic press fluid levels?\"\n",
    "ask(question_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10126f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Let's try another, more specific question ---\n",
    "question_2 = \"What is the first step for robot maintenance?\"\n",
    "ask(question_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d50d21",
   "metadata": {},
   "source": [
    "## üéâ Congratulations and Next Steps!\n",
    "\n",
    "You have successfully built a production-grade, source-citing RAG pipeline using LangChain!\n",
    "\n",
    "This notebook demonstrated the immense power and efficiency of using a framework. By leveraging LangChain's high-level abstractions, we were able to construct a sophisticated, modular, and trustworthy question-answering system with remarkably little code.\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "-   **Modularity is Power:** LangChain's component-based architecture (Loaders, Splitters, Embedders, Vector Stores, Chains) allows you to easily swap parts of your pipeline. Want to use a different LLM? Just change the `llm` object. Need to load PDFs instead of TXT files? Swap `TextLoader` for `PyPDFLoader`. The rest of your code remains the same.\n",
    "-   **Abstraction Simplifies Complexity:** High-level chains like `RetrievalQA` handle all the complex orchestration of retrieval, prompt formatting, and generation, letting you focus on the overall application logic rather than the boilerplate.\n",
    "-   **Traceability Builds Trust:** The ability to return source documents is not just a feature; it's a fundamental requirement for building responsible AI. It provides transparency and allows users to verify the system's answers, which is critical in a manufacturing or any other high-stakes environment.\n",
    "\n",
    "### Looking Ahead:\n",
    "\n",
    "In the next notebook, **`09_vector_embeddings.ipynb`**, we will peel back one final layer of abstraction and dive deep into the heart of our RAG system: the **vector embeddings**. We will explore:\n",
    "- What are embeddings, intuitively?\n",
    "- How does an embedding model *really* work?\n",
    "- Why is choosing the right embedding model so critical for retrieval quality?\n",
    "\n",
    "Understanding embeddings is the key to mastering R-A-G and unlocking the next level of performance for your AI applications. Let's dive in!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
