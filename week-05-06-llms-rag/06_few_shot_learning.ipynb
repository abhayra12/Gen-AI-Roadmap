{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9f5140c",
   "metadata": {},
   "source": [
    "# 🎯 Week 5-6 · Notebook 06 · Few-Shot Learning for Actionable Insights\n",
    "\n",
    "**Module:** LLMs, Prompt Engineering & RAG  \n",
    "**Project:** Build the Knowledge Core for the Manufacturing Copilot\n",
    "\n",
    "---\n",
    "\n",
    "Fine-tuning a model is expensive and time-consuming. **Few-Shot Learning** offers a powerful alternative: guiding a pre-trained LLM with a handful of high-quality examples directly in the prompt. This notebook demonstrates how to use this technique to build a system that suggests corrective actions for machine operators, a core feature of our Manufacturing Copilot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175b98a5",
   "metadata": {},
   "source": [
    "## 🎯 Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. ✅ **Understand Few-Shot vs. Zero-Shot:** Know when to provide examples to improve performance.\n",
    "2. ✅ **Curate High-Quality Exemplars:** Create a dataset of examples that reflect real-world manufacturing problems and solutions.\n",
    "3. ✅ **Automate Example Selection:** Use semantic similarity to find the most relevant examples for a new query.\n",
    "4. ✅ **Build and Evaluate a Few-Shot System:** Combine prompts, exemplars, and an LLM to generate useful, actionable recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fe0fda",
   "metadata": {},
   "source": [
    "## ⚙️ Setup: Model and Data\n",
    "\n",
    "We'll continue using `google/flan-t5-base` for its balance of performance and resource efficiency. We will also create a small, curated dataset of incident tickets and the corresponding corrective actions taken by experienced engineers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2706204d",
   "metadata": {},
   "source": [
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "generator = pipeline(\n",
    "    'text2text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    max_length=100, # Actions should be concise\n",
    "    temperature=0.1,\n",
    ")\n",
    "# Our curated dataset of high-quality examples (exemplars)\n",
    "exemplars_df = pd.DataFrame([\n",
    "    {\"ticket\": \"Vibration spike on compressor #7 after bearing replacement.\", \"action\": \"Inspect alignment and re-torque fasteners immediately.\"},\n",
    "    {\"ticket\": \"Hydraulic leak detected on the main clamp cylinder of Press-3.\", \"action\": \"Isolate the machine using LOTO protocol and replace the primary seals.\"},\n",
    "    {\"ticket\": \"Camera on SMT line is misreading part numbers due to glare.\", \"action\": \"Adjust the angle of the overhead light and recalibrate vision system thresholds.\"},\n",
    "    {\"ticket\": \"The packing line robot #2 is flagging repeated overcurrent alarms on its wrist axis.\", \"action\": \"Check the robot's payload to ensure it is within spec and recalibrate torque limits.\"},\n",
    "    {\"ticket\": \"AGV-5 is slowing down near charging station B, reporting intermittent lidar faults.\", \"action\": \"Clean the lidar sensor lens and check for firmware updates.\"}\n",
    "])\n",
    "\n",
    "print(\"Exemplar dataset loaded:\")\n",
    "exemplars_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32964c80",
   "metadata": {},
   "source": [
    "## 🛠️ Building a Few-Shot Prompt\n",
    "\n",
    "The core idea is to construct a prompt that includes both the instructions and the examples. The LLM learns the desired input-output pattern from the examples.\n",
    "\n",
    "A good few-shot prompt has three parts:\n",
    "1.  **Instruction:** A clear command telling the model what to do.\n",
    "2.  **Exemplars:** A few `input -> output` pairs.\n",
    "3.  **Query:** The new input for which we want a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d5d464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_few_shot_prompt(examples, query):\n",
    "    \"\"\"Constructs a prompt with instructions and examples.\"\"\"\n",
    "    \n",
    "    # 1. The instruction\n",
    "    instruction = \"You are a senior reliability engineer. Based on the incident ticket, recommend a single, concise corrective action.\"\n",
    "    \n",
    "    # 2. The exemplars\n",
    "    exemplar_texts = []\n",
    "    for _, row in examples.iterrows():\n",
    "        exemplar_texts.append(f\"Ticket: {row['ticket']}\\nAction: {row['action']}\")\n",
    "        \n",
    "    # 3. The new query\n",
    "    query_text = f\"Ticket: {query}\\nAction:\"\n",
    "    \n",
    "    # Combine all parts\n",
    "    return \"\\n===\\n\".join([instruction] + exemplar_texts + [query_text])\n",
    "\n",
    "# Let's test it with a new incident\n",
    "new_ticket = \"The coolant pump for CNC-12 is showing a gradual pressure drop over the last 3 hours.\"\n",
    "# We'll use the first 3 exemplars for our prompt\n",
    "few_shot_prompt = build_few_shot_prompt(exemplars_df.head(3), new_ticket)\n",
    "print(\"--- Generated Few-Shot Prompt ---\")\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa4d6a9",
   "metadata": {},
   "source": [
    "print(\"--- LLM Response (Few-Shot) ---\")\n",
    "response = generator(few_shot_prompt)\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd286a0",
   "metadata": {},
   "source": [
    "## 🤖 Automating Exemplar Selection with Semantic Search\n",
    "\n",
    "Manually picking examples doesn't scale. For a real system, we need to automatically find the most *relevant* examples for any given ticket. We can do this using **semantic search**.\n",
    "\n",
    "The process is:\n",
    "1.  **Embed:** Convert all our exemplar tickets into numerical vectors (embeddings) using a sentence transformer model.\n",
    "2.  **Compare:** Convert the new ticket into an embedding.\n",
    "3.  **Select:** Find the exemplar embeddings that are most similar (closest) to the new ticket's embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0a2806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to install the sentence-transformers library\n",
    "# !pip install -q sentence-transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Use a lightweight but effective model for embedding\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# 1. Embed our exemplar tickets\n",
    "corpus_embeddings = embedder.encode(exemplars_df['ticket'].tolist(), convert_to_tensor=True)\n",
    "\n",
    "# 2. Embed our new ticket\n",
    "query_embedding = embedder.encode(new_ticket, convert_to_tensor=True)\n",
    "\n",
    "# 3. Calculate cosine similarity to find the closest matches\n",
    "similarities = util.cos_sim(query_embedding, corpus_embeddings).squeeze()\n",
    "\n",
    "# Add similarities to our dataframe and sort\n",
    "exemplars_df['similarity'] = similarities.cpu().numpy()\n",
    "retrieval_df = exemplars_df.sort_values(\"similarity\", ascending=False)\n",
    "\n",
    "print(f\"--- Most Relevant Exemplars for query: '{new_ticket}' ---\")\n",
    "retrieval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1b6170",
   "metadata": {},
   "source": [
    "Now we can build a prompt using the *best* examples found by semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c137663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the top 3 most similar exemplars\n",
    "top_k = 3\n",
    "best_exemplars = retrieval_df.head(top_k)\n",
    "\n",
    "# Build a new prompt with these dynamically selected examples\n",
    "dynamic_prompt = build_few_shot_prompt(best_exemplars, new_ticket)\n",
    "\n",
    "print(\"--- Dynamically Generated Few-Shot Prompt ---\")\n",
    "print(dynamic_prompt)\n",
    "\n",
    "print(\"\\n--- LLM Response (Dynamic Few-Shot) ---\")\n",
    "dynamic_response = generator(dynamic_prompt)\n",
    "print(dynamic_response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2083bde8",
   "metadata": {},
   "source": [
    "## ✅ Next Steps\n",
    "\n",
    "This notebook demonstrated the power of few-shot learning with dynamic exemplar selection. By providing relevant, high-quality examples, we can guide a general-purpose LLM to perform a specific, domain-intensive task without any fine-tuning.\n",
    "\n",
    "Key takeaways:\n",
    "- **Quality over Quantity:** A few good examples are better than many poor ones.\n",
    "- **Relevance is Key:** Semantic search is crucial for finding the right examples for a given query.\n",
    "- **Systematic Approach:** Combining a prompt template, an exemplar database, and a retrieval mechanism creates a robust and scalable system.\n",
    "\n",
    "In the next notebook, we will explore the fundamentals of **Retrieval-Augmented Generation (RAG)**, a technique that takes this idea a step further by retrieving information from a large knowledge base (like technical manuals or past incident reports) to answer questions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
