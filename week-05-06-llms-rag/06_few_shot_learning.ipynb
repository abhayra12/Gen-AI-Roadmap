{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c37f4cb0",
   "metadata": {},
   "source": [
    "# üéØ Week 5-6 ¬∑ Notebook 06 ¬∑ Few-Shot Learning\n",
    "\n",
    "Steer large language models with curated in-context examples instead of expensive fine-tuning. We'll design exemplar libraries, automate retrieval, and measure lift for manufacturing workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cfd8e1",
   "metadata": {},
   "source": [
    "## üöÄ Learning Outcomes\n",
    "- Diagnose when few-shot prompting outperforms zero-shot and approaches fine-tuned accuracy.\n",
    "- Curate exemplar catalogs that reflect manufacturing vocab, units, and edge cases.\n",
    "- Automate example retrieval with embeddings, clustering, and recency filters.\n",
    "- Evaluate response quality and iterate on prompt + exemplar combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f42af34",
   "metadata": {},
   "source": [
    "## üè≠ Manufacturing Use Cases\n",
    "| Workflow | Few-Shot Goal | Example Inputs | Desired Output |\n",
    "| --- | --- | --- | --- |\n",
    "| Maintenance triage | Recommend first fix | Incident text, telemetry snippet | Single actionable step |\n",
    "| Quality deviation | Suggest containment plan | NCR description, lot data | Checklist with owners |\n",
    "| Supplier response | Draft bilingual reply | Email thread excerpt | EN/ES email |\n",
    "| Production reports | Summarize shift log | Raw log paragraphs | 120-word summary with KPIs |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c907a1e7",
   "metadata": {},
   "source": [
    "## üß± Exemplar Design Principles\n",
    "1. Mirror the target output format exactly (tone, length, structure).\n",
    "2. Cover edge cases: multilingual, sensor gaps, safety triggers.\n",
    "3. Include metadata like machine IDs to anchor model understanding.\n",
    "4. Rotate examples periodically to prevent model complacency.\n",
    "5. Keep prompts within context window‚Äîaim for ‚â§ 512 tokens per request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0bbb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "few_shot_model = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"tiiuae/falcon-7b-instruct\",\n",
    "    max_new_tokens=120,\n",
    "    temperature=0.25,\n",
    ")\n",
    "\n",
    "examples_df = pd.DataFrame([\n",
    "    {\"ticket\": \"Vibration spike on compressor 7 after bearing replacement.\", \"action\": \"Inspect alignment and re-torque fasteners.\"},\n",
    "    {\"ticket\": \"Hydraulic leak detected on clamp cylinder.\", \"action\": \"Isolate machine and replace seals.\"},\n",
    "    {\"ticket\": \"Camera misreads due to glare on SMT line.\", \"action\": \"Adjust lighting and recalibrate vision thresholds.\"},\n",
    "    {\"ticket\": \"Packing line robot flags repeated overcurrent alarms.\", \"action\": \"Check payload weight and recalibrate torque limits.\"},\n",
    "])\n",
    "\n",
    "examples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679ba771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(examples, query, persona=\"reliability engineer\"):\n",
    "    header = [\n",
    "        f\"You are a {persona}. Recommend the first corrective action in one concise sentence.\",\n",
    "        \"Respond with the structure: Action: <verb phrase>.\",\n",
    "    ]\n",
    "    for _, ex in examples.iterrows():\n",
    "        header.append(f\"Ticket: {ex.ticket}\\nAction: {ex.action}\")\n",
    "    header.append(f\"Ticket: {query}\\nAction:\")\n",
    "    return \"\\n\".join(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39acdf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ticket = \"AGV slowed near station 5 due to repeated lidar faults.\"\n",
    "few_shot_prompt = build_prompt(examples_df.head(3), query_ticket)\n",
    "print(\"Prompt preview:\\n\", \"\\n\".join(few_shot_prompt.splitlines()[:7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00368e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = few_shot_model(few_shot_prompt)[0][\"generated_text\"]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8172c180",
   "metadata": {},
   "source": [
    "## üîç Exemplar Retrieval Pipeline\n",
    "1. Embed tickets using a lightweight sentence transformer.\n",
    "2. Filter by metadata (e.g., machine family, shift) to avoid irrelevant matches.\n",
    "3. Pick top-k exemplars balancing similarity and diversity.\n",
    "4. Cap total prompt tokens (< 512) and append the query last.\n",
    "5. Log which exemplars were used for traceability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fd9f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "corpus_embeddings = embedder.encode(examples_df.ticket.tolist(), convert_to_tensor=True)\n",
    "query_embedding = embedder.encode(query_ticket, convert_to_tensor=True)\n",
    "\n",
    "similarities = util.cos_sim(query_embedding, corpus_embeddings).squeeze().tolist()\n",
    "retrieval_df = (\n",
    "    examples_df.assign(similarity=[round(s, 3) for s in similarities])\n",
    "    .sort_values(\"similarity\", ascending=False)\n",
    ")\n",
    "retrieval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed480f1e",
   "metadata": {},
   "source": [
    "## üìà Evaluation Dashboard\n",
    "| Metric | Definition | Tooling |\n",
    "| --- | --- | --- |\n",
    "| Recommendation accuracy | % of prompts matching SME-labelled actions | Manual review, eval harness |\n",
    "| Safety compliance | No missing lockout-tagout or PPE steps | Safety checklist automation |\n",
    "| Token cost | Prompt + completion tokens per call | Billing export |\n",
    "| Latency | End-to-end response time | Observability dashboards |\n",
    "| Drift | Similarity drop between query and exemplars | Embedding similarity logs |\n",
    "\n",
    "Iterate by swapping exemplars, editing instructions, or expanding context windows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f3ff5b",
   "metadata": {},
   "source": [
    "## üóÑÔ∏è Exemplar Management\n",
    "- Store examples in a versioned dataset with tags (machine, shift, language).\n",
    "- Capture SME approval status and last refresh date.\n",
    "- Use data quality checks to remove outdated or conflicting exemplars.\n",
    "- Pair with analytics to track coverage gaps across product lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49f2d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking = pd.DataFrame([\n",
    "    {\"prompt\": \"few_shot_v1\", \"k\": 3, \"accuracy\": 0.81, \"safety\": 1.0, \"latency_ms\": 910},\n",
    "    {\"prompt\": \"few_shot_v2\", \"k\": 4, \"accuracy\": 0.87, \"safety\": 1.0, \"latency_ms\": 1040},\n",
    "    {\"prompt\": \"zero_shot_baseline\", \"k\": 0, \"accuracy\": 0.58, \"safety\": 0.85, \"latency_ms\": 650},\n",
    "])\n",
    "tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66692d6",
   "metadata": {},
   "source": [
    "## üõ°Ô∏è Safety & Compliance\n",
    "- Include at least one safety-critical exemplar to bias toward conservative actions.\n",
    "- Ask the model to state assumptions and confidence for traceability.\n",
    "- Log exemplar IDs used per request to support audits.\n",
    "- Restrict prompts to non-sensitive data when using cloud-hosted models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb04ef",
   "metadata": {},
   "source": [
    "## ‚úÖ Checklist\n",
    "- [ ] Exemplars curated, tagged, and SME-approved\n",
    "- [ ] Retrieval pipeline tested for latency and relevance\n",
    "- [ ] Evaluation metrics logged across zero/ few-shot baselines\n",
    "- [ ] Safety review completed for exemplar content\n",
    "- [ ] Deployment playbook documented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557508d5",
   "metadata": {},
   "source": [
    "## üìö References\n",
    "- *In-Context Learning for Industrial NLP* (2024)\n",
    "- Prompt engineering best practices (Week 05)\n",
    "- SentenceTransformers documentation\n",
    "- Reliability engineering checklists (PlantOps 2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418fa741",
   "metadata": {},
   "source": [
    "## üß™ Lab Assignment\n",
    "1. Curate 30 labelled incidents spanning at least three machine families.\n",
    "2. Build an embedding index and implement top-k exemplar retrieval with diversity sampling.\n",
    "3. Compare zero-shot, few-shot (k=3,5), and fine-tuned classifier results on a held-out set.\n",
    "4. Document accuracy, safety, latency, and cost metrics in the tracker.\n",
    "5. Present findings plus recommended deployment strategy to leadership."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
