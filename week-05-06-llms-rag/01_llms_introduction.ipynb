{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2fbac8e",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Notebook 01: An Introduction to Large Language Models (LLMs)\n",
    "\n",
    "**Week 5-6: LLMs, Prompt Engineering & RAG**  \n",
    "**Gen AI Masters Program**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objectives\n",
    "\n",
    "Welcome to the world of Large Language Models! LLMs are the engine behind the generative AI revolution. This notebook provides a comprehensive introduction to what LLMs are, how they are built, and how to start using them effectively.\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "\n",
    "1.  **Explain the LLM Lifecycle:** Understand the key stages of pre-training, instruction tuning, and alignment that create modern LLMs.\n",
    "2.  **Compare Model Types:** Differentiate between major open-source (like Llama, Mixtral) and proprietary (like GPT-4, Claude) model families.\n",
    "3.  **Perform Basic Inference:** Use the Hugging Face `pipeline` to perform tasks like summarization and text generation with powerful pre-trained models.\n",
    "4.  **Develop a Model Selection Strategy:** Learn to choose the right model for a given task based on criteria like performance, cost, latency, and privacy.\n",
    "5.  **Understand Core Concepts:** Grasp key terminology such as \"foundation model,\" \"RLHF,\" and \"context window.\"\n",
    "\n",
    "**Estimated Time:** 2-3 hours\n",
    "\n",
    "---\n",
    "\n",
    "## ü§î What is a Large Language Model?\n",
    "\n",
    "At its core, an LLM is a massive neural network trained on vast quantities of text data. Its fundamental capability is to **predict the next word in a sequence**. While this sounds simple, when scaled up to billions of parameters and trained on terabytes of data, this core function gives rise to emergent abilities like translation, summarization, question-answering, and even reasoning.\n",
    "\n",
    "This notebook will demystify these powerful tools and give you the foundational knowledge to harness their capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae60eb73",
   "metadata": {},
   "source": [
    "## üï∞Ô∏è A Brief History: The Evolution of Language Models\n",
    "\n",
    "The journey to today's powerful LLMs has been one of increasing scale and architectural innovation.\n",
    "\n",
    "| Era         | Representative Models        | Key Breakthrough                                       | Impact on AI                                                    |\n",
    "| :---------- | :--------------------------- | :----------------------------------------------------- | :-------------------------------------------------------------- |\n",
    "| **Pre-2018**  | word2vec, GloVe, FastText    | **Static Word Embeddings:** Words have fixed vector representations. | Enabled basic semantic search and text classification.          |\n",
    "| **2018-2020** | ELMo, BERT, T5, GPT-2        | **The Transformer & Contextual Embeddings:** A word's vector changes based on its context. | Revolutionized NLP, achieving state-of-the-art on most benchmarks. |\n",
    "| **2020-2023** | GPT-3, PaLM, Llama, Claude   | **Massive Scale & Instruction Tuning:** Models with 100B+ parameters trained to follow human instructions. | Gave rise to conversational AI and powerful \"zero-shot\" abilities. |\n",
    "| **2023+**     | GPT-4, Mixtral, Llama 3, Phi-3 | **Multi-modality & Extreme Efficiency:** Models that understand images, are safer, and can run on smaller devices. | Pushing AI towards general-purpose assistants and on-device intelligence. |\n",
    "\n",
    "This rapid progress is built on three pillars: **more data**, **bigger models**, and **better training techniques**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5598a38a",
   "metadata": {},
   "source": [
    "### Key Terminology You Need to Know\n",
    "\n",
    "*   **Foundation Model:** A large-scale model pre-trained on a massive, broad dataset (e.g., the whole internet). It's not specialized for any single task but can be adapted to many different tasks. `GPT-4` and `Llama 3` are foundation models.\n",
    "*   **Instruction Tuning (or Supervised Fine-Tuning, SFT):** The process of further training a pre-trained model on a dataset of high-quality `(prompt, response)` examples. This teaches the model to be more helpful, follow instructions, and engage in conversation.\n",
    "*   **Alignment:** The process of ensuring an LLM's behavior aligns with human values and intentions. This involves making the model more helpful, honest, and harmless.\n",
    "*   **RLHF (Reinforcement Learning from Human Feedback):** A key alignment technique. Humans rank different model responses to the same prompt. A \"reward model\" is trained on these rankings, and then reinforcement learning is used to fine-tune the LLM to generate responses that maximize the reward score.\n",
    "*   **Context Window:** The maximum number of tokens (words and subwords) the model can consider at one time. A larger context window allows the model to process longer documents and maintain longer conversations.\n",
    "*   **Parameters:** The weights and biases of the neural network. The number of parameters is a rough measure of the model's size and capacity (e.g., 7 billion, 70 billion)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de168d69",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è The Anatomy of an LLM Training Pipeline\n",
    "\n",
    "Creating a state-of-the-art LLM is a monumental engineering effort. The process can be broken down into three main stages:\n",
    "\n",
    "1.  **Pre-training:**\n",
    "    *   **Goal:** To learn general knowledge about language, facts, and reasoning.\n",
    "    *   **Data:** A massive, diverse corpus of text and code from the public internet (e.g., Common Crawl, Wikipedia, GitHub).\n",
    "    *   **Process:** The model is trained on a simple, self-supervised objective, typically **next-token prediction**. Given a sequence of words, it learns to predict the most likely next word. This is done for trillions of tokens over thousands of powerful GPUs for weeks or months. The result is a **base foundation model**.\n",
    "\n",
    "2.  **Instruction Tuning (Supervised Fine-Tuning - SFT):**\n",
    "    *   **Goal:** To teach the base model how to follow instructions and be a helpful assistant.\n",
    "    *   **Data:** A smaller, high-quality dataset of curated `(prompt, response)` pairs. These examples are often created by human labelers.\n",
    "    *   **Process:** The base model is fine-tuned on this dataset. It learns the *style* and *format* of being a helpful chatbot.\n",
    "\n",
    "3.  **Alignment (RLHF / DPO):**\n",
    "    *   **Goal:** To make the model safer, more honest, and better aligned with human preferences.\n",
    "    *   **Data:** Human preference data. For a given prompt, humans rank several model-generated responses from best to worst.\n",
    "    *   **Process:** Techniques like **RLHF** (Reinforcement Learning from Human Feedback) or **DPO** (Direct Preference Optimization) are used. A \"reward model\" is trained to predict the human preference score for any given response. Then, the LLM is fine-tuned to generate responses that maximize this reward score.\n",
    "\n",
    "This multi-stage process transforms a raw, next-word predictor into a capable and safe AI assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9767a84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## üöÄ Hands-On: Basic Inference with a Foundation Model\n",
    "\n",
    "Enough theory! Let's use a real LLM. We'll start with a simple task: summarizing a technical report using the Hugging Face `pipeline`. This is a great example of using a foundation model's \"zero-shot\" capabilities‚Äîits ability to perform a task it wasn't explicitly trained for, just by being prompted correctly.\n",
    "\n",
    "We will use `facebook/bart-large-cnn`, a model specifically fine-tuned for summarizing news articles. While not trained on technical logs, its general understanding of language makes it surprisingly effective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edc2c21",
   "metadata": {},
   "source": [
    "## üßÆ Model Sizes, Hardware, and The Trade-Offs\n",
    "\n",
    "LLMs come in a wide range of sizes, measured by their number of **parameters**. This size has a direct impact on their performance, cost, and hardware requirements.\n",
    "\n",
    "| Model Family                               | Parameters | VRAM (16-bit) | Typical Use Case                               |\n",
    "| :----------------------------------------- | :--------- | :------------ | :--------------------------------------------- |\n",
    "| `distilbert-base-uncased`                  | 66 Million | ~0.3 GB       | Fast, simple text classification on the edge.  |\n",
    "| `meta-llama/Meta-Llama-3-8B-Instruct`      | 8 Billion  | ~16 GB        | High-quality chat & instruction-following on a single GPU. |\n",
    "| `mistralai/Mixtral-8x7B-Instruct-v0.1`     | 47 Billion (MoE) | ~94 GB        | Top-tier performance, requires multiple GPUs. Efficient for its size. |\n",
    "| `meta-llama/Meta-Llama-3-70B-Instruct`     | 70 Billion | ~140 GB       | Frontier-level reasoning, requires powerful server hardware. |\n",
    "\n",
    "*VRAM estimates are for inference using 16-bit precision (FP16). Training requires significantly more.*\n",
    "\n",
    "> **Key Insight:** There is no \"best\" model, only the **best model for your specific use case**. A 70B parameter model is overkill for simple sentiment analysis, while a 66M parameter model cannot perform complex reasoning.\n",
    "\n",
    "> **Quantization:** Techniques like 4-bit or 8-bit quantization can dramatically reduce the VRAM footprint (by 2-4x), allowing larger models to run on smaller hardware. However, this often comes with a small trade-off in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52fe78a",
   "metadata": {},
   "source": [
    "### Tokenization: The Language of LLMs\n",
    "\n",
    "LLMs don't see words; they see **tokens**. Tokenization is the process of breaking down raw text into smaller units that the model can understand. These tokens are then mapped to unique integer IDs.\n",
    "\n",
    "**Common Tokenization Strategies:**\n",
    "1.  **Word-Based:** Each word is a token. Fails on typos, variations (\"run\", \"running\"), and rare words.\n",
    "2.  **Character-Based:** Each character is a token. Handles any word but loses semantic meaning and creates very long sequences.\n",
    "3.  **Subword (e.g., BPE, SentencePiece):** The modern standard. It breaks words into semantically meaningful chunks. Common words get their own token (e.g., \"hello\"), while rare words are broken down (e.g., \"tokenization\" -> \"token\" + \"ization\"). This provides a perfect balance between vocabulary size and sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b3a5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hands-On: Exploring Tokenization\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a tokenizer for a specific model\n",
    "# Each model has its own unique tokenizer trained on its specific dataset.\n",
    "# Using the wrong tokenizer for a model will lead to poor performance.\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# Note: You may need to request access to this model on Hugging Face Hub.\n",
    "# If you don't have access, you can use \"distilbert-base-uncased\" as an alternative.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# --- Example 1: A simple sentence ---\n",
    "text = \"Hello, world! This is a test.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "token_ids = tokenizer.encode(text)\n",
    "\n",
    "print(f\"Original Text: '{text}'\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# --- Example 2: Subword tokenization in action ---\n",
    "text_complex = \"Tokenization is foundational to LLMs.\"\n",
    "tokens_complex = tokenizer.tokenize(text_complex)\n",
    "token_ids_complex = tokenizer.encode(text_complex)\n",
    "\n",
    "print(f\"Original Text: '{text_complex}'\")\n",
    "print(f\"Tokens: {tokens_complex}\")\n",
    "print(f\"Token IDs: {token_ids_complex}\")\n",
    "\n",
    "# You can see how \"Tokenization\" is broken into 'Token' and 'ization'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2f6084",
   "metadata": {},
   "source": [
    "## \ude80 Case Study: Building a Downtime Incident Assistant\n",
    "\n",
    "Let's apply these concepts to a practical industrial scenario.\n",
    "\n",
    "**Goal:** Create an intelligent assistant that can help technicians respond to production line incidents more effectively.\n",
    "\n",
    "**Our Plan:**\n",
    "1.  **Step 1: Zero-Shot Classification:** Use a general-purpose LLM to instantly categorize incoming incident reports without any initial training.\n",
    "2.  **Step 2: Suggest First Actions:** Use an instruction-tuned LLM to recommend the immediate next step for a technician based on the report.\n",
    "3.  **Step 3: Evaluate the Suggestion:** Use a question-answering model to validate the LLM's suggestion against a trusted Standard Operating Procedure (SOP).\n",
    "\n",
    "This multi-step approach demonstrates how different models can be chained together to build a robust and reliable system.\n",
    "\n",
    "### Step 1: Zero-Shot Incident Classification\n",
    "\n",
    "When a new incident report comes in, the first task is to route it to the correct department. A **zero-shot classification** model is perfect for this. It can classify text into predefined categories, even if it has never been explicitly trained on those specific labels. This allows for rapid prototyping and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00528124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hands-On: Zero-Shot Classification\n",
    "from transformers import pipeline\n",
    "\n",
    "# The incident report from the factory floor\n",
    "incident_report = 'Vision system flagged misaligned solder joints on PCB lot 2025-A34 during the night shift.'\n",
    "\n",
    "# The categories we want to classify the report into\n",
    "possible_labels = ['safety', 'quality', 'maintenance', 'supply-chain']\n",
    "\n",
    "# Load a pre-trained zero-shot classification pipeline\n",
    "# Model: facebook/bart-large-mnli is a popular choice for this task.\n",
    "classifier = pipeline(\n",
    "    'zero-shot-classification',\n",
    "    model='facebook/bart-large-mnli'\n",
    ")\n",
    "\n",
    "# Run the classification\n",
    "result = classifier(incident_report, candidate_labels=possible_labels, multi_label=False)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Incident Report: '{result['sequence']}'\")\n",
    "print(f\"Predicted Category: {result['labels'][0]}\")\n",
    "print(f\"Confidence Score: {result['scores'][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d489690",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 2: Suggesting a First Action with an Instruction-Tuned Model\n",
    "\n",
    "Now that the incident is categorized, we can use a more powerful **instruction-tuned LLM** to suggest a concrete first action for the technician. These models are specifically trained to follow commands and provide helpful, direct responses.\n",
    "\n",
    "We will formulate a clear prompt that provides the model with the necessary context (the incident report) and asks for a specific output (the recommended first action). This is a form of **prompt engineering**, a critical skill for getting the best results from LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf964f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hands-On: Generating a First Action\n",
    "# Note: Running this cell requires significant GPU memory (~15GB for a 7B model) and may be slow.\n",
    "# It is provided as a demonstration of how to use a large instruction-tuned model.\n",
    "# If you have limited resources, this cell may fail, which is expected.\n",
    "\n",
    "try:\n",
    "    # Load an instruction-tuned model pipeline\n",
    "    # tiiuae/falcon-7b-instruct is a powerful and popular open-source choice.\n",
    "    assistant = pipeline(\n",
    "        'text-generation',\n",
    "        model='tiiuae/falcon-7b-instruct',\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\"  # Automatically uses GPU if available\n",
    "    )\n",
    "\n",
    "    # Engineer a prompt to guide the model\n",
    "    prompt = f\"\"\"\n",
    "    Given the incident report: '{incident_report}'\n",
    "    What is the single most important first action for a technician to take? Be concise and direct.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate the response\n",
    "    result = assistant(\n",
    "        prompt,\n",
    "        max_new_tokens=50,       # Limit the length of the response\n",
    "        do_sample=True,          # Use sampling for more creative responses\n",
    "        temperature=0.7,         # A lower temperature makes the output more deterministic\n",
    "        top_k=50,                # Consider the top 50 most likely tokens at each step\n",
    "        top_p=0.95               # Use nucleus sampling\n",
    "    )\n",
    "    print(result[0]['generated_text'])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not run text generation pipeline, likely due to resource constraints. Error: {e}\")\n",
    "    print(\"Skipping this step. This is expected on most consumer hardware.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6c4bec",
   "metadata": {},
   "source": [
    "### Step 3: Evaluating the Response with a QA Model\n",
    "\n",
    "An LLM's suggestion is powerful, but is it correct? For safety and reliability, we should not blindly trust the model's output, especially in a production environment.\n",
    "\n",
    "One way to validate the response is to check it against a **trusted knowledge source**, like a Standard Operating Procedure (SOP). We can use a **Question-Answering (QA) model** for this task.\n",
    "\n",
    "The process is:\n",
    "1.  Provide the QA model with a `context` (the SOP text).\n",
    "2.  Ask a specific `question` (\"What is the first action?\").\n",
    "3.  The model will find and extract the most likely answer from the provided context.\n",
    "\n",
    "This creates a verifiable, document-grounded check on the generative model's suggestion, forming a basic but effective **RAG (Retrieval-Augmented Generation)** pattern, which we will explore deeply later in this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6038b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hands-On: Validating with a QA Model\n",
    "from transformers import pipeline\n",
    "\n",
    "# The official Standard Operating Procedure for this type of incident\n",
    "sop_context = \"\"\"\n",
    "Standard Operating Procedure for Quality Alerts (QA-SOP-004):\n",
    "1. Upon receiving a quality alert, the first and most critical action is to quarantine the affected batch to prevent it from entering the main production flow or being shipped to customers.\n",
    "2. Immediately notify the shift supervisor and the Quality Assurance (QA) department.\n",
    "3. Document the incident in the Quality Management System (QMS) with all relevant details, including lot number, timestamp, and a description of the defect.\n",
    "4. An assigned engineer will then conduct a root cause analysis (RCA).\n",
    "\"\"\"\n",
    "\n",
    "# The question we want to answer based on the SOP\n",
    "question = \"What is the first action for a quality alert?\"\n",
    "\n",
    "# Load a QA pipeline. 'distilbert-base-cased-distilled-squad' is a lightweight but effective model.\n",
    "qa_pipeline = pipeline('question-answering', model='distilbert-base-cased-distilled-squad')\n",
    "\n",
    "# Find the answer within the context\n",
    "answer = qa_pipeline(question=question, context=sop_context)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer from SOP: '{answer['answer']}'\")\n",
    "print(f\"Confidence Score: {answer['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43b514a",
   "metadata": {},
   "source": [
    "## üõ°Ô∏è Safety, Governance, and Responsible AI\n",
    "\n",
    "Deploying LLMs, especially in industrial settings, carries significant responsibility. A mistake can impact physical processes, safety, and product quality. Building a robust AI system requires a strong governance framework.\n",
    "\n",
    "**Key Pillars of Responsible AI:**\n",
    "\n",
    "*   **Guardrails:** Implement automated checks on both the inputs to the model and the outputs it generates.\n",
    "    *   *Input Filtering:* Block prompts that are off-topic, malicious, or seek to exploit the model.\n",
    "    *   *Output Filtering:* Prevent the model from generating harmful, toxic, or nonsensical content. For example, if an assistant suggests \"increase the boiler pressure to maximum,\" a guardrail should flag this as a potentially dangerous and invalid action.\n",
    "\n",
    "*   **Hallucination Mitigation:** LLMs can sometimes \"hallucinate\" or invent facts. The most effective way to combat this is with **Retrieval-Augmented Generation (RAG)**, which we will cover in detail later. RAG grounds the model's responses in a specific set of trusted documents, forcing it to base its answers on verifiable facts rather than its internal knowledge.\n",
    "\n",
    "*   **Bias Audits:** Models can inherit biases from their training data. It's crucial to audit your system for potential biases related to different factory shifts, employee roles, or equipment types to ensure fairness and equity.\n",
    "\n",
    "*   **Human-in-the-Loop (HITL):** For any high-stakes decision, an LLM should assist, not decide. A human expert must always have the final say. For example, an LLM can *suggest* shutting down a machine, but the action should require explicit approval from a qualified operator.\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Further Reading & Resources\n",
    "\n",
    "This notebook provides a high-level introduction. To deepen your understanding, we highly recommend exploring the original papers and technical reports for the models discussed.\n",
    "\n",
    "*   **Foundational Paper:** [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762) (Vaswani et al., 2017) - The paper that introduced the Transformer architecture, the bedrock of modern LLMs.\n",
    "*   **Safety & Ethics:** [\"Building Safe LLM Systems\"](https://www.anthropic.com/news/building-safe-llm-systems) (Anthropic, 2024) - A comprehensive guide to practical safety measures in LLM development.\n",
    "*   **Model-Specific Details:**\n",
    "    *   [Llama 3 Technical Report](https://ai.meta.com/blog/meta-llama-3/)\n",
    "    *   [Mixtral of Experts Technical Report](https://arxiv.org/abs/2401.04088)\n",
    "    *   [Phi-3 Technical Report](https://arxiv.org/abs/2404.14219)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
