{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2fbac8e",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Week 5-6 ¬∑ Notebook 01 ¬∑ Introduction to Large Language Models\n",
    "\n",
    "**Module:** LLMs, Prompt Engineering & RAG  \n",
    "**Estimated time:** 4.5 hours  \n",
    "**Prerequisites:** Transformers (Week 3-4) + Attention + HuggingFace basics\n",
    "\n",
    "---\n",
    "\n",
    "Manufacturing leaders need copilots that speak the language of maintenance logs, quality checklists, and supplier bulletins. This notebook lays the foundation for working with Large Language Models (LLMs) in industrial settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6272730c",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "By the end of this notebook you will be able to:\n",
    "1. Explain how modern LLMs are pre-trained, aligned, and deployed.\n",
    "2. Compare open-source and proprietary model families for plant-floor tasks.\n",
    "3. Run baseline inference on domain text using HuggingFace pipelines.\n",
    "4. Build a decision matrix to choose the right model for latency, privacy, and safety constraints.\n",
    "5. Design evaluation loops that incorporate manufacturing-specific metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b979538",
   "metadata": {},
   "source": [
    "## üß≠ Roadmap\n",
    "1. LLM evolution timeline and terminology\n",
    "2. Anatomy of foundation model training\n",
    "3. Industrial deployment considerations\n",
    "4. Hands-on inference walkthroughs\n",
    "5. Case study: Downtime incident assistant\n",
    "6. Evaluation, safety, and governance checklists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae60eb73",
   "metadata": {},
   "source": [
    "## üï∞Ô∏è Evolution of Language Models\n",
    "| Era | Representative Models | Breakthrough | Manufacturing Impact |\n",
    "| --- | --- | --- | --- |\n",
    "| 2013-2017 | word2vec, GloVe, ELMo | Contextual embeddings | Keyword search in maintenance manuals |\n",
    "| 2018-2020 | GPT, BERT, T5 | Transformer encoder/decoder scale | Automated report summaries |\n",
    "| 2021-2023 | GPT-3, PaLM, LLaMA | 100B+ parameters + instruction tuning | Conversational plant copilots |\n",
    "| 2024+ | Mixtral, Claude, Llama-3 | Safety-aligned, multi-modal | Real-time troubleshooting across modalities |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5598a38a",
   "metadata": {},
   "source": [
    "### Key Definitions\n",
    "- **Foundation model:** large model trained on broad corpus, adaptable to downstream tasks.\n",
    "- **Instruction tuning:** supervised fine-tuning on prompt/response pairs to follow instructions.\n",
    "- **RLHF (Reinforcement Learning from Human Feedback):** optimize responses for helpfulness and safety.\n",
    "- **Alignment:** ensuring outputs respect policies (quality, safety, compliance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d64bf6",
   "metadata": {},
   "source": [
    "## üè≠ Manufacturing Perspective\n",
    "- **Maintenance analytics:** interpret vibration logs, create structured work orders.\n",
    "- **Quality control:** summarize defect tickets, recommend countermeasures.\n",
    "- **Supply chain:** draft vendor communications or translate manuals.\n",
    "- **Safety:** generate checklists compliant with OSHA/ISO standards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de168d69",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Anatomy of an LLM Training Pipeline\n",
    "1. **Data curation:** mixture of public text + domain corpora (SOPs, logs).\n",
    "2. **Tokenization:** SentencePiece/BPE with <> tokens for units like `¬∞C`, `Nm`.\n",
    "3. **Pre-training:** unsupervised objectives (next-token, span corruption).\n",
    "4. **Supervised fine-tuning:** align to domain tasks (incident classification).\n",
    "5. **RLHF / DPO:** incorporate human feedback, safety rules, risk controls.\n",
    "6. **Evaluation:** perplexity, domain accuracy, hallucination tests.\n",
    "7. **Deployment:** on-prem GPU, managed endpoints, or edge devices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab53790",
   "metadata": {},
   "source": [
    "### Data Stack Considerations\n",
    "- Include multilingual logs (e.g., supplier emails in German/Japanese).\n",
    "- Govern PII and trade secrets with masking / redaction.\n",
    "- Track dataset drift across shifts, product variants, seasons.\n",
    "- Maintain data cards describing lineage and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9767a84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "downtime_report = (\n",
    "    \"Press-42 tripped due to hydraulic accumulator pressure drop. Operators rerouted flow to backup line. \"\n",
    "    \"Recommend inspection of seals and replenish fluid before restart.\"\n",
    ")\n",
    "summarizer = pipeline('summarization', model='facebook/bart-large-cnn')\n",
    "summary = summarizer(downtime_report, max_length=60, min_length=25, do_sample=False)[0]['summary_text']\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e916ef",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "- What context is missing from the summary?\n",
    "- Which stakeholders (maintenance planner, plant manager) benefit from condensed reports?\n",
    "- When would you prefer raw logs over summaries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e93a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('tiiuae/falcon-7b-instruct')\n",
    "text = 'Perform a predictive maintenance check on furnace line A before cycle 540.'\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens[:12], len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edc2c21",
   "metadata": {},
   "source": [
    "## üßÆ Parameter Counts & Hardware Sizing\n",
    "| Model | Parameters | VRAM (fp16) | Typical Use Case |\n",
    "| --- | --- | --- | --- |\n",
    "| `distilbert-base-uncased` | 66M | 1.2 GB | Edge classification |\n",
    "| `falcon-7b-instruct` | 7B | 14 GB | On-prem copilots |\n",
    "| `mistralai/Mixtral-8x7B` | 46.7B (MoE) | 80 GB | Research assistants |\n",
    "| `meta-llama/Meta-Llama-3-70B-Instruct` | 70B | 140 GB | High-accuracy digital workers |\n",
    "\n",
    "> **Tip:** Quantization (4-bit, 8-bit) reduces VRAM footprint but may impact accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52fe78a",
   "metadata": {},
   "source": [
    "## üìù Model Selection Framework\n",
    "1. **Business objective:** Explain downtime vs. automate SOP authoring.\n",
    "2. **Latency:** Real-time under 500 ms vs. offline batch.\n",
    "3. **Context window:** Do you need 8k or 200k tokens for long shift logs?\n",
    "4. **Privacy:** Can data leave the plant? If not, prefer open-source on-prem.\n",
    "5. **Cost:** GPU availability, inference pricing, licensing terms.\n",
    "6. **Safety:** Guardrails for high-risk recommendations (e.g., lock-out/tag-out)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b3a5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import model_info\n",
    "\n",
    "candidates = ['distilbert-base-uncased', 'hf-allenai/longformer-base-4096', 'tiiuae/falcon-7b-instruct']\n",
    "decision_table = []\n",
    "for name in candidates:\n",
    "    info = model_info(name)\n",
    "    decision_table.append({\n",
    "        'model': name,\n",
    "        'params': info.cardData.get('params', 'n/a'),\n",
    "        'context_length': info.cardData.get('context_length', 'n/a'),\n",
    "        'pipeline_tag': info.pipeline_tag\n",
    "    })\n",
    "decision_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214b8cdd",
   "metadata": {},
   "source": [
    "### Exercise: Model Matrix\n",
    "- Add columns for latency (edge/cloud), licensing (Apache, Llama 2), and support level.\n",
    "- Rate each model 1-5 against your plant's needs.\n",
    "- Present the matrix to stakeholders to justify model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2f6084",
   "metadata": {},
   "source": [
    "## üîÑ Case Study ¬∑ Downtime Incident Assistant\n",
    "**Scenario:** create a helper that categorizes incidents and suggests first actions.\n",
    "\n",
    "### Step 1 ¬∑ Zero-shot classification\n",
    "Use a generalist LLM to label ticket categories before training bespoke models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00528124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "incident = 'Vision system flagged misaligned solder joints on PCB lot 2025-A34 during night shift.'\n",
    "labels = ['safety', 'quality', 'maintenance', 'supply-chain']\n",
    "classifier = pipeline('zero-shot-classification', model='facebook/bart-large-mnli')\n",
    "result = classifier(incident, candidate_labels=labels, multi_label=False)\n",
    "result\n",
    "<VSCode.Cell id=\"#VSC-8fbf6d95\" language=\"markdown\">\n",
    "### Step 2 ¬∑ Suggest First Actions\n",
    "Now, let's use a text generation model to suggest a course of action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9577f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Running this cell requires significant GPU memory (~15GB) and may be slow.\n",
    "# It is provided as a demonstration of how to use a large instruction-tuned model.\n",
    "try:\n",
    "    assistant = pipeline('text-generation', model='tiiuae/falcon-7b-instruct', trust_remote_code=True, device_map=\"auto\")\n",
    "    prompt = f\"\"\"\n",
    "    Given the incident report: '{incident}'\n",
    "    What is the recommended first action for a technician? Be concise.\n",
    "    \"\"\"\n",
    "    result = assistant(prompt, max_new_tokens=50, do_sample=True, temperature=0.7)\n",
    "    print(result[0]['generated_text'])\n",
    "except Exception as e:\n",
    "    print(f\"Could not run text generation pipeline, likely due to resource constraints. Error: {e}\")\n",
    "    print(\"Skipping this step. This is expected on most consumer hardware.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6c4bec",
   "metadata": {},
   "source": [
    "### Step 3 ¬∑ Evaluate Response\n",
    "How can we validate the LLM's suggestion? We can use another model for question-answering against a known-good Standard Operating Procedure (SOP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6038b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "sop_context = \"\"\"\n",
    "Standard Operating Procedure for Quality Alerts (QA-SOP-004):\n",
    "1. Upon receiving a quality alert, the first action is to quarantine the affected batch to prevent further use.\n",
    "2. Notify the shift supervisor and the Quality Assurance department immediately.\n",
    "3. Document the incident in the Quality Management System (QMS) with all relevant details.\n",
    "4. An assigned engineer will then conduct a root cause analysis.\n",
    "\"\"\"\n",
    "\n",
    "qa_pipeline = pipeline('question-answering', model='distilbert-base-cased-distilled-squad')\n",
    "question = \"What is the first action for a quality alert?\"\n",
    "answer = qa_pipeline(question=question, context=sop_context)\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43b514a",
   "metadata": {},
   "source": [
    "## üõ°Ô∏è Safety, Governance, and Responsible AI\n",
    "- **Guardrails:** Implement input/output filters to prevent harmful or off-topic responses.\n",
    "- **Hallucination Mitigation:** Use RAG (Week 5-6) to ground responses in factual documents.\n",
    "- **Bias Audits:** Test for biases related to shifts, roles, or demographics.\n",
    "- **Human-in-the-Loop:** For high-stakes decisions (e.g., machine shutdown), require human approval.\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Further Reading\n",
    "- \"Attention Is All You Need\" (Vaswani et al., 2017)\n",
    "- \"Building Safe LLM Systems\" (Anthropic, 2024)\n",
    "- Llama 3, Mixtral, and Phi-3 Technical Reports"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
