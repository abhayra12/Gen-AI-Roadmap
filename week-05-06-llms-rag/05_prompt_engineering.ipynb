{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3725990",
   "metadata": {},
   "source": [
    "# 🎛️ Week 5-6, Notebook 5: The Art and Science of Prompt Engineering\n",
    "\n",
    "**Module:** LLMs, Prompt Engineering & RAG  \n",
    "**Project:** Build the Knowledge Core for the Manufacturing Copilot\n",
    "\n",
    "---\n",
    "\n",
    "If a Large Language Model is a powerful engine, then a **prompt** is the steering wheel, accelerator, and brake. **Prompt Engineering** is the art and science of designing effective inputs to guide an LLM toward a desired output. It is one of the most critical skills for building reliable, effective, and safe AI applications.\n",
    "\n",
    "For our Manufacturing Copilot, good prompt engineering is the difference between a helpful assistant and a confusing, unreliable tool. In this notebook, you will learn the fundamental patterns and best practices for crafting prompts that produce accurate, safe, and factually grounded responses for maintenance engineers, operators, and plant managers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d09c126",
   "metadata": {},
   "source": [
    "## 🎯 Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1.  **Master Core Prompting Patterns:** Implement and explain the use cases for Zero-Shot, Few-Shot, and Chain-of-Thought (CoT) prompting.\n",
    "2.  **Enforce Structured Outputs:** Craft prompts that constrain the LLM's output, forcing it to generate specific formats like JSON or follow a predefined schema.\n",
    "3.  **Assign Roles and Personas:** Use role-based prompts to imbue the model with a specific expertise or personality, improving the quality and relevance of its responses.\n",
    "4.  **Build a Reusable Prompt Library:** Develop a collection of version-controlled prompt templates for common manufacturing tasks, promoting consistency and maintainability.\n",
    "5.  **Evaluate Prompts Systematically:** Design and implement a basic evaluation loop to score, compare, and iterate on different prompt variations to find the most effective one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4632e888",
   "metadata": {},
   "source": [
    "## ⚙️ Setup: Choosing a Model for Prompt Engineering\n",
    "\n",
    "While massive, proprietary models like GPT-4 are incredibly powerful, smaller, open-source instruction-tuned models are often more than sufficient for many tasks and offer significant advantages in terms of speed, cost, and privacy.\n",
    "\n",
    "For this notebook, we will use `google/flan-t5-base`. This is a versatile and widely-used **sequence-to-sequence** model that is particularly good at following instructions and responding to prompts. It is small enough to run on a standard CPU or a modest GPU, making it perfect for experimentation and prototyping.\n",
    "\n",
    "**A Note on Temperature:** We will set the `temperature` parameter to a low value (e.g., `0.1`). Temperature controls the randomness of the model's output.\n",
    "*   **High Temperature (e.g., > 0.7):** More creative, random, and diverse outputs. Good for brainstorming or creative writing.\n",
    "*   **Low Temperature (e.g., < 0.3):** More deterministic, focused, and predictable outputs. Good for factual, instruction-following tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f47b7a",
   "metadata": {},
   "source": [
    "# Hands-On: Setting up the Generation Pipeline\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Set device to GPU if available, otherwise CPU\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "# T5 is a sequence-to-sequence model, so we use AutoModelForSeq2SeqLM\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "    # Create the text-generation pipeline\n",
    "    generator = pipeline(\n",
    "        'text2text-generation', # Use 'text2text-generation' for T5 models\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "        max_length=256,\n",
    "        temperature=0.1, # Low temperature for more factual and deterministic outputs\n",
    "    )\n",
    "\n",
    "    print(f\"Pipeline created successfully for model '{model_name}' on device {device}.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to create pipeline. This can happen if the model is not available or if there's a network issue.\")\n",
    "    print(f\"Error: {e}\")\n",
    "    # Assign a dummy generator to allow the rest of the notebook to run without errors.\n",
    "    generator = lambda x: [{\"generated_text\": \"Error: Pipeline not initialized.\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8babcc4e",
   "metadata": {},
   "source": [
    "## 🧪 Pattern 1: Zero-Shot Prompting (The Direct Instruction)\n",
    "\n",
    "**Zero-shot prompting** is the simplest and most common technique. You simply ask the model to perform a task directly, without providing any prior examples. This relies on the model's vast pre-training knowledge to understand and execute the instruction.\n",
    "\n",
    "**When to Use:**\n",
    "*   For simple, straightforward tasks like summarization, translation, or answering general questions.\n",
    "*   As a baseline to see how well a model performs before moving to more complex techniques.\n",
    "\n",
    "**Scenario:** A maintenance ticket comes in. We want the LLM to quickly summarize it for a busy shift supervisor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a9c00c",
   "metadata": {},
   "source": [
    "# Hands-On: Zero-Shot Summarization\n",
    "# The raw text from an incident report\n",
    "incident_report = \"\"\"\n",
    "During the night shift, the primary coolant pump for CNC-12 (Asset ID: PMP-045) failed, causing a line stoppage.\n",
    "Telemetry data shows a sudden pressure drop from 60 PSI to 5 PSI over a 30-second period before the automated system shutdown.\n",
    "The backup pump was engaged manually by the operator, and production resumed after a 15-minute delay.\n",
    "\"\"\"\n",
    "\n",
    "# A simple, direct prompt that clearly states the task and the input.\n",
    "zero_shot_prompt = f\"\"\"\n",
    "Summarize the following incident report in one single, concise sentence:\n",
    "\n",
    "{incident_report}\n",
    "\"\"\"\n",
    "\n",
    "summary = generator(zero_shot_prompt)\n",
    "\n",
    "print(\"--- Zero-Shot Summary ---\")\n",
    "print(summary[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5d6ac8",
   "metadata": {},
   "source": [
    "## 🧪 Pattern 2: Few-Shot Prompting (Learning by Example)\n",
    "\n",
    "**Few-shot prompting** is used when the task is more complex or requires the output to be in a very specific format. By providing a few high-quality examples (the \"shots\"), you give the model a template to follow. This is a form of **in-context learning**.\n",
    "\n",
    "**When to Use:**\n",
    "*   When zero-shot prompting fails or produces inconsistent results.\n",
    "*   For structured data extraction (e.g., pulling specific fields from a text).\n",
    "*   To guide the model on how to handle domain-specific jargon.\n",
    "\n",
    "**Scenario:** We need to extract structured data (Asset, Component, Failure Mode) from unstructured maintenance logs. A zero-shot prompt might struggle with this, but with a couple of examples, the model can learn the desired pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78609cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hands-On: Few-Shot Structured Data Extraction\n",
    "few_shot_prompt = f\"\"\"\n",
    "Extract the Asset, Component, and Failure Mode from the log.\n",
    "\n",
    "Log: \"The vision system on Line-5 is failing to detect part #A55-Z. It seems like a camera calibration issue.\"\n",
    "Asset: Line-5\n",
    "Component: Vision System Camera\n",
    "Failure Mode: Miscalibration\n",
    "===\n",
    "Log: \"Press-2 is showing hydraulic pressure fluctuations. The main pump seal is likely worn out and needs replacement.\"\n",
    "Asset: Press-2\n",
    "Component: Hydraulic Pump Seal\n",
    "Failure Mode: Wear and Tear\n",
    "===\n",
    "Log: \"{incident_report}\"\n",
    "Asset:\"\"\"\n",
    "# Note: We end the prompt with \"Asset:\" to guide the model to start generating the answer in the correct format.\n",
    "\n",
    "structured_data = generator(few_shot_prompt)\n",
    "\n",
    "print(\"--- Few-Shot Structured Data Extraction ---\")\n",
    "# The model will continue the pattern. We prepend \"Asset:\" to the output for a clean result.\n",
    "print(\"Asset: \" + structured_data[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6fcf47",
   "metadata": {},
   "source": [
    "## 🧪 Pattern 3: Chain-of-Thought (CoT) and Role Prompting\n",
    "\n",
    "**Chain-of-Thought (CoT) prompting** is a more advanced technique used for complex reasoning tasks. By explicitly instructing the model to \"think step-by-step,\" you encourage it to break down the problem into smaller, more manageable parts. This often leads to more accurate and logical conclusions.\n",
    "\n",
    "Combining CoT with **role prompting**—where you assign the model a persona or expertise (e.g., \"You are a senior reliability engineer\")—can make it even more powerful.\n",
    "\n",
    "**When to Use:**\n",
    "*   For multi-step reasoning, root cause analysis, or problem-solving.\n",
    "*   When you need the model to explain its reasoning process.\n",
    "*   To imbue the model with the knowledge and perspective of a specific expert.\n",
    "\n",
    "**Scenario:** A complex failure has occurred. We need a preliminary root cause analysis and a recommended action. We will assign the model the role of a \"senior reliability engineer\" and ask it to follow a specific analytical process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884067b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hands-On: Chain-of-Thought Root Cause Analysis\n",
    "cot_prompt = f\"\"\"\n",
    "You are a senior reliability engineer with 20 years of experience in industrial maintenance.\n",
    "Analyze the following incident report by thinking step-by-step.\n",
    "\n",
    "Report: \"{incident_report}\"\n",
    "\n",
    "Follow these steps:\n",
    "1.  **Sequence of Events:** Detail the events in chronological order.\n",
    "2.  **Symptoms:** State the primary and secondary symptoms observed.\n",
    "3.  **Root Cause Hypothesis:** Propose the most likely root cause of the failure.\n",
    "4.  **Recommended Action:** Recommend the single most important immediate action for the maintenance team.\n",
    "\n",
    "Begin your analysis:\n",
    "\"\"\"\n",
    "\n",
    "analysis = generator(cot_prompt)\n",
    "\n",
    "print(\"--- Chain-of-Thought Analysis from 'Senior Reliability Engineer' ---\")\n",
    "print(analysis[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e04a31",
   "metadata": {},
   "source": [
    "## 🗂️ Best Practice: Building a Reusable Prompt Template Library\n",
    "\n",
    "Hardcoding prompts directly into your application code is inefficient, hard to maintain, and makes it difficult to track changes. A much better approach is to create a centralized library of **prompt templates**.\n",
    "\n",
    "This approach has several advantages:\n",
    "*   **Consistency:** Ensures that the same task always uses the same approved prompt.\n",
    "*   **Maintainability:** If you need to update a prompt, you only have to change it in one place.\n",
    "*   **Versioning:** You can version your prompts just like you version your code, allowing you to track experiments and roll back to previous versions if needed.\n",
    "*   **Clarity:** Separates the logic of your application from the specifics of the prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a44e41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hands-On: Creating and Using a Prompt Template Library\n",
    "# In a real application, this might be a separate .py or .json file.\n",
    "PROMPT_TEMPLATES = {\n",
    "    \"v1.summarize_incident\": \"Summarize this incident report for a shift supervisor in 3 brief bullet points:\\n\\n{report}\",\n",
    "\n",
    "    \"v1.extract_entities_json\": \"\"\"\n",
    "You are a data extraction bot. From the following text, extract the asset ID, component, and a brief description of the failure.\n",
    "Respond ONLY with a valid JSON object with the keys 'asset_id', 'component', and 'failure_description'.\n",
    "\n",
    "Text: \"{report}\"\n",
    "\n",
    "JSON:\n",
    "\"\"\",\n",
    "\n",
    "    \"v1.draft_safety_alert\": \"\"\"\n",
    "You are an EHS (Environment, Health, and Safety) officer. Based on the incident below, draft a 2-sentence safety alert to be posted on the factory floor.\n",
    "The tone should be urgent, clear, and direct.\n",
    "\n",
    "Incident: \"{report}\"\n",
    "\n",
    "Alert:\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# --- Example of using a template to enforce JSON output ---\n",
    "# This is a powerful technique for getting structured data from an LLM.\n",
    "json_prompt = PROMPT_TEMPLATES['v1.extract_entities_json'].format(report=incident_report)\n",
    "json_output = generator(json_prompt)\n",
    "\n",
    "print(\"--- Prompt Template for JSON Output ---\")\n",
    "print(\"Prompt Sent to Model:\\n\", json_prompt)\n",
    "print(\"\\nGenerated JSON String:\\n\", json_output[0]['generated_text'])\n",
    "\n",
    "# You can then parse this string into a Python dictionary\n",
    "import json\n",
    "try:\n",
    "    parsed_json = json.loads(json_output[0]['generated_text'])\n",
    "    print(\"\\nSuccessfully parsed JSON:\")\n",
    "    print(parsed_json)\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"\\nFailed to parse JSON: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc8e342",
   "metadata": {},
   "source": [
    "## 📊 Best Practice: Evaluating Prompts Systematically\n",
    "\n",
    "How do you know if `prompt_v2` is actually better than `prompt_v1`? You must **test and measure**. A systematic evaluation framework is crucial for developing high-quality, reliable prompts. You should never assume a prompt is good just because it \"looks right\" on a few examples.\n",
    "\n",
    "**A Simple Evaluation Framework:**\n",
    "\n",
    "1.  **Define Your Criteria:** What does \"better\" mean for your use case?\n",
    "2.  **Create a \"Golden Dataset\":** A small, high-quality dataset of inputs and their corresponding ideal outputs.\n",
    "3.  **Run Experiments:** Run different prompt variations against your golden dataset.\n",
    "4.  **Score the Results:** Score each output against the ideal output based on your criteria.\n",
    "5.  **Log Everything:** Keep a log of your experiments to track progress and justify your choices.\n",
    "\n",
    "| Evaluation Criterion | How to Measure | Example |\n",
    "| :--- | :--- | :--- |\n",
    "| **Accuracy / Correctness** | Compare model output to a \"golden\" answer. Can be automated (e.g., `exact match`, `F1 score`) or done by a human reviewer. | Does the `extract_entities_json` prompt correctly identify the asset ID 95% of the time across 100 test cases? |\n",
    "| **Safety & Compliance** | Use a checklist, a keyword scanner, or even another LLM to flag any outputs that are unsafe, non-compliant, or toxic. | Does the `draft_safety_alert` prompt ever suggest an action that violates company safety policy? |\n",
    "| **Tone & Style** | Use semantic similarity to compare the output's embedding to a style guide example, or rely on human review. | Is the tone of the safety alert appropriately urgent and professional? |\n",
    "| **Latency & Cost** | Measure the average response time (latency) and the number of input/output tokens (cost). | Does the complex Chain-of-Thought prompt take too long to be useful in a real-time user interface? |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b316dd",
   "metadata": {},
   "source": [
    "# Hands-On: A Simple Prompt Evaluation Log\n",
    "import pandas as pd\n",
    "\n",
    "# Example of an experiment tracking log you might maintain in a spreadsheet or database.\n",
    "# This log tracks the performance of different versions of your prompts.\n",
    "prompt_experiments_log = pd.DataFrame([\n",
    "    {\n",
    "        \"prompt_name\": \"summarize_v1_zero_shot\",\n",
    "        \"accuracy\": 0.75,  # % of summaries deemed 'good' by a human reviewer\n",
    "        \"safety_flags\": 0,\n",
    "        \"avg_latency_ms\": 450,\n",
    "        \"notes\": \"Works okay, but sometimes too verbose.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt_name\": \"summarize_v2_with_role\",\n",
    "        \"accuracy\": 0.88,\n",
    "        \"safety_flags\": 0,\n",
    "        \"avg_latency_ms\": 510,\n",
    "        \"notes\": \"Better tone and more concise. Adopted as new standard.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt_name\": \"extract_json_v1_few_shot\",\n",
    "        \"accuracy\": 0.96,  # % of fields correctly extracted across the test set\n",
    "        \"safety_flags\": 0,\n",
    "        \"avg_latency_ms\": 820,\n",
    "        \"notes\": \"Very accurate but slightly slow. Acceptable for offline processing.\"\n",
    "    },\n",
    "])\n",
    "\n",
    "print(\"--- Prompt Evaluation Log ---\")\n",
    "prompt_experiments_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c693da1",
   "metadata": {},
   "source": [
    "## ✅ Summary and Next Steps\n",
    "\n",
    "You now have a powerful and systematic toolkit for guiding and controlling Large Language Models. The key takeaway is to be methodical and data-driven in your approach to prompt design.\n",
    "\n",
    "**Your Prompt Engineering Workflow:**\n",
    "\n",
    "1.  **Start Simple:** Always begin with a clear, direct zero-shot prompt. It's often surprisingly effective.\n",
    "2.  **Iterate and Add Complexity:** If the simple prompt fails, incrementally add more sophisticated techniques. Provide examples (few-shot), assign an expert role, or ask the model to think step-by-step (Chain-of-Thought).\n",
    "3.  **Enforce Structure:** Don't leave the output format to chance. Explicitly request the structure you need, such as JSON, bullet points, or a specific number of sentences.\n",
    "4.  **Test, Measure, and Log Everything:** Never assume a prompt is good. Evaluate it against your predefined criteria using a golden dataset and log the results to track your progress.\n",
    "\n",
    "In the next notebooks, we will combine these advanced prompting techniques with the concept of **Retrieval-Augmented Generation (RAG)**. This will allow us to build a powerful and trustworthy system that can answer questions based on your own private documents and knowledge bases, finally unlocking the full potential of the Manufacturing Copilot."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
