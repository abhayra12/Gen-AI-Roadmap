{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3725990",
   "metadata": {},
   "source": [
    "# üéõÔ∏è Week 5-6 ¬∑ Notebook 05 ¬∑ Prompt Engineering for Manufacturing\n",
    "\n",
    "**Module:** LLMs, Prompt Engineering & RAG  \n",
    "**Project:** Build the Knowledge Core for the Manufacturing Copilot\n",
    "\n",
    "---\n",
    "\n",
    "Prompt Engineering is the art and science of designing effective inputs to guide a Large Language Model (LLM) toward a desired output. For our Manufacturing Copilot, this means crafting prompts that produce reliable, safe, and factually grounded responses for maintenance engineers, operators, and plant managers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d09c126",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. ‚úÖ **Master Prompt Patterns:** Implement Zero-Shot, Few-Shot, and Chain-of-Thought prompts.\n",
    "2. ‚úÖ **Enforce Structure:** Use roles, constraints, and output schemas (like JSON) to control LLM behavior.\n",
    "3. ‚úÖ **Build a Prompt Library:** Create reusable prompt templates for common manufacturing tasks.\n",
    "4. ‚úÖ **Evaluate Prompts Systematically:** Design an evaluation loop to score and compare prompt variations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4632e888",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Setup: Choosing a Runnable Model\n",
    "\n",
    "While massive models like GPT-4 are powerful, smaller, instruction-tuned models are often sufficient and much faster for specific tasks. We'll use `google/flan-t5-base`, a versatile model that can run on a CPU or a modest GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f47b7a",
   "metadata": {},
   "source": [
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "# Using AutoModelForSeq2SeqLM for T5-style models\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "generator = pipeline(\n",
    "    'text2text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    max_length=200,\n",
    "    temperature=0.1, # Lower temperature for more deterministic, factual outputs\n",
    ")\n",
    "\n",
    "print(f\"Pipeline created for {model_name} on device {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8babcc4e",
   "metadata": {},
   "source": [
    "## üß™ Prompt Pattern 1: Zero-Shot Prompting\n",
    "\n",
    "**When to use:** For simple, direct tasks where the model is expected to understand the instruction without any examples.\n",
    "\n",
    "**Scenario:** A maintenance ticket comes in. We want the LLM to quickly summarize it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a9c00c",
   "metadata": {},
   "source": [
    "incident_report = \"During the night shift, the primary coolant pump for CNC-12 failed, causing a line stoppage. Telemetry shows a pressure drop from 60 PSI to 5 PSI over 30 seconds before the automated shutdown. The backup pump was engaged manually and production resumed after a 15-minute delay.\"\n",
    "\n",
    "# A simple, direct prompt\n",
    "zero_shot_prompt = f\"Summarize the following incident report in one sentence: \n",
    "\n",
    "{incident_report}\"\n",
    "\n",
    "summary = generator(zero_shot_prompt)\n",
    "print(\"--- Zero-Shot Summary ---\")\n",
    "print(summary[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5d6ac8",
   "metadata": {},
   "source": [
    "## üß™ Prompt Pattern 2: Few-Shot Prompting\n",
    "\n",
    "**When to use:** When you need the model to follow a specific format or handle domain-specific jargon. You provide a few examples (`shots`) to guide it.\n",
    "\n",
    "**Scenario:** We need to extract structured data (Asset, Component, Failure Mode) from maintenance logs. A zero-shot prompt might fail, but with examples, the model learns the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78609cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = f\"\"\"Extract the Asset, Component, and Failure Mode from the log.\n",
    "\n",
    "Log: \"The vision system on Line-5 is failing to detect part #A55-Z. Seems like a camera calibration issue.\"\n",
    "Asset: Line-5\n",
    "Component: Vision System Camera\n",
    "Failure Mode: Miscalibration\n",
    "===\n",
    "Log: \"Press-2 is showing hydraulic pressure fluctuations. The main pump seal is likely worn out.\"\n",
    "Asset: Press-2\n",
    "Component: Hydraulic Pump Seal\n",
    "Failure Mode: Wear and Tear\n",
    "===\n",
    "Log: \"{incident_report}\"\n",
    "Asset:\"\"\"\n",
    "# Note: We end the prompt here to guide the model to start generating the answer\n",
    "\n",
    "structured_data = generator(few_shot_prompt)\n",
    "print(\"--- Few-Shot Structured Data Extraction ---\")\n",
    "print(\"Asset: \" + structured_data[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6fcf47",
   "metadata": {},
   "source": [
    "## üß™ Prompt Pattern 3: Chain-of-Thought (CoT) & Roles\n",
    "\n",
    "**When to use:** For complex reasoning tasks. By asking the model to \"think step-by-step,\" we force it to break down the problem, leading to more accurate results. Combining this with a **role** makes it even more powerful.\n",
    "\n",
    "**Scenario:** A complex failure occurred. We need a root cause analysis and a recommended action. We'll assign the model the role of a \"senior reliability engineer\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884067b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_prompt = f\"\"\"You are a senior reliability engineer. Analyze the following incident report by thinking step-by-step. First, identify the sequence of events. Second, state the primary and secondary symptoms. Third, propose the most likely root cause. Finally, recommend an immediate action.\n",
    "\n",
    "Report: \"{incident_report}\"\n",
    "\n",
    "Analysis:\"\"\"\n",
    "\n",
    "analysis = generator(cot_prompt)\n",
    "print(\"--- Chain-of-Thought Analysis ---\")\n",
    "print(analysis[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e04a31",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è Building a Reusable Prompt Template Library\n",
    "\n",
    "Hardcoding prompts is inefficient. A better approach is to create a library of templates. This promotes consistency and makes maintenance easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a44e41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATES = {\n",
    "    \"summarize_incident\": \"Summarize this incident report for a shift supervisor in 3 bullet points:\\n\\n{report}\",\n",
    "    \"extract_entities_json\": \"You are a data extraction bot. From the following text, extract the asset ID, component, and a brief description of the failure. Respond ONLY with a valid JSON object with the keys 'asset_id', 'component', and 'failure_description'.\\n\\nText: \\\"{report}\\\"\\n\\nJSON:\",\n",
    "    \"draft_safety_alert\": \"You are an EHS (Environment, Health, and Safety) officer. Based on the incident below, draft a 2-sentence safety alert to be posted on the factory floor. The tone should be urgent and clear.\\n\\nIncident: \\\"{report}\\\"\\n\\nAlert:\"\n",
    "}\n",
    "\n",
    "# Example of using a template\n",
    "json_prompt = PROMPT_TEMPLATES['extract_entities_json'].format(report=incident_report)\n",
    "json_output = generator(json_prompt)\n",
    "\n",
    "print(\"--- Prompt Template for JSON Output ---\")\n",
    "print(json_output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc8e342",
   "metadata": {},
   "source": [
    "## üìä Evaluating Prompts\n",
    "\n",
    "How do you know which prompt is better? You test them. A systematic evaluation framework is crucial.\n",
    "\n",
    "| Criterion | How to Measure | Example |\n",
    "| --- | --- | --- |\n",
    "| **Accuracy** | Compare model output to a \"golden dataset\" of correct answers. | Does the `extract_entities_json` prompt correctly identify the asset 95% of the time? |\n",
    "| **Safety/Compliance** | Use a checklist or another LLM to flag policy violations. | Does the `draft_safety_alert` prompt ever suggest an unsafe action? |\n",
    "| **Tone & Style** | Semantic similarity to a desired style guide or manual check. | Is the safety alert's tone appropriately urgent? |\n",
    "| **Latency & Cost** | Measure response time and token count. | Does the Chain-of-Thought prompt take too long for a real-time interface? |\n",
    "You should maintain a log of these experiments to track which prompts perform best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b316dd",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example of an experiment tracking log\n",
    "prompt_experiments = pd.DataFrame([\n",
    "    {\n",
    "        \"prompt_name\": \"summarize_v1_zero_shot\",\n",
    "        \"accuracy\": 0.75, # % of summaries deemed 'good' by a human reviewer\n",
    "        \"safety_flags\": 0,\n",
    "        \"avg_latency_ms\": 450,\n",
    "    },\n",
    "    {\n",
    "        \"prompt_name\": \"summarize_v2_with_role\",\n",
    "        \"accuracy\": 0.88,\n",
    "        \"safety_flags\": 0,\n",
    "        \"avg_latency_ms\": 510,\n",
    "    },\n",
    "    {\n",
    "        \"prompt_name\": \"extract_json_v1_few_shot\",\n",
    "        \"accuracy\": 0.96, # % of fields correctly extracted\n",
    "        \"safety_flags\": 0,\n",
    "        \"avg_latency_ms\": 820,\n",
    "    },\n",
    "])\n",
    "\n",
    "print(\"--- Prompt Evaluation Log ---\")\n",
    "prompt_experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c693da1",
   "metadata": {},
   "source": [
    "## ‚úÖ Next Steps\n",
    "\n",
    "You now have a powerful toolkit for guiding LLMs. The key is to be systematic:\n",
    "\n",
    "1.  **Start Simple:** Always begin with a zero-shot prompt.\n",
    "2.  **Add Complexity:** If it fails, add examples (few-shot), roles, and chain-of-thought reasoning.\n",
    "3.  **Enforce Structure:** Use templates and request specific output formats like JSON.\n",
    "4.  **Test Everything:** Never assume a prompt is good. Evaluate it against your criteria.\n",
    "\n",
    "In the next notebooks, we will combine these prompting techniques with **Retrieval-Augmented Generation (RAG)** to build a system that can answer questions based on your private documents."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
