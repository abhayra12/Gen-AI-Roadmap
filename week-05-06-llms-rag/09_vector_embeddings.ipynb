{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1cb2ce4",
   "metadata": {},
   "source": [
    "# ğŸ§® Week 5-6 Â· Notebook 09 Â· A Deep Dive into Vector Embeddings\n",
    "\n",
    "**Module:** LLMs, Prompt Engineering & RAG  \n",
    "**Project:** Build the Knowledge Core for the Manufacturing Copilot\n",
    "\n",
    "---\n",
    "\n",
    "Vector embeddings are the foundation of modern semantic search and Retrieval-Augmented Generation (RAG). They are numerical representations of text that capture its semantic meaning. In this notebook, we'll explore what embeddings are, how to create them, and why they are the key to making our Manufacturing Copilot's knowledge base searchable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08806541",
   "metadata": {},
   "source": [
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. âœ… **Explain Vector Embeddings:** Describe what embeddings are and how they capture the meaning of text.\n",
    "2. âœ… **Generate and Compare Embeddings:** Use a Hugging Face model to create embeddings and measure their similarity.\n",
    "3. âœ… **Visualize Embeddings:** Use dimensionality reduction (PCA) to visualize the relationships between documents in 2D space.\n",
    "4. âœ… **Evaluate Embedding Quality:** Understand the importance of evaluating embeddings and implement a basic recall metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d05487d",
   "metadata": {},
   "source": [
    "## âš™ï¸ Setup: Installing Libraries\n",
    "\n",
    "We'll need `sentence-transformers` to create embeddings and `scikit-learn` for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b0e4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q sentence-transformers pandas numpy scikit-learn matplotlib\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be03e4f",
   "metadata": {},
   "source": [
    "## ğŸ§  What are Vector Embeddings?\n",
    "\n",
    "Imagine a giant library where books are organized not by title, but by their *content*. Books about similar topics are placed close together. Vector embeddings do this for text. An embedding model converts a piece of text (a sentence, a paragraph, or a whole document) into a list of numbers (a vector).\n",
    "\n",
    "The magic is that texts with similar meanings will have vectors that are \"close\" to each other in this high-dimensional space. This is what allows us to find relevant documents for a user's query.\n",
    "\n",
    "Let's create some embeddings for a few sample manufacturing documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09de947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more realistic set of documents for our knowledge base\n",
    "documents = pd.DataFrame([\n",
    "    {\n",
    "        \"label\": \"maintenance\", \"text\": \"The hydraulic press requires an oil change every 2000 hours of operation to prevent wear.\"},\n",
    "    {\n",
    "        \"label\": \"maintenance\", \"text\": \"Weekly inspection of the conveyor belt's tension is mandatory for all shift supervisors.\"},\n",
    "    {\n",
    "        \"label\": \"incident\", \"text\": \"Incident Report #451: CNC machine #3 stalled due to a coolant leak. Downtime was 35 minutes.\"},\n",
    "    {\n",
    "        \"label\": \"incident\", \"text\": \"Incident Report #452: Operator noticed unusual vibrations from the main compressor unit.\"},\n",
    "    {\n",
    "        \"label\": \"safety\", \"text\": \"Safety Alert: Always perform lockout-tagout before servicing any machinery with moving parts.\"},\n",
    "    {\n",
    "        \"label\": \"quality\", \"text\": \"Quality Control: Part #A-103 is showing surface defects. Calibrate the vision system immediately.\"}\n",
    "])\n",
    "\n",
    "# Load a pre-trained model from Hugging Face\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Generate embeddings for our documents\n",
    "embeddings = model.encode(documents.text.tolist(), convert_to_tensor=True)\n",
    "\n",
    "print(f\"Generated {embeddings.shape[0]} embeddings, each with {embeddings.shape[1]} dimensions.\")\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c576d20e",
   "metadata": {},
   "source": [
    "## ğŸ“ Measuring Similarity: Cosine Similarity\n",
    "\n",
    "How do we measure the \"closeness\" of two vectors? The most common method is **Cosine Similarity**. It measures the cosine of the angle between two vectors. \n",
    "\n",
    "- A value of **1** means the vectors are identical in orientation (very similar).\n",
    "- A value of **0** means they are orthogonal (unrelated).\n",
    "- A value of **-1** means they are opposite (dissimilar).\n",
    "\n",
    "Let's calculate the similarity between all of our documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b51d1a",
   "metadata": {},
   "source": [
    "# Calculate the cosine similarity matrix\n",
    "similarity_matrix = util.cos_sim(embeddings, embeddings).cpu().numpy()\n",
    "\n",
    "# Display it as a DataFrame for readability\n",
    "sim_df = pd.DataFrame(similarity_matrix, index=documents.label, columns=documents.label)\n",
    "\n",
    "print(\"--- Cosine Similarity Matrix ---\")\n",
    "# Highlighting values > 0.5 for clarity\n",
    "sim_df.style.applymap(lambda x: 'background-color: yellow' if x > 0.5 and x < 1.0 else '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c26165b",
   "metadata": {},
   "source": [
    "**Interpretation:** Notice the yellow boxes! The two `maintenance` documents are similar to each other, as are the two `incident` reports. The `safety` and `quality` documents are less similar to the others, which makes sense. This is the power of semantic search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de47e1ad",
   "metadata": {},
   "source": [
    "## ğŸ¨ Visualizing Embeddings with PCA\n",
    "\n",
    "It's hard to visualize vectors with 384 dimensions. We can use a technique called **Principal Component Analysis (PCA)** to reduce the dimensions to 2, allowing us to plot them on a graph. This helps build intuition about how the model is organizing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4919cc",
   "metadata": {},
   "source": [
    "# Reduce dimensions to 2D using PCA\n",
    "pca = PCA(n_components=2)\n",
    "coords = pca.fit_transform(embeddings.cpu().numpy())\n",
    "\n",
    "documents['pca_x'] = coords[:, 0]\n",
    "documents['pca_y'] = coords[:, 1]\n",
    "\n",
    "# Plot the 2D coordinates\n",
    "plt.figure(figsize=(10, 8))\n",
    "for label, group in documents.groupby('label'):\n",
    "    plt.scatter(group.pca_x, group.pca_y, label=label, alpha=0.7)\n",
    "\n",
    "# Annotate points\n",
    "for i, row in documents.iterrows():\n",
    "    plt.text(row.pca_x + 0.01, row.pca_y, f\"Doc {i}\", fontsize=9)\n",
    "\n",
    "plt.title(\"2D Visualization of Document Embeddings using PCA\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2976563e",
   "metadata": {},
   "source": [
    "**Interpretation:** As you can see, the model naturally clusters similar documents together. The `maintenance` and `incident` documents form distinct groups. This clustering is what enables a vector store to quickly find relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf4368c",
   "metadata": {},
   "source": [
    "## ğŸ§ª Evaluating Embedding Quality\n",
    "\n",
    "Not all embedding models are created equal. For a real-world application, you must evaluate how well your chosen model performs on *your* data. A common metric is **Recall@k**.\n",
    "\n",
    "**Recall@k:** For a given question, does the correct (relevant) document appear in the top `k` retrieved results? If yes, the score is 1, otherwise 0.\n",
    "\n",
    "Let's run a simple evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5be54b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a few test questions and the ID of the document that should answer them\n",
    "eval_questions = {\n",
    "    \"How often to change hydraulic oil?\": 0, # Should match doc 0\n",
    "    \"What happened to the CNC machine?\": 2, # Should match doc 2\n",
    "    \"What is the rule for servicing machines?\": 4 # Should match doc 4\n",
    "}\n",
    "\n",
    "def evaluate_recall(k=1):\n",
    "    correct = 0\n",
    "    for question, doc_id in eval_questions.items():\n",
    "        # Embed the question\n",
    "        query_embedding = model.encode(question, convert_to_tensor=True)\n",
    "        \n",
    "        # Find the top_k most similar documents in our corpus\n",
    "        hits = util.semantic_search(query_embedding, embeddings, top_k=k)[0]\n",
    "        \n",
    "        # Check if the correct document ID is in the retrieved hits\n",
    "        retrieved_ids = [hit['corpus_id'] for hit in hits]\n",
    "        if doc_id in retrieved_ids:\n",
    "            correct += 1\n",
    "            \n",
    "    return correct / len(eval_questions)\n",
    "\n",
    "recall_at_1 = evaluate_recall(k=1)\n",
    "recall_at_2 = evaluate_recall(k=2)\n",
    "\n",
    "print(f\"Recall@1: {recall_at_1:.2%}\") # Is the correct doc the #1 result?\n",
    "print(f\"Recall@2: {recall_at_2:.2%}\") # Is the correct doc in the top 2 results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d83889",
   "metadata": {},
   "source": [
    "## âœ… Congratulations! You've Completed the LLM & RAG Module!\n",
    "\n",
    "This notebook concludes our deep dive into the core components of modern LLM applications. You now understand:\n",
    "\n",
    "1.  **LLMs and Transformers:** The fundamental models that power generative AI.\n",
    "2.  **Prompt Engineering:** How to guide LLMs to produce desired outputs.\n",
    "3.  **Few-Shot Learning:** How to provide examples to improve performance on specific tasks.\n",
    "4.  **Retrieval-Augmented Generation (RAG):** The architecture for connecting LLMs to external knowledge.\n",
    "5.  **Vector Embeddings:** The technology that makes retrieval possible.\n",
    "\n",
    "You have all the conceptual tools needed to build the knowledge core for the Manufacturing Copilot. In the next module, we will explore how to productionize these ideas and build a complete, end-to-end application."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
