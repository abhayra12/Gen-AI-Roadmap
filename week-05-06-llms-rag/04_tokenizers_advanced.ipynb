{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5763ceda",
   "metadata": {},
   "source": [
    "# ðŸ§© Week 5-6 Â· Notebook 04 Â· Advanced Tokenizers\n",
    "\n",
    "**Module:** LLMs, Prompt Engineering & RAG  \n",
    "**Project:** Build the Knowledge Core for the Manufacturing Copilot\n",
    "\n",
    "---\n",
    "\n",
    "Tokenization is the critical, often overlooked, bridge between raw, messy text from the factory floor and the structured inputs a Large Language Model requires. A good tokenizer understands your domain's languageâ€”from part numbers to error codes. A bad one will shred important terms into meaningless pieces, crippling your model's performance.\n",
    "\n",
    "In this notebook, we will train our own **domain-specific tokenizer** on manufacturing data. This will be a key component of our Manufacturing Copilot, ensuring it understands the unique vocabulary of our factory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3bf095",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Learning Outcomes\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. âœ… **Diagnose Tokenizer Issues:** See how different standard tokenizers fail on manufacturing jargon.\n",
    "2. âœ… **Train a Custom Tokenizer:** Build a BPE tokenizer from scratch using maintenance logs.\n",
    "3. âœ… **Measure Vocabulary Coverage:** Quantify the improvement of a custom tokenizer by measuring the Out-of-Vocabulary (OOV) rate.\n",
    "4. âœ… **Package & Save a Tokenizer:** Properly save a trained tokenizer so it can be loaded with `AutoTokenizer`.\n",
    "5. âœ… **Control Padding & Truncation:** Configure tokenizers for different inference scenarios (e.g., real-time vs. batch)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1da802",
   "metadata": {},
   "source": [
    "## ðŸ­ The Problem: Standard Tokenizers vs. Manufacturing Jargon\n",
    "\n",
    "Let's see how popular, off-the-shelf tokenizers handle text they've likely never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9657fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "terms = [\n",
    "    'Hydroforming pressure calibration check for part #HFP-2024A.',\n",
    "    'OEE dropped to 71% after unplanned downtime on CNC-12.',\n",
    "    'Favor revisar torque 450 Nm en lote 18. (Spanish)',\n",
    "    'Robot axis-3 grease refill overdue per SOP-442-V3.'\n",
    "]\n",
    "\n",
    "# Note: You may need to request access or log in for meta-llama/Meta-Llama-3-8B\n",
    "tokenizers_to_test = {\n",
    "    'GPT-2 (BPE)': 'gpt2',\n",
    "    'BERT (WordPiece)': 'bert-base-uncased',\n",
    "    # 'Llama-3 (BPE)': 'meta-llama/Meta-Llama-3-8B' # Uncomment if you have access\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for label, model_name in tokenizers_to_test.items():\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        for text in terms:\n",
    "            tokens = tokenizer.tokenize(text)\n",
    "            rows.append({'Tokenizer': label, 'Text': text, 'Token Count': len(tokens), 'Tokens': ' '.join(tokens)})\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load tokenizer {model_name}. Error: {e}\")\n",
    "\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad117543",
   "metadata": {},
   "source": [
    "### **Observations & Diagnosis**\n",
    "\n",
    "1.  **Fragmented Terms:** Notice how `HFP-2024A` is split into many pieces by all tokenizers (e.g., `H`, `FP`, `-`, `2024`, `A`). The model loses the concept of this being a single part number.\n",
    "2.  **Units:** GPT-2 splits `450` and `Nm` into `['450', 'N', 'm']`. The model might not understand `Nm` as a unit of torque.\n",
    "3.  **Inconsistency:** `CNC-12` is handled differently by each tokenizer. This inconsistency makes it hard for a model to learn what a \n",
    " is.\n",
    "\n",
    "**Conclusion:** We need a tokenizer that learns *our* vocabulary. Let's build one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aca0e7",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Training a Domain-Specific Tokenizer\n",
    "\n",
    "We will use the `tokenizers` library to train a Byte-Pair Encoding (BPE) tokenizer on a small sample of maintenance logs. In a real project, you would use thousands of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af855a59",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# Step 1: Prepare our training data (a list of strings)\n",
    "maintenance_logs = [\n",
    "    'Press-24 hydraulic accumulator leak detected at 03:14.',\n",
    "    'Torque wrench calibration overdue for cell B; schedule before shift 2.',\n",
    "    'Robot cell 3 axis-2 grease refill triggered due to high temperature.',\n",
    "    'Lathe #4 vibration at 12.5 mm/s despite new SKF-6205-2Z bearing.',\n",
    "    'Favor revisar torque 450 Nm en lote 18 y reportar a calidad.',\n",
    "    'OEE for CNC-12 dropped to 71%. Root cause: spindle overheating.',\n",
    "    'Part #HFP-2024A failed quality inspection due to surface defects.'\n",
    "]\n",
    "\n",
    "# Step 2: Configure the Tokenizer\n",
    "custom_tokenizer = Tokenizer(BPE(unk_token='[UNK]'))\n",
    "custom_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Step 3: Configure the Trainer\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=1000,  # A larger vocab size can capture more specific terms\n",
    "    min_frequency=1,  # Include words that appear at least once\n",
    "    special_tokens=['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']\n",
    ")\n",
    "\n",
    "# Step 4: Train the tokenizer\n",
    "custom_tokenizer.train_from_iterator(maintenance_logs, trainer=trainer)\n",
    "\n",
    "# Step 5: Save the tokenizer file\n",
    "artifacts_dir = Path('artifacts/custom_tokenizer')\n",
    "artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
    "tokenizer_path = str(artifacts_dir / 'maintenance-tokenizer.json')\n",
    "custom_tokenizer.save(tokenizer_path)\n",
    "\n",
    "print(f\"Custom tokenizer saved to: {tokenizer_path}\")\n",
    "print(f\"Vocabulary size: {custom_tokenizer.get_vocab_size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc07c5a",
   "metadata": {},
   "source": [
    "### Testing our new tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d5e358",
   "metadata": {},
   "source": [
    "test_sentence = 'New SKF-6205-2Z bearing for CNC-12 shows high vibration.'\n",
    "encoding = custom_tokenizer.encode(test_sentence)\n",
    "\n",
    "print(f\"Test Sentence: '{test_sentence}'\")\n",
    "print(f\"Custom Tokenizer Output: {encoding.tokens}\")\n",
    "\n",
    "# Compare with BERT\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(f\"BERT Tokenizer Output:   {bert_tokenizer.tokenize(test_sentence)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358bb20c",
   "metadata": {},
   "source": [
    "**Analysis:** Our custom tokenizer correctly keeps `SKF-6205-2Z` and `CNC-12` as single tokens! This is a huge improvement. It learned these terms from our small dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb333366",
   "metadata": {},
   "source": [
    "## ðŸ“Š Coverage Audit: Measuring Out-of-Vocabulary (OOV) Risk\n",
    "\n",
    "A key metric for a tokenizer's quality is its OOV rateâ€”the percentage of tokens that it doesn't recognize and maps to `[UNK]`. A lower OOV rate is better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f30c9c",
   "metadata": {},
   "source": [
    "def calculate_oov_rate(tokenizer, text_iterator):\n",
    "    total_tokens = 0\n",
    "    unk_tokens = 0\n",
    "    \n",
    "    # Handle both 'tokenizers' library and 'transformers' library objects\n",
    "    is_hf_tokenizer = hasattr(tokenizer, 'vocab')\n",
    "    unk_token_id = tokenizer.unk_token_id if is_hf_tokenizer else tokenizer.token_to_id('[UNK]')\n",
    "\n",
    "    for text in text_iterator:\n",
    "        if is_hf_tokenizer:\n",
    "            encoding = tokenizer(text)['input_ids']\n",
    "        else:\n",
    "            encoding = tokenizer.encode(text).ids\n",
    "        \n",
    "        total_tokens += len(encoding)\n",
    "        unk_tokens += encoding.count(unk_token_id)\n",
    "        \n",
    "    return (unk_tokens / total_tokens) if total_tokens > 0 else 0\n",
    "\n",
    "# Load our custom tokenizer from the file\n",
    "from tokenizers import Tokenizer\n",
    "custom_tok = Tokenizer.from_file(tokenizer_path)\n",
    "\n",
    "# The baseline tokenizer (BERT)\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Calculate OOV rates\n",
    "custom_oov_rate = calculate_oov_rate(custom_tok, maintenance_logs)\n",
    "bert_oov_rate = calculate_oov_rate(bert_tokenizer, maintenance_logs)\n",
    "\n",
    "print(f\"OOV Rate (Custom Tokenizer): {custom_oov_rate:.2%}\")\n",
    "print(f\"OOV Rate (BERT Tokenizer):   {bert_oov_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c258557",
   "metadata": {},
   "source": [
    "Our custom tokenizer has a 0% OOV rate on its training data, which is expected. The real test is on a *held-out* test set of logs it has never seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4582de85",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Packaging for HuggingFace `AutoTokenizer`\n",
    "\n",
    "To make our tokenizer easily reusable, we need to save it in a format that `AutoTokenizer.from_pretrained()` understands. This requires the `tokenizer.json` file and a `tokenizer_config.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749a5005",
   "metadata": {},
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "import json\n",
    "\n",
    "# The `tokenizers` library produces a single JSON file. \n",
    "# We can wrap this in a `PreTrainedTokenizerFast` object, which is the standard HuggingFace format.\n",
    "\n",
    "# 1. Load the trained tokenizer\n",
    "slow_tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "\n",
    "# 2. Wrap it in the HuggingFace Fast Tokenizer implementation\n",
    "hf_fast_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=slow_tokenizer,\n",
    "    unk_token=\n",
    ",\n",
    "    pad_token=\n",
    ",\n",
    "    cls_token=\n",
    ",\n",
    "    sep_token=\n",
    ",\n",
    "    mask_token=\n",
    ",\n",
    ")\n",
    "\n",
    "# 3. Save it using the HuggingFace `save_pretrained` method\n",
    "hf_tokenizer_dir = Path('artifacts/hf_custom_tokenizer')\n",
    "hf_fast_tokenizer.save_pretrained(str(hf_tokenizer_dir))\n",
    "\n",
    "print(f\n",
    "print("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b257e480",
   "metadata": {},
   "source": [
    "### Loading and Using the Packaged Tokenizer\n",
    "\n",
    "Now, anyone on the team can load our custom tokenizer with a single, standard line of code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5116e1",
   "metadata": {},
   "source": [
    "# Load the tokenizer from the directory we just saved it to\n",
    "reloaded_tokenizer = AutoTokenizer.from_pretrained(str(hf_tokenizer_dir))\n",
    "\n",
    "test_sentence = 'Part #HFP-2024A from CNC-12 needs a new SKF-6205-2Z bearing.'\n",
    "tokens = reloaded_tokenizer.tokenize(test_sentence)\n",
    "\n",
    "print(f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca77ab9",
   "metadata": {},
   "source": [
    "This packaged tokenizer is now ready to be used in our RAG pipeline (Notebook 08), ensuring that the text chunking and embedding steps use the exact same vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1748fa6d",
   "metadata": {},
   "source": [
    "## ðŸª„ Padding and Truncation\n",
    "\n",
    "When processing batches of text, we need all input sequences to be the same length. We use padding and truncation to achieve this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933fe8f3",
   "metadata": {},
   "source": [
    "sample_batch = [\n",
    "    'Shift 1: verify coolant pressure before restart.',\n",
    "    'Alert: axis-3 vibration exceeded 9 mm/s threshold on CNC-12.',\n",
    "    'Favor revisar torque 450 Nm en lote 18.'\n",
    "]\n",
    "\n",
    "# Dynamic Padding: Pads each batch to the length of the longest sequence in that batch.\n",
    "# Ideal for inference APIs where batch composition changes.\n",
    "encoded_dynamic = reloaded_tokenizer(sample_batch, padding=True, return_tensors='pt')\n",
    "\n",
    "# Static Padding: Pads all sequences to a fixed `max_length`.\n",
    "# Useful for training on GPUs where uniform shapes are more efficient.\n",
    "encoded_static = reloaded_tokenizer(sample_batch, padding='max_length', max_length=24, truncation=True, return_tensors='pt')\n",
    "\n",
    "print(f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0b3a70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bd02dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
