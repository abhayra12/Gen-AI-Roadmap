{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5763ceda",
   "metadata": {},
   "source": [
    "# ðŸ§© Week 5-6, Notebook 4: Advanced Tokenization and Custom Vocabularies\n",
    "\n",
    "**Module:** LLMs, Prompt Engineering & RAG  \n",
    "**Project:** Build the Knowledge Core for the Manufacturing Copilot\n",
    "\n",
    "---\n",
    "\n",
    "Tokenization is the critical, often overlooked, bridge between the raw, messy text from the factory floor and the structured numerical inputs a Large Language Model requires. A well-designed tokenizer understands your domain's unique languageâ€”from part numbers and error codes to specialized verbs. A poorly suited one will shred important terms into meaningless sub-pieces, crippling your model's ability to understand context and nuance.\n",
    "\n",
    "In this notebook, we move beyond using pre-trained tokenizers and take a crucial step toward building a true domain-specific model: we will **train our own tokenizer** from scratch on a corpus of manufacturing data. This custom tokenizer will form a key component of our Manufacturing Copilot, ensuring it speaks the language of our factory and can interpret technical information correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3bf095",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1.  **Diagnose Tokenizer Mismatches:** Articulate and demonstrate how standard, pre-trained tokenizers can fail on specialized manufacturing jargon.\n",
    "2.  **Train a Custom Tokenizer:** Build, train, and save a new Byte-Pair Encoding (BPE) tokenizer from scratch using a corpus of sample maintenance logs.\n",
    "3.  **Measure Vocabulary Coverage:** Quantify the effectiveness of a tokenizer by calculating its Out-of-Vocabulary (OOV) rate on a given dataset.\n",
    "4.  **Package and Distribute a Tokenizer:** Properly save a trained tokenizer in a format that can be easily loaded and shared using the standard Hugging Face `AutoTokenizer` class.\n",
    "5.  **Control Padding and Truncation:** Configure a tokenizer to handle batches of text with varying lengths, a crucial step for both training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1da802",
   "metadata": {},
   "source": [
    "## ðŸ­ Part 1: The Problem with Off-the-Shelf Tokenizers\n",
    "\n",
    "Pre-trained models from the Hugging Face Hub come with their own tokenizers, which were trained on massive, general-purpose datasets like Wikipedia and Common Crawl. While powerful, these tokenizers have often never seen the specific jargon, part numbers, and error codes common in a manufacturing environment.\n",
    "\n",
    "Let's see what happens when we feed some typical manufacturing text to these standard tokenizers. We will examine how they \"see\" the text by looking at the tokens they produce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9657fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hands-On: Diagnosing Tokenizer Failures\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "# A list of terms and phrases commonly found in a manufacturing setting\n",
    "manufacturing_terms = [\n",
    "    'Hydroforming pressure calibration check for part #HFP-2024A.',\n",
    "    'OEE dropped to 71% after unplanned downtime on CNC-12.',\n",
    "    'Favor revisar torque 450 Nm en lote 18. (Spanish)',\n",
    "    'Robot axis-3 grease refill overdue per SOP-442-V3.'\n",
    "]\n",
    "\n",
    "# Let's test a few popular tokenizers with different underlying algorithms.\n",
    "# Note: You may need to request access or log in for meta-llama/Meta-Llama-3-8B\n",
    "tokenizers_to_test = {\n",
    "    'GPT-2 (BPE)': 'gpt2',\n",
    "    'BERT (WordPiece)': 'bert-base-uncased',\n",
    "    'Llama-3 (BPE)': 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model_path in tokenizers_to_test.items():\n",
    "    try:\n",
    "        # Load the tokenizer from the Hub\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        for text in manufacturing_terms:\n",
    "            # Get the list of token strings\n",
    "            tokens = tokenizer.tokenize(text)\n",
    "            results.append({\n",
    "                'Tokenizer': name,\n",
    "                'Original Text': text,\n",
    "                'Token Count': len(tokens),\n",
    "                'Tokens': ' | '.join(tokens)  # Use a separator for clarity\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load tokenizer '{model_path}'. It may require special access. Error: {e}\")\n",
    "\n",
    "# Display the results in a DataFrame for easy comparison\n",
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad117543",
   "metadata": {},
   "source": [
    "### **Analysis and Diagnosis**\n",
    "\n",
    "The results clearly show the problem:\n",
    "\n",
    "1.  **Fragmented Technical Terms:** All tokenizers shred our domain-specific identifiers. `HFP-2024A` is broken into many meaningless pieces like `H`, `FP`, `-`, `2024`, and `A`. The model has no way of knowing this is a single, unique part number.\n",
    "2.  **Loss of Meaning:** GPT-2 splits the unit `Nm` (Newton-meters) into `N` and `m`. The model might lose the semantic meaning of this being a unit of torque.\n",
    "3.  **Inconsistent Tokenization:** `CNC-12` is handled differently by each tokenizer. This inconsistency makes it extremely difficult for a model to learn a consistent representation for what a \"CNC machine\" is.\n",
    "\n",
    "**The Core Problem:** The vocabularies of these general-purpose tokenizers do not contain our specialized terms.\n",
    "\n",
    "**The Solution:** We need to train a new tokenizer that learns *our* vocabulary from *our* data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aca0e7",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Part 2: Training a Domain-Specific Tokenizer\n",
    "\n",
    "To solve this problem, we will use the `tokenizers` library from Hugging Face to train our own **Byte-Pair Encoding (BPE)** tokenizer. BPE is a subword tokenization algorithm that starts with a base vocabulary of individual characters and iteratively merges the most frequently co-occurring pairs of tokens.\n",
    "\n",
    "This bottom-up approach allows the tokenizer to \"discover\" the common words and subwords in our corpus. By training it on our maintenance logs, it will learn to recognize terms like `CNC-12` and `HFP-2024A` as single, meaningful units.\n",
    "\n",
    "In a real-world project, you would use thousands or even millions of documents for training. For this demonstration, we will use a small, representative sample of maintenance logs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af855a59",
   "metadata": {},
   "source": [
    "# Hands-On: Training a Custom BPE Tokenizer\n",
    "from pathlib import Path\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# --- Step 1: Prepare the Training Corpus ---\n",
    "# This should be an iterator that yields strings. For a large dataset,\n",
    "# you could have a generator that reads lines from a file.\n",
    "maintenance_logs_corpus = [\n",
    "    'Press-24 hydraulic accumulator leak detected at 03:14.',\n",
    "    'Torque wrench calibration overdue for cell B; schedule before shift 2.',\n",
    "    'Robot cell 3 axis-2 grease refill triggered due to high temperature.',\n",
    "    'Lathe #4 vibration at 12.5 mm/s despite new SKF-6205-2Z bearing.',\n",
    "    'Favor revisar torque 450 Nm en lote 18 y reportar a calidad.',\n",
    "    'OEE for CNC-12 dropped to 71%. Root cause: spindle overheating.',\n",
    "    'Part #HFP-2024A failed quality inspection due to surface defects.'\n",
    "]\n",
    "\n",
    "# --- Step 2: Configure the Tokenizer ---\n",
    "# We start with a blank BPE model. The `unk_token` is used for any tokens not in the vocabulary.\n",
    "custom_tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "# The pre-tokenizer splits the text into words. Whitespace splitting is a good start.\n",
    "custom_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# --- Step 3: Configure and Run the Trainer ---\n",
    "# The trainer will learn the merge rules from our corpus.\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=1000,  # The desired size of the final vocabulary.\n",
    "    min_frequency=1,  # Include tokens that appear at least once.\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] # Define special tokens.\n",
    ")\n",
    "\n",
    "# Train the tokenizer on our data\n",
    "custom_tokenizer.train_from_iterator(maintenance_logs_corpus, trainer=trainer)\n",
    "\n",
    "# --- Step 4: Save the Trained Tokenizer ---\n",
    "# We save the tokenizer's configuration and vocabulary to a single JSON file.\n",
    "# This file contains everything needed to reuse the tokenizer later.\n",
    "artifacts_dir = Path(\"artifacts/custom_tokenizer\")\n",
    "artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
    "tokenizer_path = str(artifacts_dir / \"maintenance-tokenizer.json\")\n",
    "custom_tokenizer.save(tokenizer_path)\n",
    "\n",
    "print(f\"Custom tokenizer trained and saved to: {tokenizer_path}\")\n",
    "print(f\"Final Vocabulary Size: {custom_tokenizer.get_vocab_size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc07c5a",
   "metadata": {},
   "source": [
    "### Testing Our Newly Trained Tokenizer\n",
    "\n",
    "Now for the moment of truth. Let's compare how our custom tokenizer handles a test sentence compared to the general-purpose BERT tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d5e358",
   "metadata": {},
   "source": [
    "# A test sentence containing our specialized terms\n",
    "test_sentence = \"New SKF-6205-2Z bearing for CNC-12 shows high vibration.\"\n",
    "\n",
    "# Encode the sentence with our custom tokenizer\n",
    "encoding = custom_tokenizer.encode(test_sentence)\n",
    "\n",
    "print(f\"Test Sentence: '{test_sentence}'\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Custom Tokenizer Output:\\n{encoding.tokens}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# For comparison, let's see how the standard BERT tokenizer handles it\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(f\"BERT Tokenizer Output:\\n{bert_tokenizer.tokenize(test_sentence)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358bb20c",
   "metadata": {},
   "source": [
    "**Analysis:** Success! Our custom tokenizer correctly identifies `SKF-6205-2Z` and `CNC-12` as single, indivisible tokens. The BERT tokenizer, in contrast, shatters them into multiple, less meaningful subwords.\n",
    "\n",
    "This is a massive improvement. By learning the specific vocabulary of our domain, our tokenizer provides a much more accurate and semantically meaningful representation of the text, which will directly lead to better performance when we use it with a language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb333366",
   "metadata": {},
   "source": [
    "## ðŸ“Š Part 3: Measuring Vocabulary Coverage with the OOV Rate\n",
    "\n",
    "A key metric for evaluating a tokenizer's quality is its **Out-of-Vocabulary (OOV) rate**. This is the percentage of tokens in a given text that the tokenizer does not recognize and therefore maps to its special `[UNK]` (unknown) token.\n",
    "\n",
    "A high OOV rate is a major problem. If the tokenizer frequently encounters unknown words, the model loses valuable information and cannot make accurate predictions. Our goal is to minimize the OOV rate on our target domain data. A lower OOV rate indicates a better fit between the tokenizer's vocabulary and the text it will be processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f30c9c",
   "metadata": {},
   "source": [
    "# Hands-On: Calculating the OOV Rate\n",
    "def calculate_oov_rate(tokenizer, text_iterator):\n",
    "    \"\"\"Calculates the OOV rate for a given tokenizer and text corpus.\"\"\"\n",
    "    total_tokens = 0\n",
    "    unk_tokens = 0\n",
    "\n",
    "    # Get the ID for the unknown token\n",
    "    try:\n",
    "        # For `transformers` tokenizers\n",
    "        unk_token_id = tokenizer.unk_token_id\n",
    "    except AttributeError:\n",
    "        # For `tokenizers` library tokenizers\n",
    "        unk_token_id = tokenizer.token_to_id(\"[UNK]\")\n",
    "\n",
    "    for text in text_iterator:\n",
    "        # Get the token IDs for the text\n",
    "        if hasattr(tokenizer, 'encode_plus'): # Heuristic for transformers tokenizer\n",
    "            encoding = tokenizer.encode(text)\n",
    "        else: # Heuristic for tokenizers library tokenizer\n",
    "            encoding = tokenizer.encode(text).ids\n",
    "\n",
    "        total_tokens += len(encoding)\n",
    "        unk_tokens += encoding.count(unk_token_id)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    return (unk_tokens / total_tokens) if total_tokens > 0 else 0\n",
    "\n",
    "# --- Compare OOV Rates ---\n",
    "\n",
    "# 1. Load our custom tokenizer from the saved file\n",
    "from tokenizers import Tokenizer\n",
    "custom_tok_from_file = Tokenizer.from_file(tokenizer_path)\n",
    "\n",
    "# 2. Load the baseline BERT tokenizer\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 3. Calculate OOV rates on our maintenance log corpus\n",
    "custom_oov_rate = calculate_oov_rate(custom_tok_from_file, maintenance_logs_corpus)\n",
    "bert_oov_rate = calculate_oov_rate(bert_tokenizer, maintenance_logs_corpus)\n",
    "\n",
    "print(f\"OOV Rate (Custom Tokenizer): {custom_oov_rate:.2%}\")\n",
    "print(f\"OOV Rate (BERT Tokenizer):   {bert_oov_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c258557",
   "metadata": {},
   "source": [
    "As expected, our custom tokenizer has a 0% OOV rate on the data it was trained on. The BERT tokenizer, while not having a terrible OOV rate, still fails to recognize some of the sub-word components of our specialized terms.\n",
    "\n",
    "**The Real Test:** In a real project, you would perform this calculation on a **held-out test set**â€”a collection of documents that the tokenizer did *not* see during training. This gives you a much more honest measure of how well your tokenizer will perform on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4582de85",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Part 4: Packaging for Easy Use with `AutoTokenizer`\n",
    "\n",
    "The `tokenizers` library is excellent for training, but for deployment and sharing, we want our tokenizer to behave just like a standard Hugging Face tokenizer. This means being able to load it with the one-liner: `AutoTokenizer.from_pretrained(...)`.\n",
    "\n",
    "To achieve this, we need to convert our single `tokenizer.json` file into the standard Hugging Face format, which includes the `tokenizer.json` file along with a `tokenizer_config.json` and `special_tokens_map.json`. The `PreTrainedTokenizerFast` class provides a convenient way to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749a5005",
   "metadata": {},
   "source": [
    "# Hands-On: Saving in Hugging Face Format\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer\n",
    "import json\n",
    "\n",
    "# --- Step 1: Load the tokenizer we trained ---\n",
    "# This is the \"slow\" tokenizer object from the `tokenizers` library.\n",
    "slow_tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "\n",
    "# --- Step 2: Wrap it in a `PreTrainedTokenizerFast` object ---\n",
    "# This is the Hugging Face \"fast\" tokenizer implementation that wraps the underlying Rust-based tokenizer.\n",
    "hf_fast_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=slow_tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")\n",
    "\n",
    "# --- Step 3: Save it using the standard Hugging Face method ---\n",
    "# This will create a directory with all the necessary files (tokenizer.json, config, etc.).\n",
    "hf_tokenizer_dir = Path(\"artifacts/hf_custom_tokenizer\")\n",
    "hf_fast_tokenizer.save_pretrained(str(hf_tokenizer_dir))\n",
    "\n",
    "print(f\"Hugging Face compatible tokenizer saved to: '{hf_tokenizer_dir}'\")\n",
    "print(\"Directory contents:\", [p.name for p in hf_tokenizer_dir.iterdir()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b257e480",
   "metadata": {},
   "source": [
    "### Loading and Using the Packaged Tokenizer\n",
    "\n",
    "Now, anyone on your team can load and use your custom tokenizer with a single, standard line of code, just as they would with any pre-trained tokenizer from the Hugging Face Hub. This makes it incredibly easy to share and integrate into other parts of your project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5116e1",
   "metadata": {},
   "source": [
    "# Load the tokenizer from the directory we just saved it to\n",
    "reloaded_tokenizer = AutoTokenizer.from_pretrained(str(hf_tokenizer_dir))\n",
    "\n",
    "# Use it just like any other Hugging Face tokenizer\n",
    "test_sentence = \"Part #HFP-2024A from CNC-12 needs a new SKF-6205-2Z bearing.\"\n",
    "tokens = reloaded_tokenizer.tokenize(test_sentence)\n",
    "\n",
    "print(f\"Test Sentence: '{test_sentence}'\")\n",
    "print(f\"Tokens from reloaded tokenizer: {tokens}\")\n",
    "\n",
    "# The output is identical, proving our packaging was successful.\n",
    "# This packaged tokenizer is now ready to be used in our RAG pipeline (Notebook 08),\n",
    "# ensuring that the text chunking and embedding steps use the exact same vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1748fa6d",
   "metadata": {},
   "source": [
    "## ðŸª„ Part 5: Padding and Truncation for Batch Processing\n",
    "\n",
    "When you process multiple texts at once (a \"batch\"), you need to ensure that all the input sequences have the same length. This is because the underlying models and frameworks like PyTorch and TensorFlow require inputs to be in rectangular tensors. We use **padding** and **truncation** to achieve this.\n",
    "\n",
    "*   **Padding:** Adds special `[PAD]` tokens to the end of shorter sequences to make them match the length of the longest sequence in the batch.\n",
    "*   **Truncation:** Cuts off tokens from the end of longer sequences to ensure they do not exceed a specified maximum length.\n",
    "\n",
    "The tokenizer can handle both of these operations for you automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933fe8f3",
   "metadata": {},
   "source": [
    "# Hands-On: Padding and Truncation\n",
    "# A sample batch of texts with different lengths\n",
    "sample_batch = [\n",
    "    \"Shift 1: verify coolant pressure before restart.\",\n",
    "    \"Alert: axis-3 vibration exceeded 9 mm/s threshold on CNC-12.\",\n",
    "    \"Favor revisar torque 450 Nm en lote 18.\"\n",
    "]\n",
    "\n",
    "# --- Strategy 1: Dynamic Padding ---\n",
    "# Pad each sequence to the length of the *longest sequence in the current batch*.\n",
    "# This is efficient for inference, as it minimizes the number of padding tokens.\n",
    "encoded_dynamic = reloaded_tokenizer(sample_batch, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "print(\"--- Dynamic Padding ---\")\n",
    "print(\"Shape of Input IDs:\", encoded_dynamic['input_ids'].shape)\n",
    "print(\"Input IDs:\\n\", encoded_dynamic['input_ids'])\n",
    "print(\"Attention Mask:\\n\", encoded_dynamic['attention_mask'])\n",
    "print(\"\\nNote: The attention mask is 1 for real tokens and 0 for padding tokens.\")\n",
    "\n",
    "# --- Strategy 2: Static Padding to Max Length ---\n",
    "# Pad all sequences to a fixed `max_length`. If a sequence is longer, it will be truncated.\n",
    "# This is often used for training, as it creates uniformly shaped tensors which can be more efficient on GPUs.\n",
    "encoded_static = reloaded_tokenizer(\n",
    "    sample_batch,\n",
    "    padding=\"max_length\",\n",
    "    max_length=24,  # A fixed length for all sequences\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(\"\\n--- Static Padding to Max Length (24) ---\")\n",
    "print(\"Shape of Input IDs:\", encoded_static['input_ids'].shape)\n",
    "print(\"Input IDs:\\n\", encoded_static['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0b3a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "## âœ… Summary and Next Steps\n",
    "\n",
    "In this notebook, you dove deep into the world of tokenization and took a major step toward building a true domain-specific NLP application. You have learned:\n",
    "\n",
    "-   **Why standard tokenizers fail** on specialized text and how to diagnose these failures by inspecting the token output.\n",
    "-   **How to train a custom BPE tokenizer** from scratch on a domain-specific corpus, enabling it to learn and correctly represent your unique vocabulary.\n",
    "-   **How to measure tokenizer quality** using the Out-of-Vocabulary (OOV) rate, giving you a quantitative way to assess its performance.\n",
    "-   **How to package and save a custom tokenizer** in the standard Hugging Face format, making it easy to share, version, and load with `AutoTokenizer`.\n",
    "-   **How to control padding and truncation**, essential techniques for handling batches of text for both training and inference.\n",
    "\n",
    "You are now equipped with one of the most powerful techniques for adapting language models to new domains.\n",
    "\n",
    "In the next notebook, we will shift our focus from the data to the model itself and explore the art and science of **Prompt Engineering**. You will learn how to craft effective prompts to control the behavior and output of large language models for a variety of tasks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
