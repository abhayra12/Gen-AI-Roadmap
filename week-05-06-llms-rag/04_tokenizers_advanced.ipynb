{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1852aa02",
   "metadata": {},
   "source": [
    "# üß© Week 5-6 ¬∑ Notebook 04 ¬∑ Advanced Tokenizers\n",
    "\n",
    "Tokenization is the bridge between messy shop-floor text and model-friendly inputs. We will benchmark off-the-shelf tokenizers, build a domain-specific vocabulary, and wire the results into downstream RAG & prompt workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2dd60e",
   "metadata": {},
   "source": [
    "## üéØ Learning Outcomes\n",
    "- Diagnose how different tokenizers handle manufacturing jargon, units, and multilingual snippets.\n",
    "- Train a SentencePiece/BPE tokenizer on plant maintenance logs and export artifacts.\n",
    "- Measure vocabulary coverage and OOV (out-of-vocabulary) risk across data sources.\n",
    "- Configure padding, truncation, and special tokens for real-time vs. batch inference.\n",
    "- Package the tokenizer for use with HuggingFace `AutoTokenizer` and vector stores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b054e2",
   "metadata": {},
   "source": [
    "## üè≠ Manufacturing Text Fingerprints\n",
    "| Corpus | Example | Avg Tokens (WordPiece) | Notes |\n",
    "| --- | --- | --- | --- |\n",
    "| Shift handover | `Press 12 coolant alarm cleared, watch valve drift.` | 32 | Dense with component IDs |\n",
    "| Maintenance ticket | `Lathe #4 vibration 12 mm/s despite new bearing.` | 44 | Units + shorthand |\n",
    "| SOP snippet | `Lockout-tagout for turret press v3.2` | 65 | Mix of legal + procedural text |\n",
    "| Supplier email (es/en) | `Favor revisar torque 450 Nm en lote 18.` | 54 | Multilingual & informal |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117ff7c8",
   "metadata": {},
   "source": [
    "## üß† Tokenizer Families at a Glance\n",
    "| Family | Examples | Strengths | Watch-outs |\n",
    "| --- | --- | --- | --- |\n",
    "| Byte-Pair Encoding (BPE) | GPT-2, LLaMA | Robust on rare words, compact vocab | Sensitive to casing; may split units awkwardly |\n",
    "| WordPiece | BERT, RoBERTa | Balanced vocabulary, good for classification | Requires pre-tokenization; bigger vocab |\n",
    "| SentencePiece (BPE/Unigram) | T5, mT5 | Language-agnostic, trains on raw text | Needs proper normalization config |\n",
    "| Char / Byte | CANINE, ByT5 | Zero OOV | Longer sequences; higher compute |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1dd659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "terms = [\n",
    "    'Hydroforming pressure calibration check',\n",
    "    'OEE dropped to 71% after unplanned downtime',\n",
    "    'Favor revisar torque 450 Nm en lote 18',\n",
    "    'Robot axis-3 grease refill overdue per SOP-442',\n",
    "]\n",
    "\n",
    "tokenizers = {\n",
    "    'GPT-2 BPE': AutoTokenizer.from_pretrained('gpt2'),\n",
    "    'BERT WordPiece': AutoTokenizer.from_pretrained('bert-base-uncased'),\n",
    "    'Longformer': AutoTokenizer.from_pretrained('allenai/longformer-base-4096'),\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for label, tok in tokenizers.items():\n",
    "    for text in terms:\n",
    "        pieces = tok.tokenize(text)\n",
    "        rows.append({'tokenizer': label, 'text': text, 'token_count': len(pieces), 'tokens': pieces})\n",
    "\n",
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c331e92",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- GPT-2 BPE breaks units like `Nm` into `['N', 'm']`, inflating token count.\n",
    "- WordPiece preserves many units but lowercases machine IDs.\n",
    "- Longformer shares BERT's vocabulary yet supports 4k token windows for SOP ingestion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1adb0c",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Training a Domain Tokenizer (SentencePiece BPE)\n",
    "We'll train on a curated sample of maintenance logs. In production, feed thousands of tickets to stabilize merge statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6932476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "maintenance_logs = [\n",
    "    'Press 24 hydraulic accumulator leak detected 03:14.',\n",
    "    'Torque wrench calibration overdue for cell B; schedule before shift 2.',\n",
    "    'Robot cell 3 axis-2 grease refill triggered due to temperature rise.',\n",
    "    'Lathe #4 vibration 12 mm/s despite new bearing install.',\n",
    "    'Favor revisar torque 450 Nm en lote 18 y reportar a calidad.',\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token='[UNK]'))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=600,\n",
    "    min_frequency=1,\n",
    "    special_tokens=['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '<SHIFT>', '<ALERT>']\n",
    ")\n",
    "\n",
    "tokenizer.train_from_iterator(maintenance_logs, trainer=trainer)\n",
    "artifacts_dir = Path('artifacts/tokenizer')\n",
    "artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
    "tokenizer.save(str(artifacts_dir / 'maintenance-bpe.json'))\n",
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0941c04d",
   "metadata": {},
   "source": [
    "### Coverage Audit\n",
    "Measure the share of tokens that become `[UNK]` when using a base vs. custom tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e55f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "baseline_tok = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "custom_tok = Tokenizer.from_file(str(artifacts_dir / 'maintenance-bpe.json'))\n",
    "\n",
    "def oov_ratio(tokenizer, logs):\n",
    "    if hasattr(tokenizer, 'tokenize'):\n",
    "        total = 0\n",
    "        oov = 0\n",
    "        for text in logs:\n",
    "            tokens = tokenizer.tokenize(text)\n",
    "            total += len(tokens)\n",
    "            oov += sum(1 for tok in tokens if tok == '[UNK]')\n",
    "        return oov / total if total else 0.0\n",
    "    encoding = tokenizer.encode(logs[0])\n",
    "    unk_token = tokenizer.token_to_id('[UNK]')\n",
    "    total = 0\n",
    "    oov = 0\n",
    "    for text in logs:\n",
    "        encoding = tokenizer.encode(text)\n",
    "        total += len(encoding.tokens)\n",
    "        oov += sum(1 for tok in encoding.tokens if tokenizer.token_to_id(tok) == unk_token)\n",
    "    return oov / total if total else 0.0\n",
    "\n",
    "baseline_oov = oov_ratio(baseline_tok, maintenance_logs)\n",
    "custom_oov = oov_ratio(custom_tok, maintenance_logs)\n",
    "{'bert_wordpiece_oov': round(baseline_oov, 4), 'custom_bpe_oov': round(custom_oov, 4)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46976a44",
   "metadata": {},
   "source": [
    "Expect the custom tokenizer to reduce `[UNK]` occurrences, especially for bilingual logs and unit notation. Record these metrics in your experiment tracker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479ad08b",
   "metadata": {},
   "source": [
    "## üì¶ Packaging for HuggingFace & Vector Stores\n",
    "The HuggingFace ecosystem expects separate `vocab.json` and `merges.txt` for BPE-based tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4bb6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import pre_tokenizers\n",
    "\n",
    "# Configure whitespace split for downstream compatibility\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "# Save vocab and merges files expected by HF AutoTokenizer\n",
    "files = tokenizer.model.save(str(artifacts_dir), 'maintenance-bpe')\n",
    "\n",
    "hf_config = {\n",
    "    'model_type': 'bpe',\n",
    "    'unk_token': '[UNK]',\n",
    "    'pad_token': '[PAD]',\n",
    "    'cls_token': '[CLS]',\n",
    "    'sep_token': '[SEP]',\n",
    "    'mask_token': '[MASK]'\n",
    "}\n",
    "\n",
    "with open(artifacts_dir / 'tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(tokenizer.to_str())\n",
    "\n",
    "with open(artifacts_dir / 'tokenizer_config.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(hf_config, f, indent=2)\n",
    "\n",
    "sorted(str(path) for path in artifacts_dir.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0b2acd",
   "metadata": {},
   "source": [
    "Load with `AutoTokenizer.from_pretrained('artifacts/tokenizer')` and reuse in RAG pipelines (Notebook 08) so chunking stays aligned with vector embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b600ec",
   "metadata": {},
   "source": [
    "## ü™Ñ Padding, Truncation & Special Tokens\n",
    "### Dynamic vs. Static Padding\n",
    "- **Dynamic (`padding=True`)**: Ideal for online APIs; minimizes wasted compute.\n",
    "- **Static (`padding='max_length'`)**: Useful for batching jobs on GPU/TPU where uniform shapes matter.\n",
    "\n",
    "### Truncation Strategies\n",
    "1. **Front truncation**: Keep latest sentences for incident response.\n",
    "2. **Sliding window with overlap**: Maintain continuity for SOPs (feeds vector store chunks).\n",
    "3. **Hierarchy-aware trimming**: Preserve safety warnings before product specs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c92fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = [\n",
    "    'Shift 1: verify coolant pressure before restart.',\n",
    "    'Alert: axis-3 vibration exceeded 9 mm/s threshold.',\n",
    "    'Favor revisar torque 450 Nm en lote 18.'\n",
    "]\n",
    "\n",
    "encoded_dynamic = baseline_tok(sample_batch, padding=True, return_tensors='pt')\n",
    "encoded_static = baseline_tok(sample_batch, padding='max_length', max_length=24, return_tensors='pt')\n",
    "\n",
    "encoded_dynamic['input_ids'].shape, encoded_static['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f1857c",
   "metadata": {},
   "source": [
    "## üîÑ Integrating with Embedding Pipelines\n",
    "- Reuse the same tokenizer as your embedding model when possible to prevent semantic drift.\n",
    "- Map chunk IDs to vector store metadata so original tokens can be reconstructed for audit.\n",
    "- Align chunk overlap with vector dimensionality (Notebook 09 dives deeper into embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373a920b",
   "metadata": {},
   "source": [
    "## üß™ Lab ¬∑ Tokenizer Bake-Off\n",
    "1. Export the custom tokenizer artifacts and push to an internal Git repository.\n",
    "2. Evaluate on 500 historical tickets: measure token count reduction vs. baseline.\n",
    "3. Compute API latency savings given shorter sequences (reuse Notebook 02 pipelines).\n",
    "4. Present a decision memo recommending tokenizer + padding strategy for production rollout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1206d52e",
   "metadata": {},
   "source": [
    "## ‚úÖ Implementation Checklist\n",
    "- [ ] Tokenizer comparison table logged with metrics\n",
    "- [ ] Custom tokenizer artifacts versioned in source control\n",
    "- [ ] OOV ratio benchmarked across top corpora\n",
    "- [ ] Padding & truncation strategy documented with latency trade-offs\n",
    "- [ ] Integration test covering `AutoTokenizer.from_pretrained('artifacts/tokenizer')`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b94ad0f",
   "metadata": {},
   "source": [
    "## üìö References\n",
    "- HuggingFace Tokenizers documentation\n",
    "- *Efficient Tokenization for Industrial NLP*, ABB Research (2024)\n",
    "- SentencePiece project: https://github.com/google/sentencepiece\n",
    "- Notebook 09 ¬∑ Vector Embeddings (for downstream chunking)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
