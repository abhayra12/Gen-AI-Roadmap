{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f33fa598",
   "metadata": {},
   "source": [
    "# üè≠ Week 5-6 ¬∑ Notebook 03 ¬∑ Model Selection & Preprocessing for Manufacturing\n",
    "\n",
    "**Module:** LLMs, Prompt Engineering & RAG  \n",
    "**Project:** Build the Knowledge Core for the Manufacturing Copilot\n",
    "\n",
    "---\n",
    "\n",
    "Choosing the right Large Language Model and preparing your data correctly are critical first steps in any NLP project. In a manufacturing environment, this is especially true due to the unique nature of the data: technical jargon, machine IDs, sensor readings, and safety-critical information. This notebook provides a framework for selecting a model and preprocessing your data for tasks like classification and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b6624f",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. ‚úÖ **Create a Model Selection Rubric:** Evaluate LLMs based on criteria relevant to manufacturing (accuracy, latency, cost, privacy).\n",
    "2. ‚úÖ **Understand Tokenization:** See how different tokenizers handle domain-specific text (e.g., `CNC-12`, `500-PSI`).\n",
    "3. ‚úÖ **Perform Text Preprocessing:** Prepare text for a model by tokenizing it and converting it to tensors.\n",
    "4. ‚úÖ **Build a Simple Classifier:** Use a pre-trained model to classify maintenance tickets into categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da7b343",
   "metadata": {},
   "source": [
    "## üìä A Framework for Model Selection\n",
    "\n",
    "Not all models are created equal. A massive model like GPT-4 is powerful but might be too slow, expensive, or risky for your use case. A smaller, fine-tuned model might be better. Use a rubric to make a data-driven decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21783e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define a rubric to score candidate models\n",
    "rubric_data = {\n",
    "    'Model': ['BERT-base', 'DistilBERT', 'RoBERTa-large', 'On-prem Llama-2 7B'],\n",
    "    'Accuracy (Score 1-5)': [3, 3, 4, 4], # General pre-trained accuracy\n",
    "    'Latency (Score 1-5)': [4, 5, 2, 3], # 5 is fastest\n",
    "    'Cost (Score 1-5)': [5, 5, 4, 2], # 5 is cheapest\n",
    "    'Privacy (Score 1-5)': [5, 5, 5, 5], # All can be run on-prem\n",
    "    'Fine-tuning Ease (Score 1-5)': [4, 4, 3, 2] # 5 is easiest\n",
    "}\n",
    "\n",
    "rubric_df = pd.DataFrame(rubric_data)\n",
    "\n",
    "# Define weights for each criterion\n",
    "weights = {\n",
    "    'Accuracy (Score 1-5)': 0.30,\n",
    "    'Latency (Score 1-5)': 0.25,\n",
    "    'Cost (Score 1-5)': 0.20,\n",
    "    'Privacy (Score 1-5)': 0.15,\n",
    "    'Fine-tuning Ease (Score 1-5)': 0.10\n",
    "}\n",
    "\n",
    "# Calculate a weighted score\n",
    "score = sum(rubric_df[col] * weight for col, weight in weights.items())\n",
    "rubric_df['Weighted_Score'] = score\n",
    "\n",
    "print(\"--- Model Selection Rubric ---\")\n",
    "rubric_df.sort_values('Weighted_Score', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1833b988",
   "metadata": {},
   "source": [
    "**Conclusion:** For a balanced prototype, `DistilBERT` often wins due to its excellent balance of speed and performance. For highest accuracy, a larger model like `RoBERTa` or a fine-tuned `Llama-2` might be better, but with higher costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce855d3f",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Preprocessing: The Tokenizer\n",
    "\n",
    "Models don't understand words; they understand numbers. A **tokenizer** converts your text into a sequence of numerical IDs that correspond to tokens in the model's vocabulary.\n",
    "\n",
    "The choice of tokenizer is important, as it affects how domain-specific terms are handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8040c168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Let's use a standard tokenizer for DistilBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "manufacturing_text = \"Incident on CNC-12: pressure dropped to 5 PSI.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(manufacturing_text)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(f\"Original Text: {manufacturing_text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "\n",
    "# Notice how 'CNC-12' is split into 'cn', '##c', '-', '12'. \n",
    "# This is common for specialized terms not in the base vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bac159",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Building a Simple Maintenance Ticket Classifier\n",
    "\n",
    "Let's build a simple classifier to categorize maintenance tickets. This is a common first step for routing issues to the correct team.\n",
    "\n",
    "**Task:** Classify a ticket as `Mechanical`, `Electrical`, or `Software`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874e0413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Define our labels\n",
    "labels = [\"Mechanical\", \"Electrical\", \"Software\"]\n",
    "id2label = {i: label for i, label in enumerate(labels)}\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "\n",
    "# Load a pre-trained model, configured for our 3 labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", \n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# A new maintenance ticket arrives\n",
    "new_ticket = \"The main conveyor belt is slipping and making a loud grinding noise.\"\n",
    "\n",
    "# 1. Preprocess the text with the tokenizer\n",
    "inputs = tokenizer(new_ticket, return_tensors=\"pt\")\n",
    "\n",
    "# 2. Feed the inputs to the model\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# 3. Interpret the output\n",
    "predicted_class_id = logits.argmax().item()\n",
    "predicted_label = model.config.id2label[predicted_class_id]\n",
    "\n",
    "print(f\"Ticket: '{new_ticket}'\")\n",
    "print(f\"Predicted Category: {predicted_label}\")\n",
    "\n",
    "# Note: The prediction is essentially random because the pre-trained model hasn't been\n",
    "# fine-tuned on our specific labels. The goal here is to show the end-to-end workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da352e98",
   "metadata": {},
   "source": [
    "## üìè Handling Long Documents: Context Windows\n",
    "\n",
    "Most transformer models have a **maximum context window**‚Äîa limit on the number of tokens they can process at once (e.g., 512 for BERT, 4096 for Longformer).\n",
    "\n",
    "If you feed a document that's too long, the tokenizer will truncate it, losing information. It's important to check document lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdae8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_sop = \"\"\"Standard Operating Procedure for the Hydraulic Press #H-75. Section 1: Safety. Always ensure the machine is in a full stop and de-energized state before performing maintenance. Use lockout-tagout procedures as documented in SOP-GEN-001. Section 2: Weekly Maintenance. Check hydraulic fluid levels and top off if below the minimum line. Inspect for any visible leaks around the main cylinder and hoses. Section 3: Annual Maintenance. Replace the primary hydraulic filter (Part #HF-2045). Send a fluid sample for analysis. Check and re-torque all mounting bolts to 250 ft-lbs.\"\"\" * 5 # Make it longer\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") # Max length 512\n",
    "longformer_tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\") # Max length 4096\n",
    "\n",
    "bert_tokens = bert_tokenizer(long_sop, return_tensors=\"pt\")['input_ids']\n",
    "longformer_tokens = longformer_tokenizer(long_sop, return_tensors=\"pt\")['input_ids']\n",
    "\n",
    "print(f\"Original document length: {len(long_sop)} characters\")\n",
    "print(f\"Tokens (BERT): {bert_tokens.shape[1]}\")\n",
    "print(f\"Tokens (Longformer): {longformer_tokens.shape[1]}\")\n",
    "\n",
    "print(f\"\\nFits in BERT's 512 context window? {'Yes' if bert_tokens.shape[1] <= 512 else 'No'}\")\n",
    "print(f\"Fits in Longformer's 4096 context window? {'Yes' if longformer_tokens.shape[1] <= 4096 else 'No'}\")\n",
    "\n",
    "# For documents longer than the context window, you need a strategy like chunking (see RAG notebooks)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f08c92",
   "metadata": {},
   "source": [
    "## ‚úÖ Next Steps\n",
    "\n",
    "This notebook covered the essential first steps for any applied NLP project: choosing the right model and preparing your data. You've learned:\n",
    "\n",
    "- How to evaluate models systematically for a specific use case.\n",
    "- The role of the tokenizer and how it handles domain-specific text.\n",
    "- The end-to-end workflow for a simple classification task.\n",
    "- The importance of considering context window limits.\n",
    "\n",
    "In the next notebook, we will dive into the art of **tokenization** in more detail, including how to train your own custom tokenizer to better understand your manufacturing vocabulary."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
