{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f33fa598",
   "metadata": {},
   "source": [
    "# üè≠ Week 5-6, Notebook 3: Model Selection & Data Preprocessing\n",
    "\n",
    "**Module:** LLMs, Prompt Engineering & RAG  \n",
    "**Project:** Build the Knowledge Core for the Manufacturing Copilot\n",
    "\n",
    "---\n",
    "\n",
    "Choosing the right Large Language Model and preparing your data correctly are two of the most critical decisions you will make in any applied NLP project. This is especially true in a specialized domain like manufacturing, where the language is filled with technical jargon, machine IDs, sensor readings, and safety-critical information.\n",
    "\n",
    "This notebook provides a structured framework for:\n",
    "1.  **Model Selection:** How to choose the best model for your specific task by balancing performance, speed, cost, and privacy.\n",
    "2.  **Data Preprocessing:** The essential steps of turning raw text into a format that a model can understand‚Äîa process known as tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b6624f",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1.  **Create a Model Selection Rubric:** Systematically evaluate and compare different LLMs based on criteria crucial for manufacturing, such as accuracy, latency, cost, and data privacy.\n",
    "2.  **Understand Tokenization in Depth:** Analyze how different tokenizers handle domain-specific text (e.g., `CNC-12`, `500-PSI`, `Part#A34-Z2`) and the impact this has on performance.\n",
    "3.  **Perform Text Preprocessing:** Master the workflow of preparing raw text for a model by tokenizing it and converting it into the numerical tensors the model expects.\n",
    "4.  **Build a Foundational Classifier:** Implement an end-to-end workflow for a simple text classifier, demonstrating how to use a pre-trained model for a custom task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da7b343",
   "metadata": {},
   "source": [
    "## üìä Part 1: A Framework for Model Selection\n",
    "\n",
    "Not all models are created equal. A massive, 70-billion-parameter model is incredibly powerful but might be too slow, expensive, or insecure for your specific use case. Conversely, a small, fast model might not be accurate enough for a critical task.\n",
    "\n",
    "To make a data-driven decision, we can use a **selection rubric**. This is a simple but powerful tool for scoring candidate models against the criteria that matter most to your project.\n",
    "\n",
    "**Key Criteria for Manufacturing:**\n",
    "\n",
    "*   **Accuracy:** How well does the model perform the target task (e.g., classification accuracy, summarization quality)?\n",
    "*   **Latency:** How quickly does the model produce a response? Critical for real-time applications.\n",
    "*   **Cost:** What is the hardware and operational cost? Includes GPU requirements and API fees.\n",
    "*   **Privacy/Security:** Can the model be run on-premises (`on-prem`) to protect sensitive company data?\n",
    "*   **Fine-tuning Ease:** How difficult is it to adapt the model to your specific data and terminology?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21783e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hands-On: Building a Model Selection Rubric\n",
    "import pandas as pd\n",
    "\n",
    "# Define a rubric to score candidate models for a maintenance ticket classification task.\n",
    "# Scores are on a 1-5 scale, where 5 is best.\n",
    "rubric_data = {\n",
    "    'Model': ['BERT-base-uncased', 'DistilBERT-base-uncased', 'RoBERTa-large', 'On-prem Llama-3-8B'],\n",
    "    'Accuracy (General)': [3, 3, 4, 5],  # General pre-trained accuracy on standard benchmarks\n",
    "    'Latency (Speed)': [4, 5, 2, 3],     # 5 = fastest inference\n",
    "    'Cost (Hardware/API)': [5, 5, 4, 2], # 5 = cheapest to run\n",
    "    'Privacy (On-Prem)': [5, 5, 5, 5],   # All these models can be run on-premises\n",
    "    'Fine-Tuning Ease': [4, 4, 3, 2]      # 5 = easiest to fine-tune\n",
    "}\n",
    "\n",
    "rubric_df = pd.DataFrame(rubric_data)\n",
    "\n",
    "# Define the weights for each criterion based on project priorities.\n",
    "# For a prototype, we might prioritize speed and cost.\n",
    "weights = {\n",
    "    'Accuracy (General)': 0.30,\n",
    "    'Latency (Speed)': 0.25,\n",
    "    'Cost (Hardware/API)': 0.20,\n",
    "    'Privacy (On-Prem)': 0.15,\n",
    "    'Fine-Tuning Ease': 0.10\n",
    "}\n",
    "\n",
    "# Calculate a weighted score for each model\n",
    "# Note: .iloc[:, 1:] selects all rows and all columns from the second column onward.\n",
    "weighted_scores = (rubric_df.iloc[:, 1:] * list(weights.values())).sum(axis=1)\n",
    "rubric_df['Weighted_Score'] = weighted_scores\n",
    "\n",
    "print(\"--- Model Selection Rubric ---\")\n",
    "# Display the results sorted by the final score\n",
    "rubric_df.sort_values('Weighted_Score', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1833b988",
   "metadata": {},
   "source": [
    "**Conclusion:** Based on this rubric, `DistilBERT-base-uncased` emerges as the top candidate for a balanced prototype. It offers the best latency and cost while maintaining good enough accuracy and fine-tuning potential. For a final production system where accuracy is paramount, `RoBERTa-large` or a fine-tuned `Llama-3-8B` might be a better choice, but they come with significantly higher computational costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce855d3f",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Part 2: Preprocessing with the Tokenizer\n",
    "\n",
    "Models don't understand words; they understand numbers. A **tokenizer** is the critical component that bridges this gap. It converts your raw text into a sequence of numerical IDs that correspond to tokens in the model's vocabulary.\n",
    "\n",
    "The choice of tokenizer is just as important as the choice of model. The tokenizer must match the model it was trained with. Using the wrong tokenizer will result in poor performance because the numerical IDs will not align with what the model expects.\n",
    "\n",
    "Let's see how a standard tokenizer handles the specialized vocabulary found in a manufacturing environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8040c168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hands-On: Exploring Tokenization on Manufacturing Data\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Let's load the tokenizer for our chosen model, DistilBERT.\n",
    "# The \"uncased\" part means the tokenizer converts all text to lowercase.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# A typical sentence from a maintenance log\n",
    "manufacturing_text = \"Incident on CNC-12: pressure dropped to 5 PSI and triggered alarm A-45.\"\n",
    "\n",
    "# The .tokenize() method splits the text into a list of token strings.\n",
    "tokens = tokenizer.tokenize(manufacturing_text)\n",
    "\n",
    "# The .encode() method performs the full pipeline: tokenization and conversion to numerical IDs.\n",
    "token_ids = tokenizer.encode(manufacturing_text)\n",
    "\n",
    "print(f\"Original Text: '{manufacturing_text}'\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "\n",
    "# --- Analysis ---\n",
    "# Notice how the tokenizer handles the domain-specific terms:\n",
    "# - \"cnc-12\" is split into ['cn', '##c', '-', '12'].\n",
    "# - \"psi\" is a single token: ['psi'].\n",
    "# - \"a-45\" is split into ['a', '-', '45'].\n",
    "# The \"##\" prefix indicates that the token is a subword and should be attached to the previous token.\n",
    "\n",
    "# This splitting of unknown words is a key feature of subword tokenization, but it can\n",
    "# sometimes hurt performance on highly specialized vocabularies. We will address this\n",
    "# in the next notebook by training a custom tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bac159",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Part 3: Building a Simple Maintenance Ticket Classifier\n",
    "\n",
    "Now, let's put everything together to build a simple classifier. This is a common and highly valuable task, as it can automatically route maintenance tickets to the correct team (e.g., Mechanical, Electrical, or Software).\n",
    "\n",
    "**The Goal:** Classify a new maintenance ticket into one of three categories.\n",
    "\n",
    "**The Workflow:**\n",
    "1.  **Load a pre-trained model:** We'll use `DistilBERT`, but we need to tell it that we want to use it for a classification task with 3 specific labels.\n",
    "2.  **Prepare the input:** We take a new ticket (raw text) and use our tokenizer to convert it into the format the model expects (input IDs, attention mask, etc.).\n",
    "3.  **Feed the input to the model:** We pass the tokenized input through the model to get its raw output, known as **logits**.\n",
    "4.  **Interpret the output:** We convert the logits into a predicted label.\n",
    "\n",
    "**Important Note:** The model we are using has been pre-trained on a massive amount of general text, but it has **not** been fine-tuned on our specific maintenance labels. Therefore, its predictions will be essentially random. The purpose of this exercise is to demonstrate the complete, end-to-end workflow, which is the foundation for fine-tuning later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874e0413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hands-On: End-to-End Classification Workflow\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# --- Step 1: Define Labels and Load Model ---\n",
    "\n",
    "# Define the categories for our classification task\n",
    "labels = [\"Mechanical\", \"Electrical\", \"Software\"]\n",
    "id2label = {i: label for i, label in enumerate(labels)}\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "\n",
    "# Load the DistilBERT model, but configure it for sequence classification with our 3 labels.\n",
    "# This adds a new, randomly initialized classification \"head\" on top of the pre-trained model.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# --- Step 2: Prepare Input Data ---\n",
    "\n",
    "# A new maintenance ticket arrives\n",
    "new_ticket = \"The main conveyor belt is slipping off its track and making a loud grinding noise near motor M-17.\"\n",
    "\n",
    "# Tokenize the text. `return_tensors=\"pt\"` tells the tokenizer to output PyTorch tensors.\n",
    "inputs = tokenizer(new_ticket, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "print(f\"Ticket: '{new_ticket}'\\n\")\n",
    "print(f\"Tokenizer Output (Input IDs): {inputs['input_ids']}\\n\")\n",
    "\n",
    "# --- Step 3: Get Model Prediction ---\n",
    "\n",
    "# We use `torch.no_grad()` to disable gradient calculations, as we are only doing inference, not training.\n",
    "# This makes the process faster and uses less memory.\n",
    "with torch.no_grad():\n",
    "    # The model returns a dictionary. The raw, unnormalized scores are in the 'logits' key.\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "print(f\"Model Logits: {logits}\\n\")\n",
    "\n",
    "# --- Step 4: Interpret the Results ---\n",
    "\n",
    "# The logits are the raw scores for each class. To get a probability, you can apply a softmax function.\n",
    "probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "print(f\"Probabilities: {probabilities.numpy()}\\n\")\n",
    "\n",
    "# To get the final predicted label, we find the class with the highest logit/probability.\n",
    "predicted_class_id = logits.argmax().item()\n",
    "predicted_label = model.config.id2label[predicted_class_id]\n",
    "\n",
    "print(f\"--> Predicted Category: {predicted_label}\")\n",
    "print(\"\\nNote: This prediction is from an untrained classification head. It will become accurate after fine-tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da352e98",
   "metadata": {},
   "source": [
    "## üìè Part 4: Handling Long Documents and Context Windows\n",
    "\n",
    "A critical limitation of most Transformer models is their **maximum context window**. This is the fixed number of tokens the model can process at one time. For example:\n",
    "*   `BERT-base`: 512 tokens\n",
    "*   `Llama-3-8B`: 8,192 tokens\n",
    "*   `GPT-4-Turbo`: 128,000 tokens\n",
    "\n",
    "If you provide a document that is longer than the model's context window, the tokenizer will **truncate** it by default, cutting off the end of the document. This can lead to the loss of crucial information.\n",
    "\n",
    "It is essential to be aware of your model's context window and have a strategy for handling documents that exceed it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdae8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hands-On: Checking Document Length Against Context Windows\n",
    "# A long Standard Operating Procedure (SOP) document\n",
    "long_sop = \"\"\"\n",
    "Standard Operating Procedure for the Hydraulic Press #H-75.\n",
    "Section 1: Safety Protocols. Before any maintenance, ensure the machine is in a full stop and de-energized state. All personnel must follow the lockout-tagout procedures as documented in SOP-GEN-001. Personal Protective Equipment (PPE), including safety glasses and steel-toed boots, is mandatory within a 10-foot radius of the machine.\n",
    "Section 2: Weekly Maintenance Checklist. 1. Check hydraulic fluid levels and top off if below the minimum fill line. Use only approved fluid type H-45. 2. Inspect for any visible leaks around the main cylinder, hoses, and fittings. Report any leaks immediately to the shift supervisor. 3. Clean debris from the base of the press.\n",
    "Section 3: Annual Maintenance and Calibration. 1. Replace the primary hydraulic filter (Part #HF-2045-B). 2. Send a sample of the hydraulic fluid to an external lab for analysis of particulate matter and viscosity. 3. Check and re-torque all main frame mounting bolts to the specified 250 ft-lbs. 4. Calibrate the pressure sensor using the Fluke 700G Precision Pressure Gauge Calibrator.\n",
    "\"\"\" * 3 # Repeat the text to make it longer\n",
    "\n",
    "# Load tokenizers for models with different context window sizes\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # Max length: 512\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\") # Max length: 8192\n",
    "\n",
    "# Tokenize the long document with each tokenizer\n",
    "bert_tokens = bert_tokenizer(long_sop, return_tensors=\"pt\")['input_ids']\n",
    "llama_tokens = llama_tokenizer(long_sop, return_tensors=\"pt\")['input_ids']\n",
    "\n",
    "print(f\"Original document length: {len(long_sop.split())} words\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Tokens (BERT tokenizer): {bert_tokens.shape[1]}\")\n",
    "print(f\"Tokens (Llama-3 tokenizer): {llama_tokens.shape[1]}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Check if the document fits within each model's context window\n",
    "print(f\"Fits in BERT's 512 context window? {'Yes' if bert_tokens.shape[1] <= 512 else 'No'}\")\n",
    "print(f\"Fits in Llama-3's 8192 context window? {'Yes' if llama_tokens.shape[1] <= 8192 else 'No'}\")\n",
    "\n",
    "# For documents that are too long, you must implement a chunking strategy.\n",
    "# This is a core concept in Retrieval-Augmented Generation (RAG), which we will cover in detail later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f08c92",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary and Next Steps\n",
    "\n",
    "This notebook covered the essential foundational steps for any applied NLP project: choosing the right model and preparing your data. You have learned:\n",
    "\n",
    "-   **How to Evaluate Models Systematically:** You used a weighted rubric to compare different models based on criteria like accuracy, speed, and cost, allowing you to make a data-driven choice for your specific use case.\n",
    "-   **The Role of the Tokenizer:** You saw how a tokenizer converts text into numerical IDs and how it handles domain-specific vocabulary, including the use of subword tokens for unknown terms.\n",
    "-   **The End-to-End Classification Workflow:** You implemented the full process of taking raw text, tokenizing it, feeding it to a model, and interpreting the output for a classification task.\n",
    "-   **The Importance of Context Windows:** You learned about the token limits of different models and why it's crucial to handle long documents to avoid information loss through truncation.\n",
    "\n",
    "In the next notebook, we will dive even deeper into the art of **tokenization**. You will learn about different tokenization algorithms and how to **train your own custom tokenizer** to better understand the unique vocabulary of your manufacturing environment, which is a key step toward improving model performance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
