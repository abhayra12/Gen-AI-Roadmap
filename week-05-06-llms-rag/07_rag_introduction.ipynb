{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a303e3fe",
   "metadata": {},
   "source": [
    "# üß† Week 5-6 ¬∑ Notebook 07 ¬∑ Retrieval-Augmented Generation (RAG) Foundations\n",
    "\n",
    "**Module:** LLMs, Prompt Engineering & RAG  \n",
    "**Project:** Build the Knowledge Core for the Manufacturing Copilot\n",
    "\n",
    "---\n",
    "\n",
    "Large Language Models have a knowledge cut-off and are not aware of your private, domain-specific data. **Retrieval-Augmented Generation (RAG)** solves this problem. It's a technique that connects an LLM to an external knowledge base, allowing it to answer questions by first *retrieving* relevant documents and then *generating* an answer based on them. This is the core technology for our Manufacturing Copilot's knowledge capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391bd661",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. ‚úÖ **Explain the RAG Architecture:** Describe the key components of a RAG pipeline (Ingest, Embed, Retrieve, Generate).\n",
    "2. ‚úÖ **Compare RAG vs. Fine-Tuning:** Understand the trade-offs between RAG and fine-tuning for different manufacturing scenarios.\n",
    "3. ‚úÖ **Build a Simple RAG System:** Implement a basic RAG pipeline from scratch using Hugging Face and sentence transformers.\n",
    "4. ‚úÖ **Identify RAG's Risks and Mitigations:** Recognize common failure modes and how to address them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ed0e90",
   "metadata": {},
   "source": [
    "## üèóÔ∏è The RAG Pipeline: A High-Level View\n",
    "\n",
    "A RAG system has two main stages:\n",
    "\n",
    "1.  **Offline Indexing (The \"Library\")**: This is where you prepare your knowledge base. It's done once and updated periodically.\n",
    "    - **Load & Chunk:** Documents (PDFs, text files, etc.) are loaded and split into smaller, manageable chunks.\n",
    "    - **Embed & Store:** Each chunk is converted into a numerical vector (embedding) and stored in a specialized database called a **Vector Store**.\n",
    "\n",
    "2.  **Online Retrieval & Generation (The \"Librarian\")**: This happens in real-time when a user asks a question.\n",
    "    - **Retrieve:** The user's question is embedded, and the Vector Store finds the most relevant document chunks (the ones with the closest embeddings).\n",
    "    - **Augment & Generate:** The retrieved chunks are added to the user's question in a prompt, and the LLM generates an answer based on this augmented context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffdc2e6",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Building a Mini-RAG System\n",
    "\n",
    "Let's build a small-scale RAG system to see these concepts in action. We'll create a tiny knowledge base of three Standard Operating Procedures (SOPs) for our factory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c68727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure necessary libraries are installed\n",
    "# !pip install -q sentence-transformers pandas transformers\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc2a43e",
   "metadata": {},
   "source": [
    "### Step 1: Prepare the Knowledge Base (Indexing)\n",
    "\n",
    "First, we define our documents and create embeddings for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5549e917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our knowledge base: A few snippets from SOPs\n",
    "knowledge_base = pd.DataFrame([\n",
    "    {\n",
    "        \"doc_id\": \"SOP-HYD-001\",\n",
    "        \"text\": \"For hydraulic press maintenance, first perform lockout-tagout. Then, inspect all seals for leaks, verify pressure gauges read within the 500-550 PSI range, and top off hydraulic fluid if necessary.\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"SOP-CONV-003\",\n",
    "        \"text\": \"To troubleshoot a conveyor belt stoppage, first check for physical obstructions. If clear, verify the motor's thermal overload has not tripped. Finally, inspect belt tension and sensor alignment.\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"SOP-ROBO-002\",\n",
    "        \"text\": \"Preventive maintenance for a robotic arm involves greasing all major joints monthly, recalibrating torque sensors quarterly, and validating vision system alignment weekly.\"\n",
    "    },\n",
    "])\n",
    "\n",
    "# Load a model to create embeddings\n",
    "retriever_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create embeddings for our knowledge base\n",
    "corpus_embeddings = retriever_model.encode(knowledge_base.text.tolist(), convert_to_tensor=True)\n",
    "\n",
    "print(\"Knowledge base indexed successfully!\")\n",
    "print(f\"Shape of corpus embeddings: {corpus_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ede2bcd",
   "metadata": {},
   "source": [
    "### Step 2: Ask a Question (Retrieval & Generation)\n",
    "\n",
    "Now, a user asks a question. We'll retrieve relevant documents and use an LLM to generate an answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16b5da5",
   "metadata": {},
   "source": [
    "# User's question\n",
    "question = \"What are the steps to fix a hydraulic press?\"\n",
    "\n",
    "# --- Retrieval --- #\n",
    "# Embed the user's question\n",
    "query_embedding = retriever_model.encode(question, convert_to_tensor=True)\n",
    "\n",
    "# Find the top-k most similar documents (we'll take k=1 for simplicity)\n",
    "top_k = 1\n",
    "hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=top_k)[0]\n",
    "\n",
    "# Get the retrieved document text\n",
    "retrieved_doc = knowledge_base.iloc[hits[0]['corpus_id']]\n",
    "retrieved_context = retrieved_doc['text']\n",
    "retrieved_doc_id = retrieved_doc['doc_id']\n",
    "\n",
    "print(f\"--- Retrieved Document (ID: {retrieved_doc_id}) ---\")\n",
    "print(retrieved_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bc151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Augment & Generate --- #\n",
    "\n",
    "# Create the prompt, augmenting it with the retrieved context\n",
    "rag_prompt = f\"\"\"You are a helpful manufacturing assistant. Answer the user's question based ONLY on the provided context. Cite the document ID.\n",
    "\n",
    "Context from {retrieved_doc_id}:\n",
    "\"{retrieved_context}\"\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Load a text-generation model\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "generator = pipeline(\n",
    "    'text2text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    max_length=150\n",
    ")\n",
    "\n",
    "# Generate the final answer\n",
    "final_answer = generator(rag_prompt)\n",
    "\n",
    "print(\"\\n--- Final Answer from RAG System ---\")\n",
    "print(final_answer[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df0cb9e",
   "metadata": {},
   "source": [
    "## ü§î RAG vs. Fine-Tuning\n",
    "\n",
    "| Aspect | RAG | Fine-Tuning |\n",
    "| --- | --- | --- |\n",
    "| **Knowledge Source** | External (Vector DB) | Internal (Model Weights) |\n",
    "| **Updating Knowledge** | Easy & Fast (re-index documents) | Hard & Slow (re-train model) |\n",
    "| **Hallucination Risk** | Lower (answers are grounded in retrieved text) | Higher (can invent facts) |\n",
    "| **Best For...** | Fact-based Q&A, knowledge-intensive tasks | Learning a new style, tone, or task format |\n",
    "| **Manufacturing Example** | Answering questions from SOPs | Creating a chatbot that speaks like a senior engineer |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecb3669",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Common Risks & How to Mitigate Them\n",
    "\n",
    "- **Risk: Poor Retrieval:** The wrong documents are retrieved, leading to irrelevant answers.\n",
    "  - **Mitigation:** Use better embedding models, improve document chunking strategy, and clean your source data.\n",
    "- **Risk: Stale Information:** The knowledge base is out of date.\n",
    "  - **Mitigation:** Implement a regular pipeline to re-index documents from their source of truth.\n",
    "- **Risk: Hallucination:** The LLM ignores the retrieved context and makes things up.\n",
    "  - **Mitigation:** Use strong prompts that strictly instruct the model to only use the provided context. Add a check to verify the answer against the source document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704b0a99",
   "metadata": {},
   "source": [
    "## ‚úÖ Next Steps\n",
    "\n",
    "You've now seen the fundamental mechanics of a RAG system. While our example was simple, it demonstrates the power of connecting LLMs to external knowledge.\n",
    "\n",
    "In the next notebook, **`08_rag_implementation.ipynb`**, we will build a more robust RAG pipeline using the LangChain library, which provides helpful abstractions for chunking, retrieval, and prompt management, making it much easier to build production-ready RAG applications."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
