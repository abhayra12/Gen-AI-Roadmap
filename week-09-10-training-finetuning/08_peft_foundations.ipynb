{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f15f4b0b",
   "metadata": {},
   "source": [
    "# üß© Week 09-10 ¬∑ Notebook 08 ¬∑ PEFT Foundations with PEFT Library\n",
    "\n",
    "Introduce adapter-based fine-tuning techniques to upgrade multilingual maintenance assistants under strict compute budgets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0200a3",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "- **Understand PEFT:** Explain the trade-offs between different Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA, prefix-tuning, and prompt tuning.\n",
    "- **Implement LoRA:** Configure and apply LoRA (Low-Rank Adaptation) using the Hugging Face `peft` library to adapt a model for bilingual manufacturing FAQs.\n",
    "- **Measure Performance:** Quantify the impact on latency and memory usage when using adapters compared to full fine-tuning.\n",
    "- **Align with Governance:** Ensure that model updates using PEFT align with the plant's IT change management and maintenance freeze policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43b863b",
   "metadata": {},
   "source": [
    "## üß© Scenario\n",
    "A multilingual plant uses English, Hindi, and Spanish. Leadership wants improved Hindi accuracy without doubling GPU spend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190ecad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f6c3cb",
   "metadata": {},
   "source": [
    "## üìö Synthetic Bilingual FAQ\n",
    "Short Q/A pairs about maintenance tasks. Replace with your plant's bilingual corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9143a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs_data = [\n",
    "    {\"question\": \"How do I check the pressure on the hydraulic press?\", \"answer\": \"Check the main gauge on panel B. It should be between 1800 and 2000 psi.\"},\n",
    "    {\"question\": \"‡§π‡§æ‡§á‡§°‡•ç‡§∞‡•ã‡§≤‡§ø‡§ï ‡§™‡•ç‡§∞‡•á‡§∏ ‡§™‡§∞ ‡§¶‡§¨‡§æ‡§µ ‡§ï‡•Ä ‡§ú‡§æ‡§Ç‡§ö ‡§ï‡•à‡§∏‡•á ‡§ï‡§∞‡•á‡§Ç?\", \"answer\": \"‡§™‡•à‡§®‡§≤ ‡§¨‡•Ä ‡§™‡§∞ ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§ó‡•á‡§ú ‡§ï‡•Ä ‡§ú‡§æ‡§Ç‡§ö ‡§ï‡§∞‡•á‡§Ç‡•§ ‡§Ø‡§π 1800 ‡§î‡§∞ 2000 psi ‡§ï‡•á ‡§¨‡•Ä‡§ö ‡§π‡•ã‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è‡•§\"},\n",
    "    {\"question\": \"What is the torque spec for the main bolts on the CNC machine?\", \"answer\": \"The torque specification is 120 Nm. Use a calibrated torque wrench.\"},\n",
    "    {\"question\": \"‡§∏‡•Ä‡§è‡§®‡§∏‡•Ä ‡§Æ‡§∂‡•Ä‡§® ‡§™‡§∞ ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§¨‡•ã‡§≤‡•ç‡§ü ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ü‡•â‡§∞‡•ç‡§ï ‡§∏‡•ç‡§™‡•á‡§ï ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?\", \"answer\": \"‡§ü‡•â‡§∞‡•ç‡§ï ‡§∏‡•ç‡§™‡•á‡§∏‡§ø‡§´‡§ø‡§ï‡•á‡§∂‡§® 120 Nm ‡§π‡•à‡•§ ‡§ï‡•à‡§≤‡§ø‡§¨‡•ç‡§∞‡•á‡§ü‡•á‡§° ‡§ü‡•â‡§∞‡•ç‡§ï ‡§∞‡§ø‡§Ç‡§ö ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç‡•§\"},\n",
    "    {\"question\": \"Emergency stop procedure for the conveyor belt?\", \"answer\": \"Press the large red button located at the start and end of the line. Do not restart without supervisor approval.\"},\n",
    "    {\"question\": \"‡§ï‡§®‡•ç‡§µ‡•á‡§Ø‡§∞ ‡§¨‡•á‡§≤‡•ç‡§ü ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ü‡§™‡§æ‡§§‡§ï‡§æ‡§≤‡•Ä‡§® ‡§∏‡•ç‡§ü‡•â‡§™ ‡§™‡•ç‡§∞‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ?\", \"answer\": \"‡§≤‡§æ‡§á‡§® ‡§ï‡•Ä ‡§∂‡•Å‡§∞‡•Å‡§Ü‡§§ ‡§î‡§∞ ‡§Ö‡§Ç‡§§ ‡§Æ‡•á‡§Ç ‡§∏‡•ç‡§•‡§ø‡§§ ‡§¨‡§°‡§º‡•á ‡§≤‡§æ‡§≤ ‡§¨‡§ü‡§® ‡§ï‡•ã ‡§¶‡§¨‡§æ‡§è‡§Ç‡•§ ‡§™‡§∞‡•ç‡§Ø‡§µ‡•á‡§ï‡•ç‡§∑‡§ï ‡§ï‡•Ä ‡§Æ‡§Ç‡§ú‡•Ç‡§∞‡•Ä ‡§ï‡•á ‡§¨‡§ø‡§®‡§æ ‡§™‡•Å‡§®‡§∞‡§æ‡§∞‡§Ç‡§≠ ‡§® ‡§ï‡§∞‡•á‡§Ç‡•§\"},\n",
    "    {\"question\": \"How to clean the laser sensor?\", \"answer\": \"Use a microfiber cloth with isopropyl alcohol. Do not apply direct pressure.\"},\n",
    "    {\"question\": \"‡§≤‡•á‡§ú‡§∞ ‡§∏‡•á‡§Ç‡§∏‡§∞ ‡§ï‡•ã ‡§ï‡•à‡§∏‡•á ‡§∏‡§æ‡§´ ‡§ï‡§∞‡•á‡§Ç?\", \"answer\": \"‡§Ü‡§á‡§∏‡•ã‡§™‡•ç‡§∞‡•ã‡§™‡§ø‡§≤ ‡§Ö‡§≤‡•ç‡§ï‡•ã‡§π‡§≤ ‡§ï‡•á ‡§∏‡§æ‡§• ‡§è‡§ï ‡§Æ‡§æ‡§á‡§ï‡•ç‡§∞‡•ã‡§´‡§æ‡§á‡§¨‡§∞ ‡§ï‡§™‡§°‡§º‡•á ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç‡•§ ‡§∏‡•Ä‡§ß‡§æ ‡§¶‡§¨‡§æ‡§µ ‡§® ‡§°‡§æ‡§≤‡•á‡§Ç‡•§\"}\n",
    "]\n",
    "qa_pairs = Dataset.from_list(qa_pairs_data)\n",
    "qa_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0de867",
   "metadata": {},
   "source": [
    "## üßæ Tokenization & Data Collation\n",
    "Use a small instruction-tuned base model (stub)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa988cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a small, instruction-tuned model as our base.\n",
    "# In a real scenario, this might be a larger model like Llama or Mistral.\n",
    "model_name = 'google/flan-t5-small'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e485ea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(batch):\n",
    "    \"\"\"Prepares the data for training by tokenizing questions and answers.\"\"\"\n",
    "    # We format the input as a task-specific prompt\n",
    "    inputs = [f\"Question: {q}\" for q in batch['question']]\n",
    "    \n",
    "    # Tokenize the inputs (questions) and targets (answers)\n",
    "    model_inputs = tokenizer(inputs, text_target=batch['answer'], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_ds = qa_pairs.map(preprocess_function, batched=True)\n",
    "print(f\"Tokenized dataset columns: {tokenized_ds.column_names}\")\n",
    "print(f\"\\nExample input_ids: {tokenized_ds[0]['input_ids']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6548001c",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configure LoRA Adapter\n",
    "Even though this notebook focuses on PEFT overview, we demo LoRA since it balances latency and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b35be71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base model\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank of the update matrices. Lower rank means fewer parameters to train.\n",
    "    lora_alpha=32,  # Alpha is a scaling factor.\n",
    "    target_modules=[\"q\", \"v\"],  # Apply LoRA to the query and value layers of the attention blocks.\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM  # This is a sequence-to-sequence model.\n",
    ")\n",
    "\n",
    "# Create the PEFT model\n",
    "peft_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Print the percentage of trainable parameters\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d763396",
   "metadata": {},
   "source": [
    "## üß™ Training Arguments (Demo)\n",
    "For illustration, run a few steps; in production increase epochs and dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e469378",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./peft-faq-assistant\",\n",
    "    auto_find_batch_size=True, # Automatically find a batch size that fits on the GPU\n",
    "    learning_rate=3e-4,\n",
    "    num_train_epochs=10,\n",
    "    logging_steps=1,\n",
    "    report_to=\"none\", # Disable reporting to services like W&B for this demo\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Start the training\n",
    "# This will only train the adapter layers, not the full model.\n",
    "print(\"--- Starting PEFT Training ---\")\n",
    "# trainer.train() # Uncomment to run fine-tuning (requires a GPU and will take time)\n",
    "print(\"--- (Skipping actual training for this demo) ---\")\n",
    "print(\"--- PEFT Training Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bbb486",
   "metadata": {},
   "source": [
    "## ‚è±Ô∏è Latency & Memory Comparison\n",
    "Measure inference timing before and after adapters (pseudo-benchmark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb5269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_inference(model, tokenizer, prompt, device=\"cpu\"):\n",
    "    \"\"\"A simple function to benchmark latency and memory.\"\"\"\n",
    "    model.to(device)\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Warm-up run to load model onto GPU, etc.\n",
    "    with torch.inference_mode():\n",
    "        _ = model.generate(**inputs, max_new_tokens=50)\n",
    "    \n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # Measure latency\n",
    "    latencies = []\n",
    "    for _ in range(10): # More runs for a stable average\n",
    "        start_time = time.perf_counter()\n",
    "        with torch.inference_mode():\n",
    "            _ = model.generate(**inputs, max_new_tokens=50)\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        end_time = time.perf_counter()\n",
    "        latencies.append((end_time - start_time) * 1000) # Convert to ms\n",
    "    \n",
    "    avg_latency_ms = np.mean(latencies)\n",
    "    \n",
    "    # Measure peak memory usage\n",
    "    peak_memory_mb = torch.cuda.max_memory_allocated(device) / 1e6 if device == \"cuda\" else 0\n",
    "    torch.cuda.reset_peak_memory_stats(device) if device == \"cuda\" else None\n",
    "    \n",
    "    return avg_latency_ms, peak_memory_mb\n",
    "\n",
    "# --- Run Benchmarks ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "prompt = \"Question: What is the torque spec for the main bolts on the CNC machine?\"\n",
    "\n",
    "print(f\"--- Benchmarking on {device} ---\")\n",
    "\n",
    "if device == \"cpu\":\n",
    "    print(\"CUDA not available. Skipping benchmark as it requires a GPU for meaningful comparison.\")\n",
    "else:\n",
    "    # Benchmark Base Model\n",
    "    base_model_latency, base_model_memory = benchmark_inference(base_model, tokenizer, prompt, device)\n",
    "    \n",
    "    # Benchmark PEFT Model\n",
    "    peft_model_latency, peft_model_memory = benchmark_inference(peft_model, tokenizer, prompt, device)\n",
    "\n",
    "    # The memory reported is peak memory during inference, not storage size of weights.\n",
    "    # We'll add the trainable params memory for a complete picture.\n",
    "    base_model_param_mb = base_model.get_memory_footprint() / 1e6\n",
    "    peft_model_param_mb = peft_model.get_memory_footprint() / 1e6\n",
    "    \n",
    "    benchmark_df = pd.DataFrame([\n",
    "        {\"Model\": \"Base Model\", \"Avg Latency (ms)\": f\"{base_model_latency:.2f}\", \"Peak Inference Memory (MB)\": f\"{base_model_memory:.2f}\", \"Model Size (MB)\": f\"{base_model_param_mb:.2f}\"},\n",
    "        {\"Model\": \"PEFT (LoRA)\", \"Avg Latency (ms)\": f\"{peft_model_latency:.2f}\", \"Peak Inference Memory (MB)\": f\"{peft_model_memory:.2f}\", \"Model Size (MB)\": f\"{peft_model_param_mb:.2f}\"}\n",
    "    ])\n",
    "\n",
    "    print(benchmark_df)\n",
    "    print(\"\\\\nNote: PEFT model size is slightly larger due to the added adapter weights, but the key benefit is the tiny size of the *trainable* weights that need to be saved and deployed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2625e6",
   "metadata": {},
   "source": [
    "### üß≠ Maintenance Freeze Checklist\n",
    "- Deploy adapters during scheduled maintenance window (Friday 23:00-02:00).\n",
    "- Back up base model and adapter weights in model registry.\n",
    "- Smoke-test multilingual prompts before shift turnover.\n",
    "- Document change in OT ticketing system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99c9ab3",
   "metadata": {},
   "source": [
    "## üß™ Lab Assignment\n",
    "1. Expand dataset with 50 bilingual Q/A pairs from your plant.\n",
    "2. Train prefix-tuning (`PeftType.PREFIX_TUNING`) and compare accuracy.\n",
    "3. Log GPU memory usage with and without adapters (`torch.cuda.memory_allocated`).\n",
    "4. Produce an adapter release note aligned with IT governance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b7143e",
   "metadata": {},
   "source": [
    "## ‚úÖ Checklist\n",
    "- [ ] Adapter method selected with justification\n",
    "- [ ] Bilingual dataset curated\n",
    "- [ ] Latency/memory benchmark recorded\n",
    "- [ ] Change-management plan approved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37dff9a",
   "metadata": {},
   "source": [
    "## üìö References\n",
    "- HuggingFace PEFT Documentation\n",
    "- *Adapter Tuning for Industrial NLP* (Siemens, 2025)\n",
    "- Week 05-06 prompt libraries for reference prompts"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
