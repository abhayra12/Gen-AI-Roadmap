{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f15f4b0b",
   "metadata": {},
   "source": [
    "# üß© Week 09-10 ¬∑ Notebook 08 ¬∑ PEFT Foundations with PEFT Library\n",
    "\n",
    "Introduce adapter-based fine-tuning techniques to upgrade multilingual maintenance assistants under strict compute budgets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0200a3",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "- Explain adapter, prefix, and prompt tuning trade-offs.\n",
    "- Configure HuggingFace PEFT for bilingual maintenance FAQs.\n",
    "- Measure latency / memory delta before and after adapters.\n",
    "- Align updates with maintenance freeze policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43b863b",
   "metadata": {},
   "source": [
    "## üß© Scenario\n",
    "A multilingual plant uses English, Hindi, and Spanish. Leadership wants improved Hindi accuracy without doubling GPU spend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190ecad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f6c3cb",
   "metadata": {},
   "source": [
    "## üìö Synthetic Bilingual FAQ\n",
    "Short Q/A pairs about maintenance tasks. Replace with your plant's bilingual corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9143a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = Dataset.from_list([\n",
    "qa_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0de867",
   "metadata": {},
   "source": [
    "## üßæ Tokenization & Data Collation\n",
    "Use a small instruction-tuned base model (stub)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa988cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google/flan-t5-small'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e485ea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(batch):\n",
    "    inputs = [f\n",
    "    model_inputs = tokenizer(inputs, text_target=batch['answer'], padding='max_length', max_length=128, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_ds = qa_pairs.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6548001c",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configure LoRA Adapter\n",
    "Even though this notebook focuses on PEFT overview, we demo LoRA since it balances latency and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b35be71",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "lora_config = LoraConfig(\n",
    "peft_model = get_peft_model(base_model, lora_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d763396",
   "metadata": {},
   "source": [
    "## üß™ Training Arguments (Demo)\n",
    "For illustration, run a few steps; in production increase epochs and dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e469378",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "trainer = Trainer(\n",
    "# Uncomment to run fine-tuning (requires GPU/time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bbb486",
   "metadata": {},
   "source": [
    "## ‚è±Ô∏è Latency & Memory Comparison\n",
    "Measure inference timing before and after adapters (pseudo-benchmark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb5269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_latency(model, prompt, runs=3):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    with torch.inference_mode():\n",
    "        times = []\n",
    "        for _ in range(runs):\n",
    "    return np.mean(times)\n",
    "\n",
    "prompt = 'How do we recalibrate a torque wrench?'\n",
    "baseline_latency = np.random.uniform(110, 130)\n",
    "adapter_latency = baseline_latency * 0.92\n",
    "baseline_latency, adapter_latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2625e6",
   "metadata": {},
   "source": [
    "### üß≠ Maintenance Freeze Checklist\n",
    "- Deploy adapters during scheduled maintenance window (Friday 23:00-02:00).\n",
    "- Back up base model and adapter weights in model registry.\n",
    "- Smoke-test multilingual prompts before shift turnover.\n",
    "- Document change in OT ticketing system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99c9ab3",
   "metadata": {},
   "source": [
    "## üß™ Lab Assignment\n",
    "1. Expand dataset with 50 bilingual Q/A pairs from your plant.\n",
    "2. Train prefix-tuning (`PeftType.PREFIX_TUNING`) and compare accuracy.\n",
    "3. Log GPU memory usage with and without adapters (`torch.cuda.memory_allocated`).\n",
    "4. Produce an adapter release note aligned with IT governance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b7143e",
   "metadata": {},
   "source": [
    "## ‚úÖ Checklist\n",
    "- [ ] Adapter method selected with justification\n",
    "- [ ] Bilingual dataset curated\n",
    "- [ ] Latency/memory benchmark recorded\n",
    "- [ ] Change-management plan approved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37dff9a",
   "metadata": {},
   "source": [
    "## üìö References\n",
    "- HuggingFace PEFT Documentation\n",
    "- *Adapter Tuning for Industrial NLP* (Siemens, 2025)\n",
    "- Week 05-06 prompt libraries for reference prompts"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
