{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a80a676c",
   "metadata": {},
   "source": [
    "# üèóÔ∏è Week 09-10 ¬∑ Notebook 10 ¬∑ End-to-End Fine-tuning Pipeline\n",
    "\n",
    "Assemble a production-ready instruction-tuning workflow covering data governance, training orchestration, evaluation, and packaging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bec67c",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "- **Curate & Govern Datasets:** Implement a data governance checklist to screen instruction-tuning datasets for PII, freshness, and SME approval.\n",
    "- **Orchestrate Scalable Training:** Use `accelerate` and `deepspeed` configuration files to launch distributed, production-ready fine-tuning jobs.\n",
    "- **Implement Robust Evaluation:** Combine automatic metrics (e.g., BLEU, ROUGE) with a structured human review process to ensure model quality and safety.\n",
    "- **Package for Deployment:** Systematically package all model artifacts‚Äîincluding weights, tokenizer configs, and evaluation reports‚Äîinto a versioned structure ready for a model registry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ad382d",
   "metadata": {},
   "source": [
    "## üß© Scenario\n",
    "A plant governance board requires a formal SOP before deploying a fine-tuned SOP assistant. You must demonstrate data QA, structured training, and evaluation sign-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98deb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import yaml\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0161ed2a",
   "metadata": {},
   "source": [
    "## üõ°Ô∏è Data Governance Checklist\n",
    "Start with a QA table capturing PII screening, freshness, and SME review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270b66cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "governance_data = [\n",
    "    {\n",
    "        \"dataset_name\": \"sop_assistant_v1\",\n",
    "        \"source_document\": \"SOP-MECH-001.pdf\",\n",
    "        \"pii_screened\": True,\n",
    "        \"sme_reviewer\": \"amit.kumar@example.com\",\n",
    "        \"review_date\": \"2024-10-12\",\n",
    "        \"approved_for_training\": True,\n",
    "        \"notes\": \"Contains no PII. Covers standard hydraulic press maintenance.\"\n",
    "    },\n",
    "    {\n",
    "        \"dataset_name\": \"sop_assistant_v1\",\n",
    "        \"source_document\": \"SOP-ELEC-003.docx\",\n",
    "        \"pii_screened\": True,\n",
    "        \"sme_reviewer\": \"priya.sharma@example.com\",\n",
    "        \"review_date\": \"2024-10-11\",\n",
    "        \"approved_for_training\": True,\n",
    "        \"notes\": \"Covers PLC firmware update procedures.\"\n",
    "    },\n",
    "    {\n",
    "        \"dataset_name\": \"sop_assistant_v1\",\n",
    "        \"source_document\": \"incident_reports_q3.csv\",\n",
    "        \"pii_screened\": False,\n",
    "        \"sme_reviewer\": \"sanjay.mehta@example.com\",\n",
    "        \"review_date\": \"2024-10-10\",\n",
    "        \"approved_for_training\": False,\n",
    "        \"notes\": \"Contains technician names and IDs. Requires anonymization before use.\"\n",
    "    }\n",
    "]\n",
    "governance_records = pd.DataFrame(governance_data)\n",
    "governance_records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45478a2f",
   "metadata": {},
   "source": [
    "## üìë Instruction Dataset Blueprint\n",
    "Structure prompts/responses referencing SOP sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595e8892",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_data = [\n",
    "    {\n",
    "        \"instruction\": \"What are the first three steps for the hydraulic press startup procedure?\",\n",
    "        \"context\": \"Document: SOP-MECH-001, Section: 2.1\",\n",
    "        \"response\": \"1. Ensure the main power breaker is in the 'ON' position. 2. Verify that the emergency stop button is disengaged. 3. Check that the hydraulic fluid level is within the green zone on the sight glass.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Summarize the safety warning for updating PLC firmware.\",\n",
    "        \"context\": \"Document: SOP-ELEC-003, Section: 4.0\",\n",
    "        \"response\": \"Before updating firmware, you must disconnect the PLC from the live production network to prevent unintended machine operations. A full backup of the current logic is mandatory.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"How long should the machine be powered down before maintenance?\",\n",
    "        \"context\": \"Document: SOP-MECH-001, Section: 3.2\",\n",
    "        \"response\": \"The machine must be fully powered down and locked-out/tagged-out (LOTO) for a minimum of 15 minutes to allow capacitors to discharge before any maintenance is performed.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# In a real pipeline, this would be saved to a JSONL file\n",
    "Path(\"data\").mkdir(exist_ok=True)\n",
    "with open(\"data/instruction_dataset.jsonl\", \"w\") as f:\n",
    "    for record in instruction_data:\n",
    "        f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "# Create a Hugging Face Dataset object\n",
    "instructions_dataset = Dataset.from_list(instruction_data)\n",
    "instructions_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1f7a38",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Accelerate / DeepSpeed Config (YAML)\n",
    "Store configuration for reproducible training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33b634d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This configuration is for a multi-GPU setup using DeepSpeed ZeRO Stage 3\n",
    "accelerate_config = {\n",
    "    \"compute_environment\": \"local_machine\",\n",
    "    \"distributed_type\": \"DEEPSPEED\",\n",
    "    \"machine_rank\": 0,\n",
    "    \"num_machines\": 1,\n",
    "    \"num_processes\": 4, # e.g., 4 GPUs\n",
    "    \"use_cpu\": False,\n",
    "    \"deepspeed_config\": {\n",
    "        \"train_batch_size\": \"auto\",\n",
    "        \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "        \"gradient_accumulation_steps\": \"auto\",\n",
    "        \"optimizer\": {\n",
    "            \"type\": \"AdamW\",\n",
    "            \"params\": {\n",
    "                \"lr\": 2e-5,\n",
    "                \"betas\": [0.9, 0.999],\n",
    "                \"eps\": 1e-8\n",
    "            }\n",
    "        },\n",
    "        \"fp16\": {\n",
    "            \"enabled\": True\n",
    "        },\n",
    "        \"zero_optimization\": {\n",
    "            \"stage\": 3,\n",
    "            \"offload_optimizer\": {\n",
    "                \"device\": \"cpu\",\n",
    "                \"pin_memory\": True\n",
    "            },\n",
    "            \"allgather_bucket_size\": 5e8,\n",
    "            \"reduce_bucket_size\": 5e8,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create a directory for configs and save the file\n",
    "Path(\"configs\").mkdir(exist_ok=True)\n",
    "config_path = Path(\"configs/deepspeed_zero3_config.yaml\")\n",
    "with open(config_path, 'w', encoding='utf-8') as f:\n",
    "    yaml.dump(accelerate_config, f)\n",
    "\n",
    "print(f\"--- Accelerate/DeepSpeed Config written to {config_path} ---\")\n",
    "print(config_path.read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8223463",
   "metadata": {},
   "source": [
    "## üèÉ Training Launcher Script\n",
    "Use HuggingFace CLI entry point referencing the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7ac2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command would be run in the terminal to start the distributed training job.\n",
    "# It uses the `accelerate launch` command, pointing to the config file and the training script.\n",
    "launcher_script = f\"\"\"\n",
    "accelerate launch --config_file configs/deepspeed_zero3_config.yaml scripts/run_instruction_tuning.py \\\\\n",
    "    --model_name_or_path \"meta-llama/Llama-2-7b-hf\" \\\\\n",
    "    --dataset_path \"data/instruction_dataset.jsonl\" \\\\\n",
    "    --output_dir \"models/sop-assistant-v1.1\" \\\\\n",
    "    --num_train_epochs 3 \\\\\n",
    "    --learning_rate 2e-5 \\\\\n",
    "    --logging_steps 10\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Example Training Launch Command ---\")\n",
    "print(launcher_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ebee79",
   "metadata": {},
   "source": [
    "*(Create `scripts/train_instruction.py` following HuggingFace Trainer patterns; see repo template.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2527ef31",
   "metadata": {},
   "source": [
    "## üìä Evaluation Harness\n",
    "Combine automatic metrics and human governance review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b623c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This represents a set of prompts to evaluate the model against.\n",
    "# In a real pipeline, this would be a separate, held-out dataset.\n",
    "eval_data = [\n",
    "    {\n",
    "        \"prompt_id\": \"EVAL-001\",\n",
    "        \"prompt\": \"What are the first three steps for the hydraulic press startup procedure?\",\n",
    "        \"ground_truth_response\": \"1. Ensure the main power breaker is in the 'ON' position. 2. Verify that the emergency stop button is disengaged. 3. Check that the hydraulic fluid level is within the green zone on the sight glass.\",\n",
    "        \"generated_response\": \"1. Turn on the main power. 2. Check the e-stop. 3. Look at the hydraulic fluid.\", # A slightly less formal but correct response\n",
    "        \"severity\": \"medium\",\n",
    "        \"reviewer\": \"amit.kumar@example.com\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt_id\": \"EVAL-002\",\n",
    "        \"prompt\": \"Summarize the safety warning for updating PLC firmware.\",\n",
    "        \"ground_truth_response\": \"Before updating firmware, you must disconnect the PLC from the live production network to prevent unintended machine operations. A full backup of the current logic is mandatory.\",\n",
    "        \"generated_response\": \"You must disconnect the PLC from the network before updating.\", # Missing the critical backup step\n",
    "        \"severity\": \"high\",\n",
    "        \"reviewer\": \"priya.sharma@example.com\"\n",
    "    }\n",
    "]\n",
    "eval_set = pd.DataFrame(eval_data)\n",
    "eval_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d3d0ed",
   "metadata": {},
   "source": [
    "### Automatic Metrics\n",
    "\n",
    "This section demonstrates how to use standard NLP metrics to evaluate the model's performance automatically. While these metrics are not a substitute for human review, they provide a valuable, scalable way to track model quality over time. We will use the `evaluate` library from Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4665ea",
   "metadata": {},
   "source": [
    "from evaluate import load\n",
    "\n",
    "# Load common text generation metrics\n",
    "try:\n",
    "    rouge = load('rouge')\n",
    "    bleu = load('bleu')\n",
    "    \n",
    "    # --- Run Automatic Evaluation ---\n",
    "    predictions = eval_set['generated_response'].tolist()\n",
    "    references = eval_set['ground_truth_response'].tolist()\n",
    "\n",
    "    rouge_results = rouge.compute(predictions=predictions, references=references)\n",
    "    bleu_results = bleu.compute(predictions=predictions, references=references)\n",
    "\n",
    "    print(\"--- Automatic Metric Results ---\")\n",
    "    print(f\"ROUGE-L Score: {rouge_results['rougeL']:.4f}\")\n",
    "    print(f\"BLEU Score: {bleu_results['bleu']:.4f}\")\n",
    "    \n",
    "    # --- Human Review Sign-off ---\n",
    "    # In a real pipeline, this would be a more complex workflow,\n",
    "    # but here we simulate it by adding a status to our eval DataFrame.\n",
    "    eval_set['human_review_status'] = eval_set.apply(\n",
    "        lambda row: 'FAIL' if row['severity'] == 'high' and 'backup' not in row['generated_response'].lower() else 'PASS',\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\\\n--- Human Review Simulation ---\")\n",
    "    display(eval_set[['prompt_id', 'severity', 'generated_response', 'human_review_status']])\n",
    "    \n",
    "    # Final sign-off check\n",
    "    if 'FAIL' in eval_set['human_review_status'].values:\n",
    "        print(\"\\\\n‚ùå GOVERNANCE FAILED: At least one high-severity prompt failed human review.\")\n",
    "    else:\n",
    "        print(\"\\\\n‚úÖ GOVERNANCE PASSED: All prompts passed.\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"The 'evaluate' library is not installed. Please run 'pip install evaluate rouge_score sacrebleu'.\")\n",
    "    print(\"Skipping automatic metric calculation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ce1f50",
   "metadata": {},
   "source": [
    "## üì¶ Packaging Artifacts for Model Registry\n",
    "\n",
    "A robust pipeline concludes by packaging all necessary components into a versioned directory, ready for upload to a model registry like Hugging Face Hub, MLflow, or a simple versioned file share.\n",
    "\n",
    "```bash\n",
    "# Example structure for a packaged model\n",
    "models/\n",
    "‚îî‚îÄ‚îÄ sop-assistant-v1.1/\n",
    "    ‚îú‚îÄ‚îÄ adapter_config.json\n",
    "    ‚îú‚îÄ‚îÄ adapter_model.bin\n",
    "    ‚îú‚îÄ‚îÄ tokenizer_config.json\n",
    "    ‚îú‚îÄ‚îÄ tokenizer.json\n",
    "    ‚îú‚îÄ‚îÄ special_tokens_map.json\n",
    "    ‚îú‚îÄ‚îÄ training_args.bin\n",
    "    ‚îú‚îÄ‚îÄ evaluation_report.json  # Contains ROUGE, BLEU, and human review sign-off\n",
    "    ‚îî‚îÄ‚îÄ governance_log.csv      # A snapshot of the data governance records used\n",
    "```\n",
    "\n",
    "This structure ensures that the model is fully reproducible and auditable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976f3cec",
   "metadata": {},
   "source": [
    "## üö® Release Checklist\n",
    "- ‚úÖ Governance QA complete\n",
    "- ‚úÖ Automatic metrics above threshold\n",
    "- ‚úÖ Human SMEs signed off\n",
    "- ‚úÖ Artifacts published to registry\n",
    "- ‚úÖ Rollback plan documented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5500f33f",
   "metadata": {},
   "source": [
    "## üß™ Lab Assignment\n",
    "1.  **Expand the Governance Log:** Add two more records to the `governance_records` DataFrame. One should be an approved document, and one should be a document that is rejected due to being outdated (e.g., `review_date` is from 2022).\n",
    "2.  **Create a Training Script:** Create a file named `scripts/run_instruction_tuning.py`. You don't need to write the full training logic, but the file should:\n",
    "    *   Import `argparse`, `torch`, and `transformers`.\n",
    "    *   Set up an `ArgumentParser` to accept the command-line arguments shown in the launcher script (`--model_name_or_path`, `--dataset_path`, etc.).\n",
    "    *   Include a `main()` function that prints the parsed arguments.\n",
    "3.  **Enhance the Evaluation:** Add a new entry to the `eval_data` list that represents a \"low\" severity prompt. Run the evaluation and confirm that the human review logic correctly passes it.\n",
    "4.  **Automate Artifact Packaging:** Write a Python function `package_artifacts(model_dir)` that takes a directory path and creates the folder structure and dummy files shown in the \"Packaging Artifacts\" section (e.g., creates `adapter_config.json`, `evaluation_report.json`, etc.).\n",
    "\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-a1b2c3d4\" language=\"markdown\">\n",
    "## ‚úÖ Checklist\n",
    "- [ ] Data governance process is defined and auditable.\n",
    "- [ ] Distributed training configuration is version-controlled.\n",
    "- [ ] Evaluation includes both automatic metrics and human-in-the-loop review.\n",
    "- [ ] Model artifacts are versioned and packaged for a model registry.\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-e5f6g7h8\" language=\"markdown\">\n",
    "## üìö References\n",
    "- Hugging Face `accelerate` Documentation\n",
    "- DeepSpeed Documentation\n",
    "- *Building Production-Ready NLP Systems* (O'Reilly, 2024)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
