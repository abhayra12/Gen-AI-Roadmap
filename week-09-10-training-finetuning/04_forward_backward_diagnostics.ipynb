{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15320d36",
   "metadata": {},
   "source": [
    "# 🔁 Week 09-10 · Notebook 04 · Forward & Backward Pass Diagnostics\n",
    "\n",
    "Trace gradients through manufacturing classifiers, detect instability, and enforce quality gates before deploying fine-tuned models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6042541e",
   "metadata": {},
   "source": [
    "## 🎯 Learning Objectives\n",
    "- **Build Intuition:** Manually compute gradients for a simple network to understand the mechanics of backpropagation.\n",
    "- **Inspect Gradients:** Use PyTorch's autograd hooks to inspect layer-wise gradient norms during training.\n",
    "- **Diagnose Instability:** Identify and mitigate exploding or vanishing gradients, which are common when training on imbalanced manufacturing data (e.g., rare equipment failures).\n",
    "- **Implement Governance:** Create a monitoring and alerting strategy for training stability, ensuring model quality and compliance before deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ae9afc",
   "metadata": {},
   "source": [
    "## 🧩 Scenario\n",
    "You fine-tuned a classifier to flag safety incidents. After deploying to four plants, you observe erratic confidence scores. Root cause: gradients exploded due to class imbalance between critical and routine events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71d80d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "torch.manual_seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3ea1f1",
   "metadata": {},
   "source": [
    "## 🧮 Manual Gradient Walkthrough\n",
    "For a simple logistic unit, compute the gradient explicitly and compare with autograd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce76c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0.5, 1.0]], requires_grad=True)\n",
    "w = torch.tensor([[1.2], [-0.7]], requires_grad=True)\n",
    "y_true = torch.tensor([[1.0]])\n",
    "\n",
    "logits = x @ w\n",
    "y_pred = torch.sigmoid(logits)\n",
    "loss = F.binary_cross_entropy(y_pred, y_true)\n",
    "loss.backward()\n",
    "x.grad, w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117ffd45",
   "metadata": {},
   "source": [
    "Compare the output with the analytical derivative to build confidence in your gradient checks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ced9c6",
   "metadata": {},
   "source": [
    "## 🧱 Safety Incident Classifier\n",
    "A miniature MLP flags whether a maintenance note contains a safety-critical issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c11126",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncidentNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(16, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.sigmoid(self.fc3(x))\n",
    "\n",
    "model = IncidentNet()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c45b477",
   "metadata": {},
   "source": [
    "## 📊 Gradient Norm Logging\n",
    "Attach hooks to capture gradient statistics after each backward pass and store them for dashboards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac96834",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_log = []\n",
    "gradient_norms = {}\n",
    "\n",
    "def capture_gradient_hook(module, grad_input, grad_output):\n",
    "    \"\"\"\n",
    "    A hook that runs after a backward pass.\n",
    "    It captures the norm of the gradient of the output of a specific layer.\n",
    "    \"\"\"\n",
    "    # Find the name of the module that this hook is attached to.\n",
    "    layer_name = [name for name, mod in model.named_modules() if mod is module][0]\n",
    "    \n",
    "    # grad_output is a tuple containing gradients with respect to the module's output.\n",
    "    if grad_output[0] is not None:\n",
    "        grad_norm = grad_output[0].norm().item()\n",
    "        # Store the latest norm for quick access\n",
    "        gradient_norms[layer_name] = grad_norm\n",
    "        # Append to a log for historical analysis\n",
    "        gradient_log.append({'layer': layer_name, 'grad_norm': grad_norm})\n",
    "\n",
    "def register_hooks(model):\n",
    "    \"\"\"\n",
    "    Registers a backward hook on all nn.Linear layers of the model.\n",
    "    This allows us to inspect gradients without altering the training loop.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # register_full_backward_hook is the modern and recommended way to register hooks.\n",
    "            module.register_full_backward_hook(capture_gradient_hook)\n",
    "\n",
    "register_hooks(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3339f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(batch_size=32):\n",
    "    model.train()\n",
    "    feature_scale = torch.tensor([1.0] * 8 + [5.0] * 8)\n",
    "    inputs = torch.randn(batch_size, 16) * feature_scale\n",
    "    targets = torch.bernoulli(torch.full((batch_size, 1), 0.1))\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "for epoch in range(5):\n",
    "    _ = training_step()\n",
    "\n",
    "gradient_df = pd.DataFrame(gradient_log)\n",
    "gradient_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6628187b",
   "metadata": {},
   "source": [
    "## 📈 Gradient Trends\n",
    "Use the recorded norms to trigger alerts when variance or magnitude exceeds thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91f1a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 5.0\n",
    "alerts = gradient_df[gradient_df['grad_norm'] > threshold]\n",
    "alerts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f83b409",
   "metadata": {},
   "source": [
    "### 🚨 Governance Guidance\n",
    "- If any layer norm exceeds 5.0 consistently, pause training and notify QA.\n",
    "- Log gradient alerts in the safety incident tracker and escalate to the MLOps on-call engineer.\n",
    "- Keep gradient logs for 30 days to support ISO 45001 audits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a1a713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gradient_clipping(max_norm=1.0):\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "gradient_log.clear()\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    inputs = torch.randn(32, 16)\n",
    "    targets = torch.bernoulli(torch.full((32, 1), 0.1))\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    apply_gradient_clipping(1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "pd.DataFrame(gradient_log).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a2e63e",
   "metadata": {},
   "source": [
    "## 🧪 Lab Assignment\n",
    "1. Load your plant's labeled incident dataset and reproduce gradient logs.\n",
    "2. Implement class-weighted loss and observe gradient stability improvements.\n",
    "3. Build a Streamlit dashboard displaying gradient histograms per epoch.\n",
    "4. Draft an SOP section documenting gradient alert handling procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7c5781",
   "metadata": {},
   "source": [
    "## ✅ Checklist\n",
    "- [ ] Manual gradient calculations validated\n",
    "- [ ] Autograd hooks configured with logging\n",
    "- [ ] Alerts and mitigations documented\n",
    "- [ ] Governance dashboard shared with safety committee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea56d0ba",
   "metadata": {},
   "source": [
    "## 📚 References\n",
    "- PyTorch Autograd Mechanics\n",
    "- *Human Factors in Safety-Critical AI* (2024)\n",
    "- ISO 45001: Occupational Health and Safety Management"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
