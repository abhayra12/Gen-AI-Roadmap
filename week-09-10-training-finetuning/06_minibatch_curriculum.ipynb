{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70767218",
   "metadata": {},
   "source": [
    "# üì¶ Week 09-10 ¬∑ Notebook 06 ¬∑ Mini-batch Training & Curriculum Scheduling\n",
    "\n",
    "Design batching strategies and curricula to stabilize training on skewed manufacturing corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff28a2ff",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "- **Implement Custom Data Loaders:** Build a PyTorch `Dataset` and `DataLoader` that can handle our manufacturing data with its different severity levels.\n",
    "- **Design a Curriculum Strategy:** Implement a curriculum learning approach where the model first learns from common, \"easy\" examples (routine maintenance) before being shown rare, \"hard\" examples (critical failures).\n",
    "- **Balance Training Parameters:** Analyze the trade-offs between batch size, training throughput, and GPU memory constraints, which is critical in a production environment with shared resources.\n",
    "- **Document Operational Procedures:** Draft a Standard Operating Procedure (SOP) for handing off long-running training jobs between shifts, ensuring continuity and accountability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b582d08",
   "metadata": {},
   "source": [
    "## üß© Scenario\n",
    "Only 3% of maintenance logs capture critical failures, yet they matter most. You need a training curriculum that ramps from routine notes ‚Üí cautionary signals ‚Üí critical incidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65602d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "torch.manual_seed(909)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f5441d",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è Curriculum Tags\n",
    "Each sample is assigned a severity level. Critical incidents are oversampled later in the curriculum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c49c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_curriculum_dataset(num_samples=2000):\n",
    "    \"\"\"\n",
    "    Creates a dataset with three levels of maintenance severity.\n",
    "    'critical' samples have a distinct feature pattern to make them learnable.\n",
    "    \"\"\"\n",
    "    severities = ['routine', 'warning', 'critical']\n",
    "    # Skewed distribution: most logs are routine.\n",
    "    severity_weights = [0.85, 0.12, 0.03]\n",
    "    records = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        level = random.choices(severities, weights=severity_weights)[0]\n",
    "        # Base embedding for all samples\n",
    "        embedding = torch.randn(128)\n",
    "        \n",
    "        # Add a signal for more severe events\n",
    "        if level == 'critical':\n",
    "            embedding[:32] += 2.5  # Add a strong signal to the first 32 features\n",
    "        elif level == 'warning':\n",
    "            embedding[32:64] += 1.5 # Add a moderate signal to the next 32 features\n",
    "            \n",
    "        records.append({\n",
    "            'id': f'LOG-{i:04d}',\n",
    "            'severity': level,\n",
    "            'embedding': embedding\n",
    "        })\n",
    "        \n",
    "    return records\n",
    "\n",
    "curriculum_records = create_curriculum_dataset()\n",
    "df_severity = pd.Series([r['severity'] for r in curriculum_records])\n",
    "\n",
    "print(\"Dataset Severity Distribution:\")\n",
    "print(df_severity.value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b53115",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaintenanceLogDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset to handle the maintenance log records.\n",
    "    It returns the embedding and a numerical label for the severity.\n",
    "    \"\"\"\n",
    "    def __init__(self, records):\n",
    "        self.records = records\n",
    "        # Map severity strings to integer labels\n",
    "        self.severity_map = {'routine': 0, 'warning': 1, 'critical': 2}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        record = self.records[idx]\n",
    "        embedding = record['embedding']\n",
    "        label = self.severity_map[record['severity']]\n",
    "        return embedding, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# We will use this dataset later with a custom sampler\n",
    "full_dataset = MaintenanceLogDataset(curriculum_records)\n",
    "print(f\"Created a dataset with {len(full_dataset)} records.\")\n",
    "# Example: get the first item\n",
    "embedding, label = full_dataset[0]\n",
    "print(f\"First item's embedding shape: {embedding.shape}, Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fd8290",
   "metadata": {},
   "source": [
    "## üóúÔ∏è Adaptive Batch Sampler\n",
    "Start with routine samples, then gradually mix in higher-severity items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf44a051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler\n",
    "import numpy as np\n",
    "\n",
    "class CurriculumSampler(Sampler):\n",
    "    \"\"\"\n",
    "    A custom sampler that implements curriculum learning.\n",
    "    - Epoch 0-2: Only 'routine' samples.\n",
    "    - Epoch 3-5: 'routine' and 'warning' samples.\n",
    "    - Epoch 6+: All samples, with 'critical' ones repeated to ensure they are seen often.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        # Get indices for each severity level\n",
    "        self.indices_by_severity = {\n",
    "            'routine': [i for i, r in enumerate(dataset.records) if r['severity'] == 'routine'],\n",
    "            'warning': [i for i, r in enumerate(dataset.records) if r['severity'] == 'warning'],\n",
    "            'critical': [i for i, r in enumerate(dataset.records) if r['severity'] == 'critical']\n",
    "        }\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Determine which indices to use based on the current epoch\n",
    "        if self.current_epoch < 3: # Phase 1: Easy samples\n",
    "            indices = self.indices_by_severity['routine']\n",
    "        elif self.current_epoch < 6: # Phase 2: Medium samples\n",
    "            indices = self.indices_by_severity['routine'] + self.indices_by_severity['warning']\n",
    "        else: # Phase 3: Hard samples\n",
    "            # Oversample critical and warning cases to ensure the model learns from them\n",
    "            indices = (self.indices_by_severity['routine'] + \n",
    "                       self.indices_by_severity['warning'] * 5 + \n",
    "                       self.indices_by_severity['critical'] * 15)\n",
    "        \n",
    "        random.shuffle(indices)\n",
    "        return iter(indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the sampler changes with the curriculum phase\n",
    "        if self.current_epoch < 3:\n",
    "            return len(self.indices_by_severity['routine'])\n",
    "        elif self.current_epoch < 6:\n",
    "            return len(self.indices_by_severity['routine']) + len(self.indices_by_severity['warning'])\n",
    "        else:\n",
    "            return (len(self.indices_by_severity['routine']) + \n",
    "                    len(self.indices_by_severity['warning']) * 5 + \n",
    "                    len(self.indices_by_severity['critical']) * 15)\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        \"\"\"Called by the training loop to advance the curriculum.\"\"\"\n",
    "        self.current_epoch = epoch\n",
    "\n",
    "# --- Dummy Model and Training Loop to Demonstrate the Sampler ---\n",
    "model = nn.Linear(128, 3) # 128 features, 3 severity classes\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "sampler = CurriculumSampler(full_dataset, batch_size)\n",
    "# Note: When using a custom sampler, `shuffle` in DataLoader must be False.\n",
    "data_loader = DataLoader(full_dataset, batch_size=batch_size, sampler=sampler)\n",
    "\n",
    "print(\"--- Starting Training with Curriculum Sampler ---\")\n",
    "for epoch in range(num_epochs):\n",
    "    sampler.set_epoch(epoch) # IMPORTANT: Update the sampler's epoch\n",
    "    \n",
    "    phase = \"Easy (Routine)\"\n",
    "    if 2 < epoch < 6:\n",
    "        phase = \"Medium (Routine + Warning)\"\n",
    "    elif epoch >= 6:\n",
    "        phase = \"Hard (All, Oversampled)\"\n",
    "        \n",
    "    print(f\"\\\\nEpoch {epoch+1}/{num_epochs} | Phase: {phase} | Batches: {len(data_loader)}\")\n",
    "    \n",
    "    for i, (embeddings, labels) in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(embeddings)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 20 == 0:\n",
    "            # Get the distribution of the current batch for inspection\n",
    "            label_counts = pd.Series(labels.numpy()).value_counts().to_dict()\n",
    "            print(f'  Batch {i+1}, Loss: {loss.item():.4f}, Batch Dist: {label_counts}')\n",
    "\n",
    "print(\"\\\\n--- Training Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
