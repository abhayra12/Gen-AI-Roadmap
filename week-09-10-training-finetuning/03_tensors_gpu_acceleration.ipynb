{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7344e6b4",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Week 09-10 ¬∑ Notebook 03 ¬∑ Tensors & GPU Acceleration in Edge Plants\n",
    "\n",
    "Benchmark tensor operations and GPU utilization strategies when compute is limited to shared workstations or on-prem clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4a8253",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "- Manipulate PyTorch tensors across CPU/GPU devices.\n",
    "- Evaluate mixed-precision trade-offs for manufacturing workloads.\n",
    "- Profile memory and throughput to align with plant uptime schedules.\n",
    "- Capture governance notes for IT change management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc88d15",
   "metadata": {},
   "source": [
    "## üß© Scenario\n",
    "A tier-2 supplier alternates between an A100 workstation and a shared RTX 6000. Training jobs must finish overnight without impacting SCADA traffic. You need a repeatable benchmark harness and downtime mitigation plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052e8e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9416e3",
   "metadata": {},
   "source": [
    "## üßÆ Tensor Fundamentals: Representing Manufacturing Data\n",
    "\n",
    "Tensors are the fundamental data structure for deep learning. We'll use them to represent everything from sensor readings to maintenance logs. Let's start by creating a tensor representing a batch of sensor data and inspecting its properties. This is a critical first step for governance, ensuring data types and memory layouts are compliant with our IT policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4814a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: A batch of 4 sensor readings, each with 3 values (e.g., temperature, pressure, vibration)\n",
    "sensor_data = torch.tensor([\n",
    "    [25.5, 101.3, 0.02],\n",
    "    [26.1, 101.4, 0.03],\n",
    "    [24.9, 101.2, 0.02],\n",
    "    [25.8, 101.3, 0.025]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "print(\"Sensor Data Tensor:\")\n",
    "print(sensor_data)\n",
    "print(f\"\\nShape: {sensor_data.shape}\")\n",
    "print(f\"Data Type: {sensor_data.dtype}\")\n",
    "print(f\"Device: {sensor_data.device}\")\n",
    "print(f\"Memory Layout (Stride): {sensor_data.stride()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fd9bdb",
   "metadata": {},
   "source": [
    "If a GPU is available, we can move our tensors to it for a massive speedup. This is crucial for training models overnight. We'll write a function that gracefully falls back to the CPU if no GPU is found, which is common for technicians using standard laptops on the plant floor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280f7b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device_summary():\n",
    "    \"\"\"Checks for GPU availability and returns a summary.\"\"\"\n",
    "    summary = {}\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        summary['device_type'] = \"cuda\"\n",
    "        summary['device_name'] = torch.cuda.get_device_name(0)\n",
    "        # Create a large tensor to inspect memory allocation\n",
    "        large_tensor = torch.randn((2048, 2048), device=device)\n",
    "        summary['tensor_on_gpu'] = large_tensor.is_cuda\n",
    "        summary['memory_allocated_gb'] = torch.cuda.memory_allocated(0) / 1e9\n",
    "        del large_tensor # Clean up memory\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        summary['device_type'] = \"cpu\"\n",
    "        summary['device_name'] = \"N/A\"\n",
    "        summary['tensor_on_gpu'] = False\n",
    "        summary['memory_allocated_gb'] = 0\n",
    "\n",
    "    return summary\n",
    "\n",
    "device_summary = get_device_summary()\n",
    "print(f\"Device Summary: {device_summary}\")\n",
    "\n",
    "# Set the default device for subsequent operations\n",
    "device = torch.device(device_summary['device_type'])\n",
    "print(f\"\\nDefault device set to: '{device}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6558e897",
   "metadata": {},
   "source": [
    "## ‚ö° Mixed Precision Benchmark\n",
    "Evaluate float32 vs. bfloat16/float16 throughput. Use caution on safety-critical inference pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24ead1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_benchmark(size=4096, dtype=torch.float32, device='cpu', runs=5):\n",
    "    \"\"\"Performs a matrix multiplication benchmark for a given configuration.\"\"\"\n",
    "    a = torch.randn(size, size, dtype=dtype, device=device)\n",
    "    b = torch.randn(size, size, dtype=dtype, device=device)\n",
    "    \n",
    "    # Warm-up run\n",
    "    _ = torch.matmul(a, b)\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    times = []\n",
    "    for _ in range(runs):\n",
    "        start_time = time.time()\n",
    "        _ = torch.matmul(a, b)\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.synchronize() # Wait for the GPU operation to complete\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "        \n",
    "    avg_time = sum(times) / len(times)\n",
    "    return avg_time\n",
    "\n",
    "def benchmark_suite():\n",
    "    \"\"\"Runs the benchmark across available devices and data types.\"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Define dtypes to test\n",
    "    dtypes_to_test = {\n",
    "        \"float32\": torch.float32\n",
    "    }\n",
    "    if device == 'cuda':\n",
    "        # bfloat16 is generally preferred on modern GPUs (Ampere and newer)\n",
    "        if torch.cuda.is_bf16_supported():\n",
    "             dtypes_to_test[\"bfloat16\"] = torch.bfloat16\n",
    "        dtypes_to_test[\"float16\"] = torch.float16\n",
    "\n",
    "    results = []\n",
    "    print(f\"--- Running Benchmark on {device.upper()} ---\")\n",
    "    for name, dtype in dtypes_to_test.items():\n",
    "        avg_time = matmul_benchmark(dtype=dtype, device=device)\n",
    "        results.append({\n",
    "            'device': device,\n",
    "            'dtype': name,\n",
    "            'avg_time_seconds': round(avg_time, 5)\n",
    "        })\n",
    "        print(f\"  {name}: {avg_time:.5f} seconds per matmul\")\n",
    "        \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "benchmark_results = benchmark_suite()\n",
    "print(\"\\n--- Benchmark Results ---\")\n",
    "print(benchmark_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f54061",
   "metadata": {},
   "source": [
    "## üßæ Memory Profiling Checklist\n",
    "- Capture `torch.cuda.memory_summary()` at job start and end.\n",
    "- Enforce IT's GPU allocation window (e.g., 18:00-06:00) to avoid SCADA conflicts.\n",
    "- Log utilization metrics in maintenance CMMS for accountability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a1c7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"--- GPU Memory Summary ---\")\n",
    "    # Provides a detailed breakdown of memory usage\n",
    "    summary_text = torch.cuda.memory_summary(device=device, abbreviated=True)\n",
    "    print(summary_text)\n",
    "else:\n",
    "    print(\"--- GPU Memory Summary ---\")\n",
    "    print(\"No GPU available. Memory profiling is only applicable for CUDA devices.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dca0aa",
   "metadata": {},
   "source": [
    "## üõ°Ô∏è Downtime Mitigation & Change Management Plan\n",
    "\n",
    "Deploying AI models into a production manufacturing environment requires careful planning to avoid disrupting operations. This is a template for a change management ticket.\n",
    "\n",
    "| Risk Category | Specific Risk | Mitigation Strategy | Owner |\n",
    "|---|---|---|---|\n",
    "| **Compute Resource Conflict** | AI training job overloads the shared workstation, impacting the SCADA system that monitors production lines. | **Schedule-Based Access:** Limit AI training to off-peak hours (e.g., 10 PM - 6 AM). Use `nice` in Linux or process priority settings in Windows to de-prioritize the training script. | IT / AI Team |\n",
    "| **Model Performance Degradation** | A new model version (e.g., using `bfloat16`) produces incorrect or unsafe predictions for a critical process like quality control. | **Canary Deployment:** Route 1% of inference requests to the new model. Compare its outputs against the stable `float32` model. Only roll out fully after a 24-hour validation period with no discrepancies. | AI Team / QA |\n",
    "| **GPU Hardware Failure** | The primary GPU (A100) fails mid-training, halting model development. | **Graceful Fallback:** Ensure all training scripts can run on a secondary device (e.g., RTX 6000) or even CPU with a smaller batch size. The script should automatically detect the available device. | AI Team |\n",
    "| **Network Congestion** | Transferring large datasets or model checkpoints across the plant network interferes with critical operational data flow. | **Data Locality & Off-Peak Transfers:** Pre-stage datasets on the training workstation. Schedule transfers of large model files during the approved off-peak window. | IT / Network Ops |\n",
    "| **Rollback Failure** | A deployed model needs to be rolled back, but the previous version's artifacts are missing or incompatible. | **Version Control for Models:** Use a model registry (like MLflow or a simple versioned directory structure) to store model weights, tokenizer configs, and performance metrics for every deployed version. | AI Team |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb497102",
   "metadata": {},
   "source": [
    "## üß™ Lab Assignment\n",
    "1. Run the benchmark suite on both A100 and RTX 6000, compare throughput.\n",
    "2. Add power draw instrumentation (e.g., `nvidia-smi --query-gpu=power.draw`).\n",
    "3. Propose a mixed-precision policy for safety-critical vs. internal tools.\n",
    "4. Submit change-management ticket with benchmark evidence and rollback plan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbb4d82",
   "metadata": {},
   "source": [
    "## ‚úÖ Checklist\n",
    "- [ ] Tensor device audit completed\n",
    "- [ ] Mixed-precision benchmark logged\n",
    "- [ ] Memory summary archived for compliance\n",
    "- [ ] Downtime mitigation plan approved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4f9752",
   "metadata": {},
   "source": [
    "## üìö References\n",
    "- PyTorch Performance Tuning Guide\n",
    "- NVIDIA A100 vs. RTX 6000 Comparison (2025)\n",
    "- *Operational Technology Change Management Handbook* (ISA, 2023)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
