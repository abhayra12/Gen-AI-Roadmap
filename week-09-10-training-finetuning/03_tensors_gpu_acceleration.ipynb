{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7344e6b4",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Week 09-10 ¬∑ Notebook 03 ¬∑ Tensors & GPU Acceleration in Edge Plants\n",
    "\n",
    "Benchmark tensor operations and GPU utilization strategies when compute is limited to shared workstations or on-prem clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4a8253",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "- Manipulate PyTorch tensors across CPU/GPU devices.\n",
    "- Evaluate mixed-precision trade-offs for manufacturing workloads.\n",
    "- Profile memory and throughput to align with plant uptime schedules.\n",
    "- Capture governance notes for IT change management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc88d15",
   "metadata": {},
   "source": [
    "## üß© Scenario\n",
    "A tier-2 supplier alternates between an A100 workstation and a shared RTX 6000. Training jobs must finish overnight without impacting SCADA traffic. You need a repeatable benchmark harness and downtime mitigation plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052e8e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9416e3",
   "metadata": {},
   "source": [
    "## üßÆ Tensor Fundamentals\n",
    "Inspect dtype, device, and stride configuration to confirm compliance with IT policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4814a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = torch.arange(12, dtype=torch.float32).reshape(3, 4)\n",
    "vector\n",
    "vector.stride(), vector.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fd9bdb",
   "metadata": {},
   "source": [
    "If a GPU is available, move tensors and compare memory footprints. Fallback gracefully when running on CPU-only plant laptops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280f7b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_device_summary():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    t = torch.rand((2048, 2048), device=device, dtype=torch.float32)\n",
    "    summary = {\n",
    "    return summary\n",
    "\n",
    "tensor_device_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6558e897",
   "metadata": {},
   "source": [
    "## ‚ö° Mixed Precision Benchmark\n",
    "Evaluate float32 vs. bfloat16/float16 throughput. Use caution on safety-critical inference pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24ead1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_benchmark(size=4096, dtype=torch.float32, device='cpu', runs=5):\n",
    "    a = torch.randn((size, size), dtype=dtype, device=device)\n",
    "    b = torch.randn((size, size), dtype=dtype, device=device)\n",
    "    torch.cuda.synchronize() if device == 'cuda' else None\n",
    "    times = []\n",
    "    for _ in range(runs):\n",
    "    return sum(times) / len(times)\n",
    "\n",
    "def benchmark_suite():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    dtypes = [torch.float32]\n",
    "    if device == 'cuda':\n",
    "    results = []\n",
    "    for dtype in dtypes:\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "benchmark_suite()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f54061",
   "metadata": {},
   "source": [
    "## üßæ Memory Profiling Checklist\n",
    "- Capture `torch.cuda.memory_summary()` at job start and end.\n",
    "- Enforce IT's GPU allocation window (e.g., 18:00-06:00) to avoid SCADA conflicts.\n",
    "- Log utilization metrics in maintenance CMMS for accountability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a1c7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    summary_text = torch.cuda.memory_summary(device=torch.device('cuda'), abbreviated=True)\n",
    "    summary_text.split('\n",
    "')[:10]\n",
    "else:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dca0aa",
   "metadata": {},
   "source": [
    "## üõ°Ô∏è Downtime Mitigation Plan\n",
    "| Risk | Mitigation | Owner |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb497102",
   "metadata": {},
   "source": [
    "## üß™ Lab Assignment\n",
    "1. Run the benchmark suite on both A100 and RTX 6000, compare throughput.\n",
    "2. Add power draw instrumentation (e.g., `nvidia-smi --query-gpu=power.draw`).\n",
    "3. Propose a mixed-precision policy for safety-critical vs. internal tools.\n",
    "4. Submit change-management ticket with benchmark evidence and rollback plan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbb4d82",
   "metadata": {},
   "source": [
    "## ‚úÖ Checklist\n",
    "- [ ] Tensor device audit completed\n",
    "- [ ] Mixed-precision benchmark logged\n",
    "- [ ] Memory summary archived for compliance\n",
    "- [ ] Downtime mitigation plan approved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4f9752",
   "metadata": {},
   "source": [
    "## üìö References\n",
    "- PyTorch Performance Tuning Guide\n",
    "- NVIDIA A100 vs. RTX 6000 Comparison (2025)\n",
    "- *Operational Technology Change Management Handbook* (ISA, 2023)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
