{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ddcb507",
   "metadata": {},
   "source": [
    "# ðŸ”¡ Week 09-10 Â· Notebook 02 Â· Bigram Language Model with Maintenance Logs\n",
    "\n",
    "Build a from-scratch bigram language model to understand token statistics inside manufacturing maintenance notes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c30e13e",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Learning Objectives\n",
    "- Clean and normalize maintenance text for n-gram modeling.\n",
    "- Implement a bigram language model in PyTorch and compute perplexity.\n",
    "- Analyze jargon-heavy vs. plain-language performance.\n",
    "- Document failure modes relevant to plant-floor deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca18d329",
   "metadata": {},
   "source": [
    "## ðŸ§© Scenario\n",
    "Technicians log work orders on a shared tablet. Autocomplete powered by a small language model can reduce typing time, but it must understand torque specs, part numbers, and bilingual notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e58898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import torch\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5870b61f",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Maintenance Log Samples\n",
    "Synthetic logs mimic common maintenance narratives. Replace with your CMMS export to train on real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0929b578",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = pd.DataFrame(["
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b88598",
   "metadata": {},
   "source": [
    "### ðŸ”„ Text Normalization\n",
    "Normalize units, tokenize multilingual text, and preserve domain-specific numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d154ff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text: str) -> list[str]:\n",
    "    text = text.lower()\n",
    "    text = text.replace('psi', ' psi ').replace('nm', ' nm ').replace('kpa', ' kpa ')\n",
    "    text = re.sub(r'[^]', ' ', text)\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "logs['tokens'] = logs['text'].apply(normalize)\n",
    "logs[['text', 'tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6b1068",
   "metadata": {},
   "source": [
    "## ðŸ§® Bigram Model Implementation\n",
    "We collect transition counts and transform them into log probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059ef013",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = '<s>'\n",
    "STOP_TOKEN = '</s>'\n",
    "\n",
    "def build_bigram_counts(token_sequences):\n",
    "    counts = defaultdict(Counter)\n",
    "    vocab = set()\n",
    "    for seq in token_sequences:\n",
    "    return counts, vocab\n",
    "\n",
    "counts, vocab = build_bigram_counts(logs['tokens'])\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec29e63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_probs(counts, smoothing=1.0):\n",
    "    probs = {}\n",
    "    vocab = set(counts.keys())\n",
    "    for prev_token, next_counts in counts.items():\n",
    "    return probs\n",
    "\n",
    "probs = bigram_probs(counts)\n",
    "list(probs[START_TOKEN].items())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779cbc08",
   "metadata": {},
   "source": [
    "### ðŸ“‰ Perplexity Calculation\n",
    "Evaluate how well the model predicts held-out sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc6fc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_log_prob(tokens, probs):\n",
    "    tokens = [START_TOKEN] + tokens + [STOP_TOKEN]\n",
    "    logp = 0.0\n",
    "    for prev_token, next_token in zip(tokens, tokens[1:]):\n",
    "    return logp\n",
    "\n",
    "def perplexity(dataset, probs):\n",
    "    logp_sum, token_count = 0.0, 0\n",
    "    for tokens in dataset:\n",
    "    return math.exp(-logp_sum / max(token_count, 1))\n",
    "\n",
    "pp = perplexity(logs['tokens'], probs)\n",
    "pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a4d4e0",
   "metadata": {},
   "source": [
    "## ðŸ§ª Experiment: Extend to Trigrams\n",
    "Use this cell as a template for students to implement trigram logic and compare perplexities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2291c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lab): implement trigram counts, probabilities, and perplexity\n",
    "# Document perplexity reduction and note failure cases (e.g., rare part numbers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3195da7",
   "metadata": {},
   "source": [
    "## ðŸ©º Error Analysis Framework\n",
    "- Inspect top 20 predicted next tokens for bilingual sentences.\n",
    "- Flag mispredictions on torque specs, units, and compliance phrases.\n",
    "- Track per-language perplexity to justify multilingual adapters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c5633d",
   "metadata": {},
   "source": [
    "## ðŸ§ª Lab Assignment\n",
    "1. Swap in three months of real maintenance logs and compute bigram perplexity.\n",
    "2. Implement trigram smoothing (Kneserâ€“Ney or Goodâ€“Turing) and compare results.\n",
    "3. Document failure modes tied to acronyms, bilingual phrases, and numerical tolerances.\n",
    "4. Present a memo recommending whether to proceed with larger-scale transformer pre-training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97f084d",
   "metadata": {},
   "source": [
    "## âœ… Checklist\n",
    "- [ ] Logs normalized with consistent unit handling\n",
    "- [ ] Bigram model implemented with smoothing\n",
    "- [ ] Perplexity baseline captured per language\n",
    "- [ ] Failure report shared with maintenance SMEs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76192f70",
   "metadata": {},
   "source": [
    "## ðŸ“š References\n",
    "- Jurafsky & Martin, *Speech and Language Processing* (Chapter on N-grams)\n",
    "- TorchText tutorials on language modeling\n",
    "- *Multilingual Maintenance Communications* (Society of Manufacturing Engineers, 2024)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
