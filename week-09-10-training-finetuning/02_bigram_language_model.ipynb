{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ddcb507",
   "metadata": {},
   "source": [
    "# ðŸ”¡ Week 09-10 Â· Notebook 02 Â· Bigram Language Model with Maintenance Logs\n",
    "\n",
    "Build a from-scratch bigram language model to understand token statistics inside manufacturing maintenance notes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c30e13e",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Learning Objectives\n",
    "- Clean and normalize maintenance text for n-gram modeling.\n",
    "- Implement a bigram language model in PyTorch and compute perplexity.\n",
    "- Analyze jargon-heavy vs. plain-language performance.\n",
    "- Document failure modes relevant to plant-floor deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca18d329",
   "metadata": {},
   "source": [
    "## ðŸ§© Scenario\n",
    "Technicians log work orders on a shared tablet. Autocomplete powered by a small language model can reduce typing time, but it must understand torque specs, part numbers, and bilingual notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e58898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import torch\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5870b61f",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Maintenance Log Samples\n",
    "Synthetic logs mimic common maintenance narratives. Replace with your CMMS export to train on real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0929b578",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_data = [\n",
    "    {\"log_id\": \"L001\", \"text\": \"Replaced the main conveyor belt motor (Part #MTR-789). Torqued bolts to 150 Nm. System back online.\"},\n",
    "    {\"log_id\": \"L002\", \"text\": \"Calibrated the CNC machine's laser sensor. Accuracy is now within 0.05mm tolerance. Se requiere una revisiÃ³n en 24 horas.\"},\n",
    "    {\"log_id\": \"L003\", \"text\": \"Hydraulic fluid leak detected on Press-03. Replaced seal (Part #SL-456) and refilled fluid. Pressure at 2000 psi.\"},\n",
    "    {\"log_id\": \"L004\", \"text\": \"Routine check on the HVAC system. Cleaned filters and checked coolant levels. All nominal.\"},\n",
    "    {\"log_id\": \"L005\", \"text\": \"Emergency stop button on Assembly Line 2 was faulty. Replaced the entire switch assembly. Pruebas completadas.\"},\n",
    "    {\"log_id\": \"L006\", \"text\": \"Welding robot WR-08 reported joint misalignment. Ran diagnostic and recalibrated arm. Positional error is now less than 0.1mm.\"},\n",
    "    {\"log_id\": \"L007\", \"text\": \"Power supply for the packaging machine failed. Swapped with a new PSU (Part #PSU-123). El sistema estÃ¡ funcionando.\"},\n",
    "    {\"log_id\": \"L008\", \"text\": \"Updated firmware on all PLCs to version 3.4.1. System rebooted without issues.\"},\n",
    "    {\"log_id\": \"L009\", \"text\": \"Investigated high-temperature alarm on Furnace-01. Found a faulty thermocouple. Replaced and tested. Temp stable at 900Â°C.\"},\n",
    "    {\"log_id\": \"L010\", \"text\": \"La carretilla elevadora F-05 necesita una recarga de baterÃ­a. Maintenance scheduled.\"}\n",
    "]\n",
    "logs = pd.DataFrame(logs_data)\n",
    "logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b88598",
   "metadata": {},
   "source": [
    "### ðŸ”„ Text Normalization\n",
    "Normalize units, tokenize multilingual text, and preserve domain-specific numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d154ff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_tokenize(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Cleans and tokenizes maintenance log text.\n",
    "    - Converts to lowercase\n",
    "    - Adds spaces around units and special characters for better tokenization\n",
    "    - Removes punctuation\n",
    "    - Splits into tokens\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    # Add padding to units and part numbers to treat them as separate tokens\n",
    "    text = re.sub(r'(\\d+)\\s*(nm|mm|psi|kpa|Â°c)', r' \\1 \\2 ', text)\n",
    "    text = re.sub(r'\\(part\\s*#([\\w-]+)\\)', r' part_\\1 ', text)\n",
    "    # Remove punctuation except for underscores and hyphens in part numbers\n",
    "    text = re.sub(r'[^\\w\\s-]', '', text)\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "logs['tokens'] = logs['text'].apply(normalize_and_tokenize)\n",
    "logs[['text', 'tokens']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6b1068",
   "metadata": {},
   "source": [
    "## ðŸ§® Bigram Model Implementation\n",
    "We collect transition counts and transform them into log probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059ef013",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = '<s>'\n",
    "STOP_TOKEN = '</s>'\n",
    "\n",
    "def build_bigram_counts(token_sequences):\n",
    "    \"\"\"Builds a vocabulary and counts of token pairs (bigrams).\"\"\"\n",
    "    counts = defaultdict(Counter)\n",
    "    vocab = set()\n",
    "    for seq in token_sequences:\n",
    "        # Add start and stop tokens to each sequence\n",
    "        full_seq = [START_TOKEN] + seq + [STOP_TOKEN]\n",
    "        vocab.update(full_seq)\n",
    "        for prev_token, next_token in zip(full_seq, full_seq[1:]):\n",
    "            counts[prev_token][next_token] += 1\n",
    "    return counts, vocab\n",
    "\n",
    "counts, vocab = build_bigram_counts(logs['tokens'])\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "# Example: See what follows the token 'replaced'\n",
    "print(\"\\nTokens that follow 'replaced':\")\n",
    "print(counts['replaced'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec29e63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_log_probs(counts, vocab, smoothing=1.0):\n",
    "    \"\"\"\n",
    "    Converts bigram counts to log probabilities with add-one smoothing.\n",
    "    Using log probabilities helps prevent underflow with long sequences.\n",
    "    \"\"\"\n",
    "    vocab_size = len(vocab)\n",
    "    # Calculate a default log probability for unseen bigrams.\n",
    "    # This is the probability of an unseen word following a given word.\n",
    "    default_log_prob = math.log(smoothing / (smoothing * vocab_size))\n",
    "    \n",
    "    # The outer defaultdict provides a default dictionary for unseen prev_tokens.\n",
    "    # The inner defaultdict provides the default_log_prob for unseen next_tokens.\n",
    "    probs = defaultdict(lambda: defaultdict(lambda: default_log_prob))\n",
    "\n",
    "    for prev_token, next_counts in counts.items():\n",
    "        total_count = sum(next_counts.values())\n",
    "        denominator = total_count + smoothing * vocab_size\n",
    "        \n",
    "        # For each token that *could* follow prev_token, calculate its smoothed probability.\n",
    "        # We only need to iterate through the tokens that actually appeared after prev_token.\n",
    "        # The defaultdict will handle all other unseen next_tokens.\n",
    "        for next_token, count in next_counts.items():\n",
    "            numerator = count + smoothing\n",
    "            prob = numerator / denominator\n",
    "            probs[prev_token][next_token] = math.log(prob)\n",
    "            \n",
    "    return probs\n",
    "\n",
    "log_probs = bigram_log_probs(counts, vocab)\n",
    "\n",
    "# Display some example log probabilities\n",
    "print(\"Log probabilities of tokens following the start token:\")\n",
    "sorted_start_probs = sorted(log_probs[START_TOKEN].items(), key=lambda item: item[1], reverse=True)\n",
    "for token, log_prob in sorted_start_probs[:5]:\n",
    "    print(f\"  '{token}': {log_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779cbc08",
   "metadata": {},
   "source": [
    "### ðŸ“‰ Perplexity Calculation\n",
    "Evaluate how well the model predicts held-out sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc6fc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_log_prob(tokens, log_probs):\n",
    "    \"\"\"Calculates the total log probability of a sequence.\"\"\"\n",
    "    tokens = [START_TOKEN] + tokens + [STOP_TOKEN]\n",
    "    logp = 0.0\n",
    "    for prev_token, next_token in zip(tokens, tokens[1:]):\n",
    "        logp += log_probs[prev_token].get(next_token, -np.inf) # Use get for safety, though defaultdict handles it\n",
    "    return logp\n",
    "\n",
    "def perplexity(dataset, log_probs):\n",
    "    \"\"\"\n",
    "    Calculates perplexity, a measure of how well a model predicts a sample.\n",
    "    Lower is better.\n",
    "    \"\"\"\n",
    "    total_logp = 0.0\n",
    "    total_tokens = 0\n",
    "    for tokens in dataset:\n",
    "        # Add 2 to token count for start/stop tokens\n",
    "        total_tokens += len(tokens) + 2\n",
    "        total_logp += sentence_log_prob(tokens, log_probs)\n",
    "\n",
    "    # Perplexity is e^(-L), where L is the average log probability per token\n",
    "    avg_logp = total_logp / max(total_tokens, 1)\n",
    "    return math.exp(-avg_logp)\n",
    "\n",
    "# Split data for a simple train/test evaluation\n",
    "train_data = logs['tokens'][:8]\n",
    "test_data = logs['tokens'][8:]\n",
    "\n",
    "# Build model on training data\n",
    "train_counts, train_vocab = build_bigram_counts(train_data)\n",
    "train_log_probs = bigram_log_probs(train_counts)\n",
    "\n",
    "# Evaluate on test data\n",
    "pp = perplexity(test_data, train_log_probs)\n",
    "print(f\"Perplexity on test data: {pp:.2f}\")\n",
    "\n",
    "# Example sentence probability\n",
    "test_sentence = ['updated', 'firmware', 'on', 'all', 'plcs']\n",
    "prob = sentence_log_prob(test_sentence, train_log_probs)\n",
    "print(f\"Log probability of a test sentence: {prob:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a4d4e0",
   "metadata": {},
   "source": [
    "## ðŸ§ª Experiment: Extend to Trigrams\n",
    "Use this cell as a template for students to implement trigram logic and compare perplexities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2291c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lab): implement trigram counts, probabilities, and perplexity\n",
    "# Document perplexity reduction and note failure cases (e.g., rare part numbers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3195da7",
   "metadata": {},
   "source": [
    "## ðŸ©º Error Analysis Framework\n",
    "- Inspect top 20 predicted next tokens for bilingual sentences.\n",
    "- Flag mispredictions on torque specs, units, and compliance phrases.\n",
    "- Track per-language perplexity to justify multilingual adapters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c5633d",
   "metadata": {},
   "source": [
    "## ðŸ§ª Lab Assignment\n",
    "1. Swap in three months of real maintenance logs and compute bigram perplexity.\n",
    "2. Implement trigram smoothing (Kneserâ€“Ney or Goodâ€“Turing) and compare results.\n",
    "3. Document failure modes tied to acronyms, bilingual phrases, and numerical tolerances.\n",
    "4. Present a memo recommending whether to proceed with larger-scale transformer pre-training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97f084d",
   "metadata": {},
   "source": [
    "## âœ… Checklist\n",
    "- [ ] Logs normalized with consistent unit handling\n",
    "- [ ] Bigram model implemented with smoothing\n",
    "- [ ] Perplexity baseline captured per language\n",
    "- [ ] Failure report shared with maintenance SMEs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76192f70",
   "metadata": {},
   "source": [
    "## ðŸ“š References\n",
    "- Jurafsky & Martin, *Speech and Language Processing* (Chapter on N-grams)\n",
    "- TorchText tutorials on language modeling\n",
    "- *Multilingual Maintenance Communications* (Society of Manufacturing Engineers, 2024)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
