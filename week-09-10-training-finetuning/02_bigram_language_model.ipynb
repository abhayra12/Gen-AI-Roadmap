{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ddcb507",
   "metadata": {},
   "source": [
    "# 🔡 Week 09-10 · Notebook 02 · Bigram Language Model with Maintenance Logs\n",
    "\n",
    "Build a from-scratch bigram language model to understand token statistics inside manufacturing maintenance notes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c30e13e",
   "metadata": {},
   "source": [
    "## 🎯 Learning Objectives\n",
    "- Clean and normalize maintenance text for n-gram modeling.\n",
    "- Implement a bigram language model in PyTorch and compute perplexity.\n",
    "- Analyze jargon-heavy vs. plain-language performance.\n",
    "- Document failure modes relevant to plant-floor deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca18d329",
   "metadata": {},
   "source": [
    "## 🧩 Scenario\n",
    "Technicians log work orders on a shared tablet. Autocomplete powered by a small language model can reduce typing time, but it must understand torque specs, part numbers, and bilingual notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e58898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import torch\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5870b61f",
   "metadata": {},
   "source": [
    "## 🛠️ Maintenance Log Samples\n",
    "Synthetic logs mimic common maintenance narratives. Replace with your CMMS export to train on real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0929b578",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_data = [\n",
    "    {\"log_id\": \"L001\", \"text\": \"Replaced the main conveyor belt motor (Part #MTR-789). Torqued bolts to 150 Nm. System back online.\"},\n",
    "    {\"log_id\": \"L002\", \"text\": \"Calibrated the CNC machine's laser sensor. Accuracy is now within 0.05mm tolerance. Se requiere una revisión en 24 horas.\"},\n",
    "    {\"log_id\": \"L003\", \"text\": \"Hydraulic fluid leak detected on Press-03. Replaced seal (Part #SL-456) and refilled fluid. Pressure at 2000 psi.\"},\n",
    "    {\"log_id\": \"L004\", \"text\": \"Routine check on the HVAC system. Cleaned filters and checked coolant levels. All nominal.\"},\n",
    "    {\"log_id\": \"L005\", \"text\": \"Emergency stop button on Assembly Line 2 was faulty. Replaced the entire switch assembly. Pruebas completadas.\"},\n",
    "    {\"log_id\": \"L006\", \"text\": \"Welding robot WR-08 reported joint misalignment. Ran diagnostic and recalibrated arm. Positional error is now less than 0.1mm.\"},\n",
    "    {\"log_id\": \"L007\", \"text\": \"Power supply for the packaging machine failed. Swapped with a new PSU (Part #PSU-123). El sistema está funcionando.\"},\n",
    "    {\"log_id\": \"L008\", \"text\": \"Updated firmware on all PLCs to version 3.4.1. System rebooted without issues.\"},\n",
    "    {\"log_id\": \"L009\", \"text\": \"Investigated high-temperature alarm on Furnace-01. Found a faulty thermocouple. Replaced and tested. Temp stable at 900°C.\"},\n",
    "    {\"log_id\": \"L010\", \"text\": \"La carretilla elevadora F-05 necesita una recarga de batería. Maintenance scheduled.\"}\n",
    "]\n",
    "logs = pd.DataFrame(logs_data)\n",
    "logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b88598",
   "metadata": {},
   "source": [
    "### 🔄 Text Normalization\n",
    "Normalize units, tokenize multilingual text, and preserve domain-specific numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d154ff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_tokenize(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Cleans and tokenizes maintenance log text.\n",
    "    - Converts to lowercase\n",
    "    - Adds spaces around units and special characters for better tokenization\n",
    "    - Removes punctuation\n",
    "    - Splits into tokens\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    # Add padding to units and part numbers to treat them as separate tokens\n",
    "    text = re.sub(r'(\\d+)\\s*(nm|mm|psi|kpa|°c)', r' \\1 \\2 ', text)\n",
    "    text = re.sub(r'\\(part\\s*#([\\w-]+)\\)', r' part_\\1 ', text)\n",
    "    # Remove punctuation except for underscores and hyphens in part numbers\n",
    "    text = re.sub(r'[^\\w\\s-]', '', text)\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "logs['tokens'] = logs['text'].apply(normalize_and_tokenize)\n",
    "logs[['text', 'tokens']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6b1068",
   "metadata": {},
   "source": [
    "## 🧮 Bigram Model Implementation\n",
    "We collect transition counts and transform them into log probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059ef013",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = '<s>'\n",
    "STOP_TOKEN = '</s>'\n",
    "\n",
    "def build_bigram_counts(token_sequences):\n",
    "    \"\"\"Builds a vocabulary and counts of token pairs (bigrams).\"\"\"\n",
    "    counts = defaultdict(Counter)\n",
    "    vocab = set()\n",
    "    for seq in token_sequences:\n",
    "        # Add start and stop tokens to each sequence\n",
    "        full_seq = [START_TOKEN] + seq + [STOP_TOKEN]\n",
    "        vocab.update(full_seq)\n",
    "        for prev_token, next_token in zip(full_seq, full_seq[1:]):\n",
    "            counts[prev_token][next_token] += 1\n",
    "    return counts, vocab\n",
    "\n",
    "counts, vocab = build_bigram_counts(logs['tokens'])\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "# Example: See what follows the token 'replaced'\n",
    "print(\"\\nTokens that follow 'replaced':\")\n",
    "print(counts['replaced'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec29e63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_log_probs(counts, vocab, smoothing=1.0):\n",
    "    \"\"\"\n",
    "    Converts bigram counts to log probabilities with add-one smoothing.\n",
    "    Using log probabilities helps prevent underflow with long sequences.\n",
    "    \"\"\"\n",
    "    vocab_size = len(vocab)\n",
    "    # Calculate a default log probability for unseen bigrams.\n",
    "    # This is the probability of an unseen word following a given word.\n",
    "    default_log_prob = math.log(smoothing / (smoothing * vocab_size))\n",
    "    \n",
    "    # The outer defaultdict provides a default dictionary for unseen prev_tokens.\n",
    "    # The inner defaultdict provides the default_log_prob for unseen next_tokens.\n",
    "    probs = defaultdict(lambda: defaultdict(lambda: default_log_prob))\n",
    "\n",
    "    for prev_token, next_counts in counts.items():\n",
    "        total_count = sum(next_counts.values())\n",
    "        denominator = total_count + smoothing * vocab_size\n",
    "        \n",
    "        # For each token that *could* follow prev_token, calculate its smoothed probability.\n",
    "        # We only need to iterate through the tokens that actually appeared after prev_token.\n",
    "        # The defaultdict will handle all other unseen next_tokens.\n",
    "        for next_token, count in next_counts.items():\n",
    "            numerator = count + smoothing\n",
    "            prob = numerator / denominator\n",
    "            probs[prev_token][next_token] = math.log(prob)\n",
    "            \n",
    "    return probs\n",
    "\n",
    "log_probs = bigram_log_probs(counts, vocab)\n",
    "\n",
    "# Display some example log probabilities\n",
    "print(\"Log probabilities of tokens following the start token:\")\n",
    "sorted_start_probs = sorted(log_probs[START_TOKEN].items(), key=lambda item: item[1], reverse=True)\n",
    "for token, log_prob in sorted_start_probs[:5]:\n",
    "    print(f\"  '{token}': {log_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779cbc08",
   "metadata": {},
   "source": [
    "### 📉 Perplexity Calculation\n",
    "Evaluate how well the model predicts held-out sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc6fc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_log_prob(tokens, log_probs):\n",
    "    \"\"\"Calculates the total log probability of a sequence.\"\"\"\n",
    "    tokens = [START_TOKEN] + tokens + [STOP_TOKEN]\n",
    "    logp = 0.0\n",
    "    for prev_token, next_token in zip(tokens, tokens[1:]):\n",
    "        logp += log_probs[prev_token].get(next_token, -np.inf) # Use get for safety, though defaultdict handles it\n",
    "    return logp\n",
    "\n",
    "def perplexity(dataset, log_probs):\n",
    "    \"\"\"\n",
    "    Calculates perplexity, a measure of how well a model predicts a sample.\n",
    "    Lower is better.\n",
    "    \"\"\"\n",
    "    total_logp = 0.0\n",
    "    total_tokens = 0\n",
    "    for tokens in dataset:\n",
    "        # Add 2 to token count for start/stop tokens\n",
    "        total_tokens += len(tokens) + 2\n",
    "        total_logp += sentence_log_prob(tokens, log_probs)\n",
    "\n",
    "    # Perplexity is e^(-L), where L is the average log probability per token\n",
    "    avg_logp = total_logp / max(total_tokens, 1)\n",
    "    return math.exp(-avg_logp)\n",
    "\n",
    "# Split data for a simple train/test evaluation\n",
    "train_data = logs['tokens'][:8]\n",
    "test_data = logs['tokens'][8:]\n",
    "\n",
    "# Build model on training data\n",
    "train_counts, train_vocab = build_bigram_counts(train_data)\n",
    "train_log_probs = bigram_log_probs(train_counts)\n",
    "\n",
    "# Evaluate on test data\n",
    "pp = perplexity(test_data, train_log_probs)\n",
    "print(f\"Perplexity on test data: {pp:.2f}\")\n",
    "\n",
    "# Example sentence probability\n",
    "test_sentence = ['updated', 'firmware', 'on', 'all', 'plcs']\n",
    "prob = sentence_log_prob(test_sentence, train_log_probs)\n",
    "print(f\"Log probability of a test sentence: {prob:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a4d4e0",
   "metadata": {},
   "source": [
    "## 🧪 Experiment: Extend to Trigrams\n",
    "Use this cell as a template for students to implement trigram logic and compare perplexities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2291c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lab): implement trigram counts, probabilities, and perplexity\n",
    "# Document perplexity reduction and note failure cases (e.g., rare part numbers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3195da7",
   "metadata": {},
   "source": [
    "## 🩺 Error Analysis Framework\n",
    "- Inspect top 20 predicted next tokens for bilingual sentences.\n",
    "- Flag mispredictions on torque specs, units, and compliance phrases.\n",
    "- Track per-language perplexity to justify multilingual adapters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c5633d",
   "metadata": {},
   "source": [
    "## 🧪 Lab Assignment\n",
    "1. Swap in three months of real maintenance logs and compute bigram perplexity.\n",
    "2. Implement trigram smoothing (Kneser–Ney or Good–Turing) and compare results.\n",
    "3. Document failure modes tied to acronyms, bilingual phrases, and numerical tolerances.\n",
    "4. Present a memo recommending whether to proceed with larger-scale transformer pre-training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97f084d",
   "metadata": {},
   "source": [
    "## ✅ Checklist\n",
    "- [ ] Logs normalized with consistent unit handling\n",
    "- [ ] Bigram model implemented with smoothing\n",
    "- [ ] Perplexity baseline captured per language\n",
    "- [ ] Failure report shared with maintenance SMEs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76192f70",
   "metadata": {},
   "source": [
    "## 📚 References\n",
    "- Jurafsky & Martin, *Speech and Language Processing* (Chapter on N-grams)\n",
    "- TorchText tutorials on language modeling\n",
    "- *Multilingual Maintenance Communications* (Society of Manufacturing Engineers, 2024)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
