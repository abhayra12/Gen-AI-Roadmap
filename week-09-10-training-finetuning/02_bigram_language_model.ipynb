{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ddcb507",
   "metadata": {},
   "source": [
    "# 🔡 Week 09-10 · Notebook 02 · Bigram Language Model with Maintenance Logs\n",
    "\n",
    "Build a from-scratch bigram language model to understand token statistics inside manufacturing maintenance notes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c30e13e",
   "metadata": {},
   "source": [
    "## 🎯 Learning Objectives\n",
    "- Clean and normalize maintenance text for n-gram modeling.\n",
    "- Implement a bigram language model in PyTorch and compute perplexity.\n",
    "- Analyze jargon-heavy vs. plain-language performance.\n",
    "- Document failure modes relevant to plant-floor deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca18d329",
   "metadata": {},
   "source": [
    "## 🧩 Scenario\n",
    "Technicians log work orders on a shared tablet. Autocomplete powered by a small language model can reduce typing time, but it must understand torque specs, part numbers, and bilingual notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e58898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import torch\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5870b61f",
   "metadata": {},
   "source": [
    "## 🛠️ Maintenance Log Samples\n",
    "Synthetic logs mimic common maintenance narratives. Replace with your CMMS export to train on real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0929b578",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = pd.DataFrame(["
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b88598",
   "metadata": {},
   "source": [
    "### 🔄 Text Normalization\n",
    "Normalize units, tokenize multilingual text, and preserve domain-specific numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d154ff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text: str) -> list[str]:\n",
    "    text = text.lower()\n",
    "    text = text.replace('psi', ' psi ').replace('nm', ' nm ').replace('kpa', ' kpa ')\n",
    "    text = re.sub(r'[^]', ' ', text)\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "logs['tokens'] = logs['text'].apply(normalize)\n",
    "logs[['text', 'tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6b1068",
   "metadata": {},
   "source": [
    "## 🧮 Bigram Model Implementation\n",
    "We collect transition counts and transform them into log probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059ef013",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = '<s>'\n",
    "STOP_TOKEN = '</s>'\n",
    "\n",
    "def build_bigram_counts(token_sequences):\n",
    "    counts = defaultdict(Counter)\n",
    "    vocab = set()\n",
    "    for seq in token_sequences:\n",
    "    return counts, vocab\n",
    "\n",
    "counts, vocab = build_bigram_counts(logs['tokens'])\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec29e63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_probs(counts, smoothing=1.0):\n",
    "    probs = {}\n",
    "    vocab = set(counts.keys())\n",
    "    for prev_token, next_counts in counts.items():\n",
    "    return probs\n",
    "\n",
    "probs = bigram_probs(counts)\n",
    "list(probs[START_TOKEN].items())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779cbc08",
   "metadata": {},
   "source": [
    "### 📉 Perplexity Calculation\n",
    "Evaluate how well the model predicts held-out sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc6fc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_log_prob(tokens, probs):\n",
    "    tokens = [START_TOKEN] + tokens + [STOP_TOKEN]\n",
    "    logp = 0.0\n",
    "    for prev_token, next_token in zip(tokens, tokens[1:]):\n",
    "    return logp\n",
    "\n",
    "def perplexity(dataset, probs):\n",
    "    logp_sum, token_count = 0.0, 0\n",
    "    for tokens in dataset:\n",
    "    return math.exp(-logp_sum / max(token_count, 1))\n",
    "\n",
    "pp = perplexity(logs['tokens'], probs)\n",
    "pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a4d4e0",
   "metadata": {},
   "source": [
    "## 🧪 Experiment: Extend to Trigrams\n",
    "Use this cell as a template for students to implement trigram logic and compare perplexities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2291c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lab): implement trigram counts, probabilities, and perplexity\n",
    "# Document perplexity reduction and note failure cases (e.g., rare part numbers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3195da7",
   "metadata": {},
   "source": [
    "## 🩺 Error Analysis Framework\n",
    "- Inspect top 20 predicted next tokens for bilingual sentences.\n",
    "- Flag mispredictions on torque specs, units, and compliance phrases.\n",
    "- Track per-language perplexity to justify multilingual adapters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c5633d",
   "metadata": {},
   "source": [
    "## 🧪 Lab Assignment\n",
    "1. Swap in three months of real maintenance logs and compute bigram perplexity.\n",
    "2. Implement trigram smoothing (Kneser–Ney or Good–Turing) and compare results.\n",
    "3. Document failure modes tied to acronyms, bilingual phrases, and numerical tolerances.\n",
    "4. Present a memo recommending whether to proceed with larger-scale transformer pre-training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97f084d",
   "metadata": {},
   "source": [
    "## ✅ Checklist\n",
    "- [ ] Logs normalized with consistent unit handling\n",
    "- [ ] Bigram model implemented with smoothing\n",
    "- [ ] Perplexity baseline captured per language\n",
    "- [ ] Failure report shared with maintenance SMEs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76192f70",
   "metadata": {},
   "source": [
    "## 📚 References\n",
    "- Jurafsky & Martin, *Speech and Language Processing* (Chapter on N-grams)\n",
    "- TorchText tutorials on language modeling\n",
    "- *Multilingual Maintenance Communications* (Society of Manufacturing Engineers, 2024)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
