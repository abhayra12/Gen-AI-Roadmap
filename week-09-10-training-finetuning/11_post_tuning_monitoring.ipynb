{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "555b4316",
   "metadata": {},
   "source": [
    "# 📡 Week 09-10 · Notebook 11 · Post-Tuning Evaluation, Monitoring & Drift Management\n",
    "\n",
    "Operationalize continuous evaluation, drift detection, and rollback for fine-tuned manufacturing copilots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa53b49",
   "metadata": {},
   "source": [
    "## 🎯 Learning Objectives\n",
    "- **Build Evaluation Harnesses:** Create a robust evaluation framework that combines automated metrics (e.g., ROUGE, BLEU) with structured human-in-the-loop (HITL) reviews.\n",
    "- **Monitor for Drift:** Implement strategies to detect embedding drift (semantic shift in user queries) and response quality degradation over time.\n",
    "- **Deploy with Safeguards:** Understand and apply deployment strategies like canary releases and shadow testing to de-risk model updates.\n",
    "- **Establish Governance SOPs:** Draft a Standard Operating Procedure (SOP) for model rollback and incident response, aligned with plant governance policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b6a960",
   "metadata": {},
   "source": [
    "## 🧩 Scenario\n",
    "A fine-tuned assistant is live in four plants. Leadership wants weekly monitoring packs summarizing accuracy, safety, and drift signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b446fc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.metrics.pairwise import cosine_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f5be75",
   "metadata": {},
   "source": [
    "## 🧪 Evaluation & Monitoring Log Schema\n",
    "\n",
    "To monitor the model in production, we need to log every request and response. This structured log is the foundation for all post-deployment analysis.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"request_id\": \"uuid-1234-abcd\",\n",
    "  \"timestamp\": \"2024-10-28T10:05:00Z\",\n",
    "  \"plant_id\": \"Plant_B\",\n",
    "  \"model_version\": \"sop-assistant-v1.1\",\n",
    "  \"user_prompt\": \"What is the lubrication schedule for the main conveyor?\",\n",
    "  \"prompt_embedding\": [0.12, -0.05, ...], // 768-dim embedding\n",
    "  \"generated_response\": \"The main conveyor requires lubrication every 200 operating hours. Use ISO VG 46 oil.\",\n",
    "  \"response_embedding\": [0.08, 0.21, ...],\n",
    "  \"latency_ms\": 250,\n",
    "  \"human_feedback_score\": 4, // Optional: 1-5 scale from a user feedback button\n",
    "  \"contains_safety_keyword\": false // Flagged by a post-processing check\n",
    "}\n",
    "```\n",
    "\n",
    "This schema captures not just the text, but also the semantic meaning (embeddings) and performance metrics needed for drift detection and KPI tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d209e3ec",
   "metadata": {},
   "source": [
    "### 1. Generate Synthetic Production Logs\n",
    "\n",
    "First, we'll create a synthetic dataset that mimics the production logs from our deployed Manufacturing Copilot. This data is crucial for simulating our monitoring and drift detection workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671559b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prod_logs(num_logs=500):\n",
    "    \"\"\"Generates a DataFrame of synthetic production logs for the Manufacturing Copilot.\"\"\"\n",
    "    logs = []\n",
    "    base_time = datetime.now()\n",
    "    plants = [\"Plant_A\", \"Plant_B\", \"Plant_C\", \"Plant_D\"]\n",
    "    \n",
    "    # Pre-generate random choices to speed up the loop\n",
    "    plant_choices = np.random.choice(plants, size=num_logs)\n",
    "    latency_values = np.random.randint(150, 500, size=num_logs)\n",
    "    feedback_scores = np.random.choice([3, 4, 5, 5], p=[0.1, 0.2, 0.6, 0.1], size=num_logs) # Adjusted probabilities\n",
    "    safety_flags = np.random.choice([True, False], p=[0.05, 0.95], size=num_logs)\n",
    "\n",
    "    for i in range(num_logs):\n",
    "        logs.append({\n",
    "            \"timestamp\": base_time - timedelta(hours=i),\n",
    "            \"plant_id\": plant_choices[i],\n",
    "            \"model_version\": \"sop-assistant-v1.1\",\n",
    "            \"latency_ms\": latency_values[i],\n",
    "            \"human_feedback_score\": feedback_scores[i],\n",
    "            \"contains_safety_keyword\": safety_flags[i]\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(logs)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    return df\n",
    "\n",
    "# Generate and display the logs\n",
    "prod_logs = generate_prod_logs(num_logs=500)\n",
    "print(\"Generated Production Logs:\")\n",
    "prod_logs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916528b4",
   "metadata": {},
   "source": [
    "### 2. Calculate and Display KPI Dashboard\n",
    "\n",
    "With the logs collected, we can compute key performance indicators (KPIs). This dashboard provides a high-level weekly overview for plant leadership, enabling them to quickly spot anomalies. A sudden drop in feedback score at one plant, for instance, would trigger an immediate investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f4cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calculate KPIs ---\n",
    "kpi_df = prod_logs.groupby('plant_id').agg(\n",
    "    avg_latency_ms=('latency_ms', 'mean'),\n",
    "    avg_feedback_score=('human_feedback_score', 'mean'),\n",
    "    safety_incidents=('contains_safety_keyword', lambda x: x.sum())\n",
    ").round(2)\n",
    "\n",
    "print(\"--- Weekly KPI Dashboard ---\")\n",
    "kpi_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a97f113",
   "metadata": {},
   "source": [
    "### 3. Monitor for Embedding Drift\n",
    "\n",
    "**Data drift** is a critical concept in MLOps. It occurs when the statistical properties of the production data change over time, making the model's original training data less relevant. For an LLM, this often manifests as **embedding drift** or **concept drift**, where the topics and semantics of user prompts shift.\n",
    "\n",
    "We can detect this by comparing the embeddings of recent prompts to a baseline established during training (e.g., from the validation set). A significant divergence, measured by cosine distance, indicates that the model's knowledge may be becoming stale. This is a primary signal for triggering a retraining or fine-tuning cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29742a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Simulate Embedding Drift ---\n",
    "# In a real scenario, these would be loaded from a vector DB or log storage.\n",
    "# Baseline embeddings from the validation set (e.g., centered around a certain point)\n",
    "baseline_embeddings = np.random.randn(100, 768) * 0.1 + 0.5\n",
    "\n",
    "# Current week's production prompt embeddings\n",
    "# Let's simulate a drift by changing the mean, mimicking a shift in user topics.\n",
    "current_embeddings = np.random.randn(50, 768) * 0.1 + 0.8 \n",
    "\n",
    "# --- Calculate Drift using Centroid Cosine Distance ---\n",
    "# We compute the average cosine distance between the centroid (average vector) of the\n",
    "# current embeddings and the centroid of the baseline embeddings.\n",
    "baseline_centroid = baseline_embeddings.mean(axis=0).reshape(1, -1)\n",
    "current_centroid = current_embeddings.mean(axis=0).reshape(1, -1)\n",
    "\n",
    "# Cosine distance is (1 - cosine similarity). A higher value means more difference.\n",
    "drift_score = cosine_distances(baseline_centroid, current_centroid)[0, 0]\n",
    "\n",
    "print(f\"--- Embedding Drift Report ---\")\n",
    "print(f\"Baseline Centroid Norm: {np.linalg.norm(baseline_centroid):.2f}\")\n",
    "print(f\"Current Centroid Norm:  {np.linalg.norm(current_centroid):.2f}\")\n",
    "print(f\"Drift Score (Cosine Distance): {drift_score:.4f}\")\n",
    "\n",
    "# --- Governance Rule ---\n",
    "# This threshold must be tuned based on historical data and business tolerance.\n",
    "DRIFT_THRESHOLD = 0.02 \n",
    "\n",
    "if drift_score > DRIFT_THRESHOLD:\n",
    "    print(f\"\\n🚨 ALERT: Drift score ({drift_score:.4f}) exceeds threshold of {DRIFT_THRESHOLD}. Retraining may be required.\")\n",
    "else:\n",
    "    print(f\"\\n✅ OK: Drift score ({drift_score:.4f}) is within acceptable limits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac905f19",
   "metadata": {},
   "source": [
    "### 4. Generate Automated Governance Report\n",
    "\n",
    "This final step automates the creation of a weekly performance report. The report, formatted in Markdown, summarizes KPIs and drift analysis, providing clear status indicators and recommended actions. This artifact is ready to be emailed to stakeholders or posted to a documentation portal, ensuring transparent and consistent governance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15721129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# Determine status based on drift score\n",
    "drift_status = \"ALERT\" if drift_score > DRIFT_THRESHOLD else \"OK\"\n",
    "status_color = \"red\" if drift_status == \"ALERT\" else \"green\"\n",
    "\n",
    "summary_text = \"Significant prompt drift detected. An investigation into new user query patterns is recommended.\" if drift_status == \"ALERT\" else \"No significant anomalies detected.\"\n",
    "action_items = \"1. Analyze new query clusters from production logs.\\n    - 2. Curate new training data reflecting these patterns.\\n    - 3. Schedule a potential retraining cycle.\" if drift_status == \"ALERT\" else \"- None.\"\n",
    "\n",
    "# --- Build Markdown Report ---\n",
    "report_template = f\"\"\"\n",
    "# Weekly Manufacturing Copilot Performance Report\n",
    "\n",
    "**Date:** {datetime.now().strftime('%Y-%m-%d')}\n",
    "**Model Version:** sop-assistant-v1.1\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Key Performance Indicators (KPIs)\n",
    "\n",
    "| Plant | Avg. Latency (ms) | Avg. Feedback Score | Safety Incidents |\n",
    "|:---|:---|:---|:---|\n",
    "\"\"\"\n",
    "# Populate KPI table\n",
    "for index, row in kpi_df.iterrows():\n",
    "    report_template += f\"| **{index}** | {row['avg_latency_ms']:.0f} | {row['avg_feedback_score']:.2f} | {int(row['safety_incidents'])} |\\n\"\n",
    "\n",
    "report_template += f\"\"\"\n",
    "---\n",
    "\n",
    "## 2. Model Drift Analysis\n",
    "\n",
    "- **Prompt Embedding Drift Score:** `{drift_score:.4f}`\n",
    "- **Drift Threshold:** `{DRIFT_THRESHOLD}`\n",
    "- **Status:** <font color='{status_color}'>**{drift_status}**</font>\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Summary & Actions\n",
    "\n",
    "- **Overall Performance:** The model is performing within expected parameters, but requires attention regarding data drift.\n",
    "- **Anomalies:** {summary_text}\n",
    "- **Action Items:**\n",
    "    - {action_items}\n",
    "\"\"\"\n",
    "\n",
    "# Display the rendered Markdown report\n",
    "Markdown(report_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89da1ec",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🛡️ Deployment Strategies & Rollback SOP\n",
    "\n",
    "While monitoring tells us *when* to act, we also need robust procedures for *how* to act. This involves safe deployment strategies to de-risk model updates and clear Standard Operating Procedures (SOPs) for handling incidents.\n",
    "\n",
    "### Deployment Strategies for Safe Rollouts\n",
    "\n",
    "1.  **Canary Release:**\n",
    "    -   **Concept:** Route a small percentage of live traffic (e.g., 5% of users from a single plant) to the new model version (`v1.2`), while the majority remains on the stable version (`v1.1`).\n",
    "    -   **Benefit:** Limits the \"blast radius\" of any potential issues. It provides real-world performance data from a small, controlled user group before a full rollout.\n",
    "    -   **Procedure:** If KPIs for the canary group remain stable and positive for a predefined period (e.g., 48 hours), gradually increase the traffic split (25%, 50%, and finally 100%).\n",
    "\n",
    "2.  **Shadow Testing (or Dark Launch):**\n",
    "    -   **Concept:** Run the new model `v1.2` in parallel with the current model `v1.1`. Live traffic is served by `v1.1`, but a copy of each request is also sent to `v1.2` \"in the dark.\" The `v1.2` responses are logged but not sent to the user.\n",
    "    -   **Benefit:** Allows you to compare the outputs of the new model against the old one on 100% of live traffic with zero risk to the user experience. You can analyze discrepancies, performance, and error rates offline.\n",
    "    -   **Procedure:** Log cases where `v1.2`'s response differs significantly from `v1.1` (e.g., using embedding distance or keyword checks). Use this analysis to find bugs or unexpected behaviors before committing to a release.\n",
    "\n",
    "### Standard Operating Procedure (SOP): Model Rollback\n",
    "\n",
    "A rollback SOP is a non-negotiable component of production MLOps. It ensures a rapid, predictable response to model failures, minimizing impact on business operations.\n",
    "\n",
    "---\n",
    "**SOP-MLOPS-003: Emergency Model Rollback**\n",
    "\n",
    "-   **Trigger Conditions (any of the following):**\n",
    "    1.  Critical safety incident rate (as defined by `contains_safety_keyword`) increases by > 1% over a 12-hour rolling window.\n",
    "    2.  Average human feedback score drops by > 0.5 points over a 24-hour period across any plant.\n",
    "    3.  P95 latency increases by > 50% for more than 1 hour.\n",
    "    4.  The model generates responses that are confirmed to violate a documented safety or compliance rule (e.g., hallucinating a dangerous chemical mixture).\n",
    "\n",
    "-   **Procedure:**\n",
    "    1.  **Immediate Action (On-call MLOps Engineer):** Via the API gateway or load balancer, immediately re-route 100% of traffic back to the previous stable model version (e.g., from `v1.2` to `v1.1`).\n",
    "    2.  **Communication:** Post a status update in the designated incident channel (e.g., `#manufacturing-ai-status`), notifying stakeholders that a rollback has occurred and the system is stable.\n",
    "    3.  **Investigation:** Create a high-priority incident ticket. The AI/MLOps team must perform a root cause analysis (RCA) using the production logs leading up to the incident.\n",
    "    4.  **Resolution:** The problematic model version (`v1.2`) is quarantined. It cannot be redeployed until the root cause is fixed, the model is re-validated in a staging environment, and the fix is approved by the AI Governance Board.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6588577",
   "metadata": {},
   "source": [
    "## 🧪 Lab Assignment\n",
    "\n",
    "1.  **Enhance the KPI Dashboard:**\n",
    "    *   Add a new KPI to the `kpi_df` DataFrame: `p95_latency_ms`. This should calculate the 95th percentile of latency for each plant. Use the `.quantile(0.95)` method within the `agg` function.\n",
    "    *   Update the final Markdown report to include this new P95 Latency column.\n",
    "\n",
    "2.  **Simulate a \"No Drift\" Scenario:**\n",
    "    *   In the \"Monitor for Embedding Drift\" section, create a second set of `current_embeddings_no_drift`.\n",
    "    *   These embeddings should be generated with the *same mean* as the `baseline_embeddings` (e.g., `... * 0.1 + 0.5`).\n",
    "    *   Calculate a `drift_score_no_drift` and print it. Confirm that this new score is below the `DRIFT_THRESHOLD`.\n",
    "\n",
    "3.  **Refine the Rollback SOP:**\n",
    "    *   Add a fifth \"Trigger Condition\" to the SOP. This new condition should be: \"More than 3 user-reported escalations related to incorrect or nonsensical answers are confirmed within a 4-hour window.\"\n",
    "\n",
    "4.  **Create a Drift Alert Function:**\n",
    "    *   Write a Python function `check_drift(baseline_embeds, current_embeds, threshold)` that takes baseline embeddings, current embeddings, and a threshold as input.\n",
    "    *   The function should perform the centroid distance calculation and return a tuple: `(drift_score, alert_status)`, where `alert_status` is either `\"OK\"` or `\"ALERT\"`.\n",
    "    - Call this function in your notebook to generate the drift report.\n",
    "\n",
    "## ✅ Checklist\n",
    "- [ ] Production logs are structured to capture embeddings, latency, and user feedback.\n",
    "- [ ] KPIs are automatically calculated and monitored for anomalies.\n",
    "- [ ] Embedding drift is quantitatively measured against a baseline.\n",
    "- [ ] A governance rule is in place to trigger alerts when drift exceeds a threshold.\n",
    "- [ ] A clear, automated report is generated for stakeholders.\n",
    "- [ ] Deployment strategies like canary and shadow testing are understood.\n",
    "- [ ] A formal SOP for model rollback is documented and socialized.\n",
    "\n",
    "## 📚 References\n",
    "- [MLOps Course: Setting up ML in Production](https://www.deeplearning.ai/courses/machine-learning-engineering-for-production-mlops/) (DeepLearning.AI)\n",
    "- *Designing Machine Learning Systems* by Chip Huyen (O'Reilly, 2022)\n",
    "- [Hugging Face Hub: Model Cards](https://huggingface.co/docs/hub/model-cards)\n",
    "- [Google Cloud: MLOps concepts](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
