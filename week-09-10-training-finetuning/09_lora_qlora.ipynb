{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7578dfb9",
   "metadata": {},
   "source": [
    "# ü™Ñ Week 09-10 ¬∑ Notebook 09 ¬∑ LoRA & QLoRA for Cost-Efficient Fine-tuning\n",
    "\n",
    "Apply low-rank adapters and 4-bit quantization to tailor models for remote plants running on modest GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86070509",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "- **Understand LoRA and QLoRA:** Grasp the mathematics behind LoRA and how QLoRA extends it with 4-bit quantization using the `bitsandbytes` library.\n",
    "- **Configure and Train:** Implement a QLoRA fine-tuning workflow, including setting up the `BitsAndBytesConfig` and `LoraConfig`.\n",
    "- **Evaluate Trade-offs:** Measure and compare the latency, memory footprint, and output quality of a model fine-tuned with QLoRA versus standard full-precision training.\n",
    "- **Implement Safety Gates:** Develop a \"safety gate\" function to programmatically verify that the quantized model still produces critical, non-negotiable information (like SOP steps) correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1788389e",
   "metadata": {},
   "source": [
    "## üß© Scenario\n",
    "A supplier wants an on-prem assistant running on a single NVIDIA T4. LoRA + QLoRA provides maintainable adapters without full fine-tuning cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ff4a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(29)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf65531",
   "metadata": {},
   "source": [
    "## üìÑ Synthetic Shift Reports\n",
    "Short instructions and responses representing maintenance troubleshooting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22859652",
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_reports_data = [\n",
    "    {\"instruction\": \"Summarize the issue with the CNC machine.\", \"response\": \"The CNC machine (ID: CNC-04) is showing a 'spindle drive fault' error. It requires immediate inspection by a certified technician.\"},\n",
    "    {\"instruction\": \"What is the standard procedure for a hydraulic leak?\", \"response\": \"1. Isolate the machine using the main power cutoff. 2. Place absorbent pads around the leak. 3. Notify the shift supervisor immediately. 4. Do not operate until cleared by maintenance.\"},\n",
    "    {\"instruction\": \"Generate a shift handoff note for the welding robot.\", \"response\": \"Welding robot WR-07 completed its cycle with no errors. Consumables (wire, gas) are at 75%. No scheduled maintenance is due.\"},\n",
    "    {\"instruction\": \"What action is needed for a high-temperature alert on Furnace-02?\", \"response\": \"A high-temperature alert requires reducing the setpoint by 10% and monitoring the cooling system. If the temperature does not decrease within 15 minutes, initiate a controlled shutdown.\"},\n",
    "    {\"instruction\": \"Translate the following safety warning to Hindi: 'Hard hat required in this area'.\", \"response\": \"‡§á‡§∏ ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞ ‡§Æ‡•á‡§Ç ‡§π‡§æ‡§∞‡•ç‡§° ‡§π‡•à‡§ü ‡§™‡§π‡§®‡§®‡§æ ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§π‡•à‡•§\"}\n",
    "]\n",
    "shift_reports = Dataset.from_list(shift_reports_data)\n",
    "shift_reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a89fdce",
   "metadata": {},
   "source": [
    "## üßæ Tokenizer & Preprocess\n",
    "We simulate instruction tuning with prompt ‚Üí response pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280906e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this demo, we'll use a smaller, more accessible model.\n",
    "# In a real-world scenario, you might use a larger model like Llama-2-7b.\n",
    "# NOTE: Using larger models requires significant GPU memory and access agreements.\n",
    "base_model_name = 'distilgpt2' # A smaller model for demonstration purposes\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "# Set a padding token if the model doesn't have one.\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83459e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(batch):\n",
    "    \"\"\"\n",
    "    Tokenizes the instruction and response, creating a single sequence for Causal LM training.\n",
    "    The format will be: `### Instruction: [instruction] ### Response: [response]`\n",
    "    \"\"\"\n",
    "    # Create the full prompt text\n",
    "    prompts = [f\"### Instruction: {instruction}\\n### Response: {response}\" for instruction, response in zip(batch['instruction'], batch['response'])]\n",
    "    \n",
    "    # Tokenize the prompts. We set the labels to be the same as the inputs for Causal LM.\n",
    "    tokenized_outputs = tokenizer(\n",
    "        prompts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "    )\n",
    "    \n",
    "    # For Causal LM, the labels are typically the input_ids shifted.\n",
    "    # A simpler approach for fine-tuning is to just use the input_ids as labels.\n",
    "    tokenized_outputs[\"labels\"] = tokenized_outputs[\"input_ids\"]\n",
    "    \n",
    "    return tokenized_outputs\n",
    "\n",
    "tokenized_shifts = shift_reports.map(tokenize_function, batched=True)\n",
    "print(f\"Columns in tokenized dataset: {tokenized_shifts.column_names}\")\n",
    "print(f\"\\nExample of tokenized input:\\n{tokenizer.decode(tokenized_shifts[0]['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7126da",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è LoRA Configuration\n",
    "Target query/key/value projections in attention layers for maximum leverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6def1a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"c_attn\"],  # Target the attention layers in GPT-2\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# --- Standard LoRA (Full Precision) ---\n",
    "# This model is not quantized. It's used as a baseline.\n",
    "# In a real scenario, you would train this model as well to compare against QLoRA.\n",
    "print(\"--- Setting up Full-Precision LoRA Model ---\")\n",
    "try:\n",
    "    full_precision_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "    lora_model = get_peft_model(full_precision_model, lora_config)\n",
    "    lora_model.print_trainable_parameters()\n",
    "except Exception as e:\n",
    "    print(f\"Could not load full-precision model. Error: {e}\")\n",
    "    lora_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efce3447",
   "metadata": {},
   "source": [
    "## üßÆ QLoRA Setup\n",
    "Load base model in 4-bit using `bitsandbytes` to reduce memory footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07b01bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure BitsAndBytes for 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Use the \"Normal Float 4\" quantization type\n",
    "    bnb_4bit_compute_dtype=torch.float16, # Use float16 for computations\n",
    "    bnb_4bit_use_double_quant=True, # Use a second quantization after the first one\n",
    ")\n",
    "\n",
    "# --- QLoRA (Quantized LoRA) ---\n",
    "# This loads the base model in 4-bit, significantly reducing the memory footprint.\n",
    "print(\"\\n--- Setting up QLoRA Model (4-bit) ---\")\n",
    "# Note: This requires a compatible GPU and the bitsandbytes library.\n",
    "try:\n",
    "    qlora_base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\" # Automatically map layers to available devices\n",
    "    )\n",
    "    qlora_model = get_peft_model(qlora_base_model, lora_config)\n",
    "    qlora_model.print_trainable_parameters()\n",
    "except Exception as e:\n",
    "    print(f\"Could not load QLoRA model. This usually means you don't have a compatible GPU or bitsandbytes is not installed correctly. Error: {e}\")\n",
    "    qlora_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9328bdf8",
   "metadata": {},
   "source": [
    "## üß™ Training Loop (QLoRA)\n",
    "Adjust epochs, dataset size, and evaluation hooks in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7bbcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qlora-shift-reports\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=1,\n",
    "    fp16=True, # Use mixed precision for training\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Only proceed if the QLoRA model was loaded successfully\n",
    "if qlora_model:\n",
    "    qlora_trainer = Trainer(\n",
    "        model=qlora_model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_shifts,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Starting QLoRA Training ---\")\n",
    "    # qlora_trainer.train() # Uncomment when running with a compatible GPU\n",
    "    print(\"--- (Skipping actual training for this demo) ---\")\n",
    "    print(\"--- QLoRA Training Complete ---\")\n",
    "else:\n",
    "    print(\"\\n--- Skipping QLoRA training as the model could not be loaded. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d02b5e",
   "metadata": {},
   "source": [
    "## üìâ Safety Gate Checks\n",
    "Ensure quantization preserved critical steps by verifying the model regenerates mandatory SOP steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d3b970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_safety_gate(model, tokenizer, prompt, expected_keywords):\n",
    "    \"\"\"\n",
    "    Checks if the model's output for a given prompt contains all expected keywords.\n",
    "    This is a simple but effective way to ensure critical information is not lost.\n",
    "    \"\"\"\n",
    "    if not model:\n",
    "        return \"Model not available.\", [\"N/A\"]\n",
    "        \n",
    "    # Format the prompt for inference\n",
    "    full_prompt = f\"### Instruction: {prompt}\\\\n### Response:\"\n",
    "    inputs = tokenizer(full_prompt, return_tensors='pt').to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(**inputs, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
    "        \n",
    "    # Decode the generated text and isolate the response part\n",
    "    full_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    try:\n",
    "        response_text = full_text.split(\"### Response:\")[1].strip()\n",
    "    except IndexError:\n",
    "        response_text = \"Error: Could not parse response.\"\n",
    "\n",
    "    # Check for missing keywords\n",
    "    missing = [kw for kw in expected_keywords if kw.lower() not in response_text.lower()]\n",
    "    \n",
    "    return response_text, missing\n",
    "\n",
    "# --- Run the Gate ---\n",
    "test_prompt = \"What is the standard procedure for a hydraulic leak?\"\n",
    "expected_keywords = ['isolate', 'absorbent pads', 'supervisor']\n",
    "\n",
    "print(\"--- Running Safety Gate on QLoRA Model ---\")\n",
    "qlora_response, qlora_missing = run_safety_gate(qlora_model, tokenizer, test_prompt, expected_keywords)\n",
    "print(f\"\\\\nQLoRA Generated Response:\\\\n{qlora_response}\")\n",
    "if not qlora_missing:\n",
    "    print(\"\\\\n‚úÖ QLoRA Safety Gate Passed: All keywords present.\")\n",
    "else:\n",
    "    print(f\"\\\\n‚ùå QLoRA Safety Gate Failed: Missing keywords: {qlora_missing}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*30 + \"\\\\n\")\n",
    "\n",
    "print(\"--- Running Safety Gate on Full-Precision LoRA Model ---\")\n",
    "lora_response, lora_missing = run_safety_gate(lora_model, tokenizer, test_prompt, expected_keywords)\n",
    "print(f\"\\\\nFull Precision LoRA Generated Response:\\\\n{lora_response}\")\n",
    "if not lora_missing:\n",
    "    print(\"\\\\n‚úÖ Full Precision LoRA Safety Gate Passed: All keywords present.\")\n",
    "else:\n",
    "    print(f\"\\\\n‚ùå Full Precision LoRA Safety Gate Failed: Missing keywords: {lora_missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94ad40a",
   "metadata": {},
   "source": [
    "## ‚è±Ô∏è Latency & Memory Snapshot\n",
    "Collect quick comparisons for stakeholder update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb6f903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_model_metrics():\n",
    "    \"\"\"\n",
    "    Provides a simulated comparison of memory and latency for different fine-tuning methods.\n",
    "    \"\"\"\n",
    "    # These are illustrative values. Actuals depend heavily on hardware and model size.\n",
    "    base_model_size_gb = 14.0 # For a 7B model in float16\n",
    "    \n",
    "    data = [\n",
    "        {\n",
    "            \"Method\": \"Full Fine-Tuning\",\n",
    "            \"Quantization\": \"None (FP16)\",\n",
    "            \"GPU Memory (Training)\": f\"~{base_model_size_gb * 2:.1f} GB\", # Model + Gradients\n",
    "            \"Trainable Params\": \"7B\",\n",
    "            \"Notes\": \"Requires significant hardware.\"\n",
    "        },\n",
    "        {\n",
    "            \"Method\": \"LoRA\",\n",
    "            \"Quantization\": \"None (FP16)\",\n",
    "            \"GPU Memory (Training)\": f\"~{base_model_size_gb:.1f} GB\", # No full-size gradients\n",
    "            \"Trainable Params\": \"~4M (0.06%)\",\n",
    "            \"Notes\": \"Faster training, much smaller checkpoint.\"\n",
    "        },\n",
    "        {\n",
    "            \"Method\": \"QLoRA\",\n",
    "            \"Quantization\": \"4-bit (NF4)\",\n",
    "            \"GPU Memory (Training)\": f\"~{base_model_size_gb / 2:.1f} GB\", # Base model is quantized\n",
    "            \"Trainable Params\": \"~4M (0.06%)\",\n",
    "            \"Notes\": \"Enables training on consumer GPUs.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "print(\"--- Fine-Tuning Method Comparison (Illustrative) ---\")\n",
    "comparison_df = compare_model_metrics()\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11578ca",
   "metadata": {},
   "source": [
    "### üõ°Ô∏è Governance Checklist\n",
    "- Validate licensing (LLaMA/EULA) with legal before deployment.\n",
    "- Document quantization settings in model registry.\n",
    "- Capture safety gate results and attach to release ticket.\n",
    "- Schedule drift review every 30 days."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c86ade",
   "metadata": {},
   "source": [
    "## üß™ Lab Assignment\n",
    "1. Run QLoRA training on your maintenance dataset (Zephyr or Mistral 7B).\n",
    "2. Profile latency on both T4 and A10 GPUs.\n",
    "3. Extend safety gate to include bilingual keywords and numeric tolerances.\n",
    "4. Produce a comparison memo for IT showcasing cost savings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c91af42",
   "metadata": {},
   "source": [
    "## ‚úÖ Checklist\n",
    "- [ ] LoRA targets selected and documented\n",
    "- [ ] QLoRA quantization tested\n",
    "- [ ] Safety gates passed\n",
    "- [ ] Metrics shared with stakeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e113aa",
   "metadata": {},
   "source": [
    "## üìö References\n",
    "- Dettmers et al., *QLoRA: Efficient Finetuning of Quantized LLMs* (2023)\n",
    "- HuggingFace Blog: *Low-Rank Adapters in Production*\n",
    "- Week 07 Decision Matrix Notebook"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
