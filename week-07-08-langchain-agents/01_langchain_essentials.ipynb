{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a19c1d2",
   "metadata": {},
   "source": [
    "# ðŸ§  Week 7-8 Â· Notebook 01 Â· LangChain Essentials: Building the Copilot's Foundation\n",
    "\n",
    "**Module:** LangChain, Agents, & Advanced RAG  \n",
    "**Project:** Manufacturing Copilot - Core Pipeline Construction\n",
    "\n",
    "---\n",
    "\n",
    "### Welcome to LangChain: From Components to Composition\n",
    "\n",
    "In the previous module, we built a RAG pipeline from scratch. We learned about the core components: document loaders, splitters, embedding models, vector stores, and LLMs. While building from scratch provides a deep understanding, it's not always the most efficient way to build production systems.\n",
    "\n",
    "This is where **LangChain** comes in.\n",
    "\n",
    "LangChain is a powerful framework for developing applications powered by language models. It provides a standard, extensible interface and a rich library of components that can be \"chained\" together to create sophisticated AI workflows.\n",
    "\n",
    "In this notebook, we will take our first steps with LangChain. We will learn its fundamental building blocks and use the **LangChain Expression Language (LCEL)** to compose them into our first real pipeline. This \"chain\" will form the foundation of the Manufacturing Copilot, a tool designed to provide instant, accurate answers to technicians on the factory floor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6863441f",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1.  **Understand LangChain's Core Abstractions:** Grasp the fundamental building blocks: `PromptTemplate`, `LLM`, and `OutputParser`.\n",
    "2.  **Master LangChain Expression Language (LCEL):** Use the `|` (pipe) operator to compose components into a declarative, runnable pipeline.\n",
    "3.  **Build a Simple RAG-style Chain:** Create a complete chain that takes a user's question and context to generate a helpful answer.\n",
    "4.  **Implement a Governance Wrapper:** Build a Python function to log key metadata about your chain's execution, a critical step for creating auditable, production-ready AI systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c45ce6",
   "metadata": {},
   "source": [
    "## âš™ï¸ Step 1: Environment Setup\n",
    "\n",
    "First, let's set up our environment. We'll need to install the core `langchain` library and the `langchain-openai` integration to use OpenAI's models.\n",
    "\n",
    "We will also set up our OpenAI API key. For security, it's best practice to store this in an environment variable rather than hardcoding it in the notebook.\n",
    "\n",
    "*(Note: If you are running this locally, you will need to have a `.env` file with your `OPENAI_API_KEY` or set it as an environment variable in your system.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14293fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!pip install -q langchain langchain-openai python-dotenv\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# It's recommended to set your OpenAI API key as an environment variable\n",
    "# to avoid exposing it in your code.\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "    print(\"API Key not found. Please set it as an environment variable or replace 'YOUR_API_KEY'.\")\n",
    "else:\n",
    "    print(\"OpenAI API Key loaded successfully from environment variables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3568da7",
   "metadata": {},
   "source": [
    "## ðŸ§± Step 2: The Three Core Components of a LangChain Chain\n",
    "\n",
    "At its core, a LangChain pipeline, or \"chain,\" is composed of three fundamental building blocks. Understanding these is the key to mastering LangChain.\n",
    "\n",
    "1.  **The `PromptTemplate`**: This is the blueprint for the instructions given to the LLM. It's a template that combines user input with fixed instructions to create a complete, precise prompt. This ensures that every query is sent to the LLM in a consistent and well-structured format.\n",
    "\n",
    "2.  **The Model (`ChatOpenAI`)**: This is the \"brain\" of the operation. It can be a large language model (LLM) or a chat model. We'll use `ChatOpenAI` to connect to OpenAI's powerful models like GPT-4o-mini. LangChain provides dozens of integrations, so you can easily swap this component for a different model from Hugging Face, Anthropic, or Google.\n",
    "\n",
    "3.  **The `OutputParser`**: This component takes the raw output from the LLM and transforms it into a more usable format. The simplest parser, `StrOutputParser`, takes the LLM's response and converts it into a clean string. More advanced parsers can convert the output into structured data like JSON or Python objects.\n",
    "\n",
    "We will compose these three components using the **LangChain Expression Language (LCEL)**, which uses the pipe `|` symbol. This creates a declarative, elegant, and powerful way to define our AI pipeline.\n",
    "\n",
    "`chain = prompt | model | output_parser`\n",
    "\n",
    "Let's build our first chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7f1d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 1. Prepare Sample Data ---\n",
    "# For this example, we'll create a dummy Standard Operating Procedure (SOP) document.\n",
    "# This simulates the knowledge base our copilot will use.\n",
    "SOP_DIR = Path('data/sop_manuals')\n",
    "SOP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SOP_PATH = SOP_DIR / 'punch_press_v7.txt'\n",
    "SOP_PATH.write_text(\"\"\"\n",
    "Standard Operating Procedure: Punch Press v7\n",
    "\n",
    "Safety First: Always wear safety glasses and gloves. Ensure the machine is fully locked out before performing any maintenance.\n",
    "\n",
    "Lubrication Schedule:\n",
    "- Daily: Check hydraulic fluid levels and top off if necessary.\n",
    "- Weekly: Lubricate all punch press bearings with Grade 2 lithium grease.\n",
    "- Monthly: Inspect and lubricate all other moving parts, including slides and guides.\n",
    "\"\"\", encoding='utf-8')\n",
    "\n",
    "# Load the content to be used as context\n",
    "sample_context = SOP_PATH.read_text(encoding='utf-8')\n",
    "\n",
    "# --- 2. Define the LangChain Components ---\n",
    "\n",
    "# The Prompt Template: It structures the input for the LLM.\n",
    "# We define placeholders for the dynamic parts of the prompt.\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful Manufacturing Copilot. Your role is to provide clear and concise answers based on Standard Operating Procedures.\"),\n",
    "    (\"human\", \"Please use the following context to answer the question.\\n\\nContext: {context}\\n\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "# The Model: We'll use OpenAI's gpt-4o-mini, a fast and capable model.\n",
    "# `temperature=0` makes the output deterministic and factual.\n",
    "model = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "# The Output Parser: This ensures the final output is a simple string.\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "\n",
    "# --- 3. Compose the Chain with LCEL ---\n",
    "# The pipe symbol `|` chains the components together.\n",
    "# The data flows from left to right: prompt -> model -> parser.\n",
    "chain = prompt_template | model | output_parser\n",
    "\n",
    "\n",
    "# --- 4. Invoke the Chain ---\n",
    "# We pass a dictionary with the required inputs to the `invoke` method.\n",
    "question = \"How often should I lubricate the punch press bearings?\"\n",
    "response = chain.invoke({\n",
    "    \"context\": sample_context,\n",
    "    \"question\": question\n",
    "})\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31dce35",
   "metadata": {},
   "source": [
    "## ðŸ”’ Step 3: Implementing a Governance Wrapper for Auditing\n",
    "\n",
    "In a real-world manufacturing environment, simply getting an answer isn't enough. For compliance, safety, and quality control (e.g., under ISO 9001 standards), it's crucial to maintain a record of the AI's operations. Who asked what? What information was used? How long did it take?\n",
    "\n",
    "To address this, we will build a \"governance wrapper.\" This is a Python function that will:\n",
    "1.  Execute our LangChain chain.\n",
    "2.  Record critical metadata about the run.\n",
    "3.  Log this metadata to a file, creating an immutable audit trail.\n",
    "\n",
    "This practice is essential for building responsible and trustworthy AI systems. The log can be used to debug issues, analyze performance, and demonstrate compliance to auditors.\n",
    "\n",
    "Our log will capture:\n",
    "-   A unique hash of the prompt, input, and output to ensure data integrity.\n",
    "-   The model used for the generation.\n",
    "-   The latency of the response.\n",
    "-   Contextual information like plant ID and supervisor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95940b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from hashlib import sha256\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def execute_and_log_chain(chain, inputs: dict):\n",
    "    \"\"\"\n",
    "    Executes a LangChain chain, measures latency, and logs governance metadata.\n",
    "\n",
    "    Args:\n",
    "        chain: The LangChain chain to execute.\n",
    "        inputs: The dictionary of inputs for the chain.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the response and the evidence dictionary.\n",
    "    \"\"\"\n",
    "    print(\"--- Executing Chain with Governance ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # --- 1. Execute the chain ---\n",
    "    response = chain.invoke(inputs)\n",
    "    \n",
    "    # --- 2. Calculate latency ---\n",
    "    latency_ms = (time.time() - start_time) * 1000\n",
    "    \n",
    "    # --- 3. Create the governance record (evidence) ---\n",
    "    # We hash the core components to create a verifiable signature for the run.\n",
    "    run_evidence = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"plant_id\": \"PUNE-01\",\n",
    "        \"shift\": \"morning\",\n",
    "        \"supervisor_id\": \"SUP-4872\",\n",
    "        \"prompt_hash\": sha256(str(chain.first.pretty_repr()).encode()).hexdigest()[:12],\n",
    "        \"input_context_hash\": sha256(inputs['context'].encode()).hexdigest()[:12],\n",
    "        \"output_hash\": sha256(response.encode()).hexdigest()[:12],\n",
    "        \"model\": chain.middle[0].model_name,\n",
    "        \"latency_ms\": round(latency_ms, 2)\n",
    "    }\n",
    "    \n",
    "    # --- 4. Log the evidence to a file ---\n",
    "    # We use a JSON Lines format (.jsonl) for easy, append-only logging.\n",
    "    log_dir = Path('governance_logs')\n",
    "    log_dir.mkdir(exist_ok=True)\n",
    "    log_file = log_dir / 'langchain_run_log.jsonl'\n",
    "    \n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(json.dumps(run_evidence) + '\\n')\n",
    "        \n",
    "    print(f\"Successfully logged governance evidence to {log_file.name}\")\n",
    "        \n",
    "    return response, run_evidence\n",
    "\n",
    "# --- Execute the chain using our new governance wrapper ---\n",
    "question = \"How often should I lubricate the punch press bearings?\"\n",
    "inputs = {\n",
    "    \"context\": sample_context,\n",
    "    \"question\": question\n",
    "}\n",
    "final_response, evidence = execute_and_log_chain(chain, inputs)\n",
    "\n",
    "print(\"\\n--- Chain Response ---\")\n",
    "print(final_response)\n",
    "print(\"\\n--- Governance Log ---\")\n",
    "print(json.dumps(evidence, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2de088",
   "metadata": {},
   "source": [
    "## âœ… Congratulations and Next Steps!\n",
    "\n",
    "Congratulations! You have successfully built and executed your first LangChain pipeline.\n",
    "\n",
    "You've learned about the three core componentsâ€”**Prompts, Models, and Output Parsers**â€”and how to compose them into a powerful, declarative chain using the **LangChain Expression Language (LCEL)**. You also took a critical first step towards production-readiness by implementing a **governance wrapper** to create an audit trail of your chain's activity.\n",
    "\n",
    "### Key Takeaways:\n",
    "-   **Composition is Key:** LCEL's pipe `|` syntax is the heart of modern LangChain development. It allows you to build complex logic in a simple, readable way.\n",
    "-   **Components are Swappable:** The modular nature of LangChain means you can easily swap out any component. You could change the `model` to one from Google or Hugging Face without altering the rest of your chain.\n",
    "-   **Governance is Not an Afterthought:** Building logging and auditing into your process from the beginning is essential for creating responsible and reliable AI applications.\n",
    "\n",
    "In the next notebook, **`02_message_structures.ipynb`**, we will dive deeper into the data structures that flow through these chains, exploring the different types of `Messages` that are used to manage conversations and instruct chat models. Let's continue building!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
