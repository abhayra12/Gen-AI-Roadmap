{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d32c1b4a",
   "metadata": {},
   "source": [
    "# ðŸ§© Week 7-8 Â· Notebook 03 Â· Dynamic Prompts with Templates & Chains\n",
    "\n",
    "**Module:** LangChain, Agents, & Advanced RAG  \n",
    "**Project:** Manufacturing Copilot - Building Reusable and Auditable Prompts\n",
    "\n",
    "---\n",
    "\n",
    "### From Hardcoded to Dynamic: The Power of Prompt Templates\n",
    "\n",
    "In our previous notebooks, we created prompts by manually formatting strings. This works for simple examples, but it's not a scalable or maintainable approach for production applications. What happens when you need to change the instructions? Or use the same prompt structure with different user inputs?\n",
    "\n",
    "This is where **Prompt Templates** come in.\n",
    "\n",
    "A `PromptTemplate` is a reusable and parameterizable object that defines the structure of your prompt. It separates the fixed instructions from the dynamic, user-provided data. This allows you to:\n",
    "\n",
    "-   **Standardize:** Ensure every prompt sent to the LLM follows a consistent, optimized format.\n",
    "-   **Reuse:** Use the same prompt structure across different parts of your application.\n",
    "-   **Maintain:** Update the core prompt logic in one central place without changing your application code.\n",
    "\n",
    "In this notebook, we will learn how to create dynamic, production-ready prompts using `ChatPromptTemplate` and compose them into a complete chain. We'll also implement a crucial best practice: an automated validation harness to ensure our prompts and the LLM's responses meet our quality and safety standards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b103fb80",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to build robust, maintainable, and auditable prompt engineering workflows. You will learn to:\n",
    "\n",
    "1.  **Build Parameterized `ChatPromptTemplate`s:** Create dynamic prompts that can be populated with variables at runtime.\n",
    "2.  **Inject Configuration Data:** Design prompts that can be customized with external configuration, such as plant-specific safety disclaimers or compliance standards.\n",
    "3.  **Compose a Complete LCEL Chain:** Combine a `ChatPromptTemplate`, an `LLM`, and an `OutputParser` into a seamless, executable pipeline.\n",
    "4.  **Create an Automated Validation Harness:** Write a function to programmatically check the LLM's output for critical requirements, such as the presence of a safety warning or a citation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e8c4e6",
   "metadata": {},
   "source": [
    "## âš™ï¸ Step 1: Environment Setup\n",
    "\n",
    "Let's start by installing the necessary libraries and setting up our OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4d599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!pip install -q langchain langchain-openai python-dotenv\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up OpenAI API key\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "    print(\"API Key not found. Please set it as an environment variable or replace 'YOUR_API_KEY'.\")\n",
    "else:\n",
    "    print(\"OpenAI API Key loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda205c1",
   "metadata": {},
   "source": [
    "## ðŸ­ Step 2: Creating a Dynamic, Configurable Prompt\n",
    "\n",
    "Imagine our Manufacturing Copilot needs to be deployed across multiple plants. Each plant might have slightly different compliance rules or safety warnings. Hardcoding these into the prompt is not feasible.\n",
    "\n",
    "We need a template that can be dynamically configured.\n",
    "\n",
    "Let's create a `ChatPromptTemplate` that takes not only the user's question and the retrieved context, but also plant-specific configuration data. This makes our prompt a reusable asset that can be adapted to different environments without changing its core logic.\n",
    "\n",
    "### The Scenario\n",
    "Corporate Safety has issued a new, mandatory safety disclaimer for all maintenance-related AI assistance. We need to ensure this new disclaimer is included in all responses, and we need to do it by changing a configuration file, not by editing every prompt in our codebase.\n",
    "\n",
    "We will simulate this by creating:\n",
    "1.  A `plant_config` dictionary containing the plant-specific data.\n",
    "2.  A `ChatPromptTemplate` with placeholders for this configuration.\n",
    "3.  A complete chain that uses this template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73206926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# --- 1. Define Plant-Specific Configuration ---\n",
    "# In a real application, this would be loaded from a config file (e.g., YAML, JSON) or a database.\n",
    "plant_config = {\n",
    "    \"plant_id\": \"PUNE-01\",\n",
    "    \"language\": \"en\",\n",
    "    \"compliance_grade\": \"ISO 9001\",\n",
    "    \"safety_disclaimer\": \"CRITICAL SAFETY WARNING: Always follow official Lockout/Tagout (LOTO) procedures as per SOP-900 before beginning any maintenance. Ensure a zero-energy state is confirmed.\"\n",
    "}\n",
    "\n",
    "# --- 2. Create a Parameterized Prompt Template ---\n",
    "# This template uses f-string style placeholders `{variable_name}` for all dynamic inputs.\n",
    "template_string = \"\"\"\n",
    "You are a Manufacturing Copilot for Plant {plant_id}.\n",
    "Your response must adhere to {compliance_grade} standards and be in {language}.\n",
    "\n",
    "**Mandatory Safety Protocol:**\n",
    "{safety_disclaimer}\n",
    "\n",
    "---\n",
    "**Retrieved Context from SOP:**\n",
    "{sop_snippet}\n",
    "\n",
    "---\n",
    "**Technician's Question:**\n",
    "{question}\n",
    "\n",
    "---\n",
    "Provide a concise, factual answer based ONLY on the provided SOP context.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "\n",
    "\n",
    "# --- 3. Build the LCEL Chain ---\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "# --- 4. Invoke the Chain with All Required Inputs ---\n",
    "# The `invoke` method requires a dictionary containing values for ALL variables in the template.\n",
    "sop_context = \"SOP-112, Section 4.1 states: 'The primary hydraulic pump requires a full filter replacement every 500 operating hours or 3 months, whichever comes first.'\"\n",
    "question = \"How often do I need to replace the hydraulic pump filter?\"\n",
    "\n",
    "answer = chain.invoke({\n",
    "    # Configuration variables\n",
    "    \"plant_id\": plant_config[\"plant_id\"],\n",
    "    \"language\": plant_config[\"language\"],\n",
    "    \"compliance_grade\": plant_config[\"compliance_grade\"],\n",
    "    \"safety_disclaimer\": plant_config[\"safety_disclaimer\"],\n",
    "    \n",
    "    # Runtime variables\n",
    "    \"sop_snippet\": sop_context,\n",
    "    \"question\": question\n",
    "})\n",
    "\n",
    "print(\"--- Generated Response ---\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8d99bf",
   "metadata": {},
   "source": [
    "## ðŸ§ª Step 3: Building a Validation Harness\n",
    "\n",
    "Generating a response is only half the battle. In a production environment, we need to be sure the response meets our quality and safety standards. Did the LLM follow our instructions? Did it include the mandatory safety warning?\n",
    "\n",
    "To automate this, we can create a **validation harness**. This is a function that programmatically checks the LLM's output against a set of rules. This is a form of \"testing\" for our prompts and models.\n",
    "\n",
    "Our harness will check for three things:\n",
    "1.  **Disclaimer Presence:** Did the response include the exact safety disclaimer we provided?\n",
    "2.  **SOP Citation:** Did the response reference the SOP document (e.g., \"SOP-112\")?\n",
    "3.  **Conciseness:** Is the response reasonably short and to the point?\n",
    "\n",
    "This automated check is crucial for CI/CD (Continuous Integration/Continuous Deployment) workflows. Before deploying a new prompt or model, you can run it through the validation harness to prevent regressions and ensure compliance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7767c499",
   "metadata": {},
   "source": [
    "import re\n",
    "\n",
    "def validate_response(response_text: str, config: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Validates the LLM's response against a set of predefined rules.\n",
    "\n",
    "    Args:\n",
    "        response_text: The text generated by the LLM.\n",
    "        config: The plant configuration dictionary containing the rules.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary summarizing the validation results.\n",
    "    \"\"\"\n",
    "    disclaimer = config.get(\"safety_disclaimer\", \"\")\n",
    "    \n",
    "    validation_results = {\n",
    "        \"has_disclaimer\": disclaimer in response_text,\n",
    "        \"has_sop_citation\": bool(re.search(r\"SOP-\\d+\", response_text)),\n",
    "        \"is_concise\": len(response_text.split()) < 75  # Example constraint: response should be under 75 words.\n",
    "    }\n",
    "    return validation_results\n",
    "\n",
    "# --- Run Validation on Our Previous Answer ---\n",
    "validation = validate_response(answer, plant_config)\n",
    "\n",
    "print(\"--- Validation Harness Results ---\")\n",
    "print(f\"Response Validated: '{answer[:50]}...'\")\n",
    "print(json.dumps(validation, indent=2))\n",
    "\n",
    "# --- Automated Assertions for CI/CD ---\n",
    "# In a real test suite, you would use assertions to automatically pass or fail a build.\n",
    "try:\n",
    "    assert validation['has_disclaimer'], 'CRITICAL: Safety disclaimer is missing from the response!'\n",
    "    assert validation['has_sop_citation'], 'CRITICAL: SOP citation is missing from the response!'\n",
    "    print(\"\\nâœ… All critical validation checks passed.\")\n",
    "except AssertionError as e:\n",
    "    print(f\"\\nâŒ VALIDATION FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0675a9f7",
   "metadata": {},
   "source": [
    "## âœ… Congratulations and Next Steps!\n",
    "\n",
    "You have now learned how to create dynamic, reusable, and auditable AI workflows using LangChain's `ChatPromptTemplate`.\n",
    "\n",
    "You've seen how to separate configuration from logic, allowing you to adapt your prompts to different environments without changing your code. Most importantly, you've implemented a validation harnessâ€”a critical tool for ensuring the quality, safety, and reliability of your AI's responses in a production setting.\n",
    "\n",
    "### Key Takeaways:\n",
    "-   **Templates are for Maintainability:** `ChatPromptTemplate` is the key to building scalable applications. It allows you to manage your prompt logic centrally.\n",
    "-   **Configuration Drives Flexibility:** By injecting configuration into your templates, you can create a single, reusable prompt that serves multiple purposes.\n",
    "-   **Validation Ensures Reliability:** Never trust, always verify. An automated validation harness is your first line of defense against prompt regressions and model misbehavior.\n",
    "\n",
    "In the next notebook, **`04_runnable_sequences.ipynb`**, we will dive deeper into the LangChain Expression Language (LCEL) and explore more advanced ways to construct complex, multi-step chains. Let's continue our journey to mastering LangChain!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
