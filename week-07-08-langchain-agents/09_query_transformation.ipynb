{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54f4ea0b",
   "metadata": {},
   "source": [
    "# ðŸ§  Week 07-08 Â· Notebook 09 Â· Query Transformation & Decomposition\n",
    "\n",
    "Transform technician questions into decomposed, enriched queries that boost RAG accuracy and auditability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d0a118",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Learning Objectives\n",
    "- Decompose complex maintenance requests into sub-questions.\n",
    "- Apply rewrite techniques (HyDE, paraphrasing) to improve retrieval.\n",
    "- Track transformation lineage for compliance.\n",
    "- Evaluate transformation impact on response quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15964e0d",
   "metadata": {},
   "source": [
    "## ðŸ§© Scenario\n",
    "Technicians often bundle multiple requests (diagnosis, spare parts, safety check). Decompose queries to ensure each pipeline step has precise context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8017e7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# --- 1. Define the desired output structure ---\n",
    "# Use Pydantic to define a structured output for the sub-questions.\n",
    "class DecomposedQuestions(BaseModel):\n",
    "    \"\"\"A list of simple, self-contained questions decomposed from a complex user query.\"\"\"\n",
    "    questions: List[str] = Field(description=\"A list of sub-questions.\")\n",
    "\n",
    "# --- 2. Create a parser ---\n",
    "# The parser will automatically generate formatting instructions for the LLM.\n",
    "parser = PydanticOutputParser(pydantic_object=DecomposedQuestions)\n",
    "\n",
    "# --- 3. Create the prompt ---\n",
    "# The prompt template now includes the format instructions from the parser.\n",
    "decomposition_prompt = PromptTemplate(\n",
    "    template=\"\"\"Decompose the following user query into a list of simple, self-contained sub-questions.\n",
    "Focus on breaking down the query into individual tasks like diagnosis, inventory checks, or safety procedures.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "User Query:\n",
    "{query}\n",
    "\"\"\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# --- 4. Build and run the chain ---\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "decompose_chain = decomposition_prompt | llm | parser\n",
    "\n",
    "query = 'Spindle 4 is vibrating again after the bearing swap. I need to know the root cause, check the spare parts inventory for replacement bearings, and find the correct safety procedure for this repair.'\n",
    "sub_questions = decompose_chain.invoke({\"query\": query})\n",
    "\n",
    "print(\"--- Original Query ---\")\n",
    "print(query)\n",
    "print(\"\\n--- Decomposed Sub-Questions ---\")\n",
    "for q in sub_questions.questions:\n",
    "    print(f\"- {q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f3bb48",
   "metadata": {},
   "source": [
    "### ðŸ§ª Hypothetical Document Embeddings (HyDE)\n",
    "Generate synthetic answers to improve retrieval coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81742f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# --- HyDE: Hypothetical Document Embeddings ---\n",
    "# This technique generates a hypothetical answer to the user's query first,\n",
    "# then uses the embedding of that *answer* to find similar real documents.\n",
    "# This can improve retrieval by matching on concepts rather than just keywords.\n",
    "\n",
    "# The prompt to generate the hypothetical document\n",
    "hyde_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Generate a concise, factual, hypothetical answer to the following question.\n",
    "This answer will be used to find similar real documents.\n",
    "Cite potential SOP IDs or document numbers.\n",
    "\n",
    "Question: {query}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# The chain to generate the synthetic document\n",
    "hyde_chain = hyde_prompt | llm | StrOutputParser()\n",
    "\n",
    "# The original query from before\n",
    "query = 'Spindle 4 is vibrating again after the bearing swap. What is the root cause?'\n",
    "\n",
    "# Generate the hypothetical document\n",
    "synthetic_doc = hyde_chain.invoke({\"query\": query})\n",
    "\n",
    "print(\"--- Original Query ---\")\n",
    "print(query)\n",
    "print(\"\\n--- Generated Hypothetical Document (for embedding) ---\")\n",
    "print(synthetic_doc)\n",
    "\n",
    "# In a full RAG pipeline, you would then:\n",
    "# 1. Embed `synthetic_doc`.\n",
    "# 2. Use that embedding to perform a similarity search on your vector store.\n",
    "# 3. Pass the retrieved *real* documents to the final answer-generation LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab96b923",
   "metadata": {},
   "source": [
    "## ðŸ§¾ Transformation Log\n",
    "| Stage | Output | Hash |\n",
    "- Record in governance store for traceability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf855f73",
   "metadata": {},
   "source": [
    "## ðŸ§ª Lab Assignment\n",
    "1. Build a `HypotheticalDocumentEmbedder` pipeline that indexes synthetic docs alongside real data.\n",
    "2. Run ablation study: baseline vs. decomposition+HyDE on incident QA dataset.\n",
    "3. Present findings to reliability engineering, highlighting recall improvements.\n",
    "4. Update change log with new transformation policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54d3093",
   "metadata": {},
   "source": [
    "## âœ… Checklist\n",
    "- [ ] Decomposition chain implemented\n",
    "- [ ] HyDE augmentation validated\n",
    "- [ ] Transformation log captured\n",
    "- [ ] Lab artefacts submitted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5684d5",
   "metadata": {},
   "source": [
    "## ðŸ“š References\n",
    "- LangChain Query Transformation Guide\n",
    "- HyDE Paper (Li et al., 2023)\n",
    "- Week 09 Evaluation Metrics Notebook"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
