{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "906c9224",
   "metadata": {},
   "source": [
    "# üóÉÔ∏è Week 07-08 ¬∑ Notebook 06: Embeddings & Vector Stores\n",
    "\n",
    "**Objective:** Understand the core concepts of embeddings and vector stores, and learn how to use them to build the foundation of a retrieval system.\n",
    "\n",
    "In the previous notebook, we built a pipeline to load, transform, and enrich documents. But how does a RAG system find the *right* document to answer a question? The answer lies in two key technologies: **Embeddings** and **Vector Stores**.\n",
    "\n",
    "1.  **Embeddings:** These are numerical representations (vectors) of text. An embedding model converts a piece of text (like a question or a document chunk) into a high-dimensional vector. The key idea is that semantically similar pieces of text will have vectors that are close to each other in space.\n",
    "2.  **Vector Stores (or Vector Databases):** These are specialized databases designed to store and efficiently search through millions of vectors. When a user asks a question, we first create an embedding of the question and then use the vector store to find the document vectors that are \"closest\" to the question vector.\n",
    "\n",
    "In this notebook, we will take our sanitized documents, convert them into embeddings, and store them in a vector database. We will explore two popular in-memory vector stores: **FAISS** and **Chroma**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb694bb2",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1.  **Understand Text Splitting:** Learn why and how to split large documents into smaller, more effective chunks for retrieval.\n",
    "2.  **Generate Embeddings:** Use an embedding model from Hugging Face to convert text chunks into numerical vectors.\n",
    "3.  **Index Documents in a Vector Store:** Store the document embeddings in two different in-memory vector stores: `FAISS` and `Chroma`.\n",
    "4.  **Perform Similarity Search:** Use the vector stores to find the most relevant document chunks for a given user query.\n",
    "5.  **Compare Vector Stores:** Understand the basic differences in setup and usage between FAISS and Chroma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2cb016",
   "metadata": {},
   "source": [
    "## üß© Scenario: Choosing the Right Vector Store for Manufacturing RAG\n",
    "\n",
    "You are a data engineer supporting two manufacturing plants‚Äîone in Pune, India, and one in Monterrey, Mexico. Both plants want to deploy a RAG-based maintenance assistant, but they have different infrastructure preferences:\n",
    "\n",
    "- **Pune** prefers an on-premises solution for data sovereignty and cost control (e.g., FAISS).\n",
    "- **Monterrey** prefers a managed cloud solution for scalability and ease of maintenance (e.g., pgvector on CloudSQL).\n",
    "\n",
    "Leadership has asked you to run a fair, data-driven benchmark comparing the available vector store options. Your goal is to:\n",
    "- Ingest the same set of maintenance documents into each store.\n",
    "- Measure ingestion time, query latency, and operational complexity.\n",
    "- Log all results for future audits.\n",
    "- Make a recommendation based on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ca5591",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, let's install the necessary libraries. We will need:\n",
    "-   `langchain` and `langchain-community` for the core framework.\n",
    "-   `sentence-transformers` to pull down the embedding model from Hugging Face.\n",
    "-   `faiss-cpu` for the FAISS vector store. FAISS is a library from Facebook AI for efficient similarity search. `cpu` is specified for compatibility; a `gpu` version also exists.\n",
    "-   `chromadb` for the Chroma vector store.\n",
    "\n",
    "> ‚ö†Ô∏è **Kernel Restart**: After running the installation cell below, you may need to restart the kernel for the changes to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6e9611",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain langchain-community sentence-transformers faiss-cpu chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3875a595",
   "metadata": {},
   "source": [
    "### Preparing the Documents\n",
    "\n",
    "For this notebook, we'll create a sample list of `Document` objects, simulating the output from our previous ingestion pipeline. Each document contains `page_content` and `metadata`. Notice that some documents have long content that will need to be split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d54fbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from datetime import datetime\n",
    "\n",
    "# Simulate the documents from the previous notebook\n",
    "final_docs = [\n",
    "    Document(\n",
    "        page_content=\"Standard Operating Procedure: Hydraulic Press H-45. 1. Ensure all safety guards are in place before operation. 2. Perform daily maintenance checks as per the log. 3. The hydraulic fluid must be checked weekly. Any leaks should be reported immediately. The operating pressure should not exceed 5000 PSI.\",\n",
    "        metadata={\n",
    "            \"source\": \"sop_manuals/press_safety.pdf\",\n",
    "            \"plant\": \"PNQ\",\n",
    "            \"doc_type\": \"SOP\",\n",
    "            \"ingested_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        },\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"ticket_id: TICKET-002\\ntimestamp_utc: 2025-04-17T10:00:00Z\\nissue_description: Pressure sensor fault\\ntechnician_notes: Recalibrated sensor. Work completed by Technician [REDACTED].\",\n",
    "        metadata={\n",
    "            \"source\": \"TICKET-002\",\n",
    "            \"plant\": \"PNQ\",\n",
    "            \"doc_type\": \"MaintenanceLog\",\n",
    "            \"ingested_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        },\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"ticket_id: TICKET-003\\ntimestamp_utc: 2025-10-17T12:30:00Z\\nissue_description: Emergency stop button stuck\\ntechnician_notes: Replaced the button assembly. Work completed by Technician [REDACTED].\",\n",
    "        metadata={\n",
    "            \"source\": \"TICKET-003\",\n",
    "            \"plant\": \"PNQ\",\n",
    "            \"doc_type\": \"MaintenanceLog\",\n",
    "            \"ingested_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(final_docs)} sample documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684c3766",
   "metadata": {},
   "source": [
    "## 2. Splitting Documents into Chunks\n",
    "\n",
    "LLMs have a limited context window, and retrieval is more effective when it points to a specific, relevant piece of information rather than a long, noisy document. Therefore, a critical step in the ingestion process is **splitting** large documents into smaller chunks.\n",
    "\n",
    "LangChain provides various `TextSplitter` classes. A popular choice is the `RecursiveCharacterTextSplitter`, which tries to split text based on a prioritized list of characters (e.g., `\\n\\n`, `\\n`, ` `) to keep related pieces of text together.\n",
    "\n",
    "Key parameters for a text splitter:\n",
    "-   `chunk_size`: The maximum size of a chunk (in characters).\n",
    "-   `chunk_overlap`: The number of characters to overlap between adjacent chunks. This helps maintain context across the split.\n",
    "\n",
    "We will split our documents into chunks of 200 characters with an overlap of 20 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fb2aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200, \n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Split the documents into chunks\n",
    "doc_chunks = text_splitter.split_documents(final_docs)\n",
    "\n",
    "print(f\"Original number of documents: {len(final_docs)}\")\n",
    "print(f\"Number of document chunks after splitting: {len(doc_chunks)}\")\n",
    "\n",
    "print(\"\\n--- Example of a Document Chunk ---\")\n",
    "# The long SOP document was split into multiple chunks\n",
    "for i, chunk in enumerate(doc_chunks):\n",
    "    if chunk.metadata['doc_type'] == 'SOP':\n",
    "        print(f\"Chunk {i+1} (from SOP):\")\n",
    "        print(f\"Content: \\\"{chunk.page_content}\\\"\")\n",
    "        print(f\"Metadata: {chunk.metadata}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca52d584",
   "metadata": {},
   "source": [
    "## 3. Generating Embeddings\n",
    "\n",
    "Now that we have our text chunks, we need to convert them into vectors. We will use an `Embedding` model for this. LangChain integrates with many embedding providers (like OpenAI, Cohere, and Google), but for this example, we'll use a popular open-source model from Hugging Face.\n",
    "\n",
    "The `HuggingFaceEmbeddings` class makes it easy to use models from the `sentence-transformers` library. We will use `all-MiniLM-L6-v2`, which is a small but effective model, perfect for getting started. When you initialize this class, it will download the model from Hugging Face Hub (if you don't have it cached)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c12cb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize the embedding model\n",
    "# This will download the model from Hugging Face Hub on first run\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Let's test it on a single piece of text\n",
    "sample_text = \"What is the operating pressure for the hydraulic press?\"\n",
    "sample_embedding = embedding_model.embed_query(sample_text)\n",
    "\n",
    "print(f\"Successfully created an embedding for the sample text.\")\n",
    "print(f\"Embedding dimension: {len(sample_embedding)}\")\n",
    "print(f\"First 5 values of the vector: {sample_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa252a20",
   "metadata": {},
   "source": [
    "## 4. Indexing and Searching with Vector Stores\n",
    "\n",
    "With our document chunks and embedding model ready, we can now populate our vector stores. The general process is:\n",
    "\n",
    "1.  **Instantiate the Vector Store:** Create an instance of the vector store class (e.g., `FAISS` or `Chroma`).\n",
    "2.  **Provide Documents and Embeddings:** Pass the list of document chunks and the embedding model to the vector store's `from_documents` class method. The vector store will handle the rest:\n",
    "    -   It iterates through each document chunk.\n",
    "    -   It uses the provided `embedding_model` to convert the `page_content` into a vector.\n",
    "    -   It stores the vector along with the document's `page_content` and `metadata`.\n",
    "3.  **Perform a Search:** Use the `similarity_search` method to find relevant documents for a new query.\n",
    "\n",
    "### Using FAISS\n",
    "FAISS (Facebook AI Similarity Search) is a highly optimized library for vector search. It's very fast and runs entirely in memory, making it a great choice for rapid prototyping and smaller-scale applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b6c62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# 1. Create the FAISS vector store from our document chunks\n",
    "print(\"Creating FAISS vector store...\")\n",
    "faiss_vector_store = FAISS.from_documents(doc_chunks, embedding_model)\n",
    "print(\"FAISS vector store created successfully.\")\n",
    "\n",
    "# 2. Define a query\n",
    "query = \"What is the maximum operating pressure for the hydraulic press?\"\n",
    "\n",
    "# 3. Perform a similarity search\n",
    "print(f\"\\nSearching for documents similar to: '{query}'\")\n",
    "retrieved_docs_faiss = faiss_vector_store.similarity_search(query, k=2) # k is the number of documents to retrieve\n",
    "\n",
    "# 4. Display the results\n",
    "print(\"\\n--- Top 2 Retrieved Documents (FAISS) ---\")\n",
    "for i, doc in enumerate(retrieved_docs_faiss):\n",
    "    print(f\"Result {i+1}:\")\n",
    "    print(f\"  Content: \\\"{doc.page_content}\\\"\")\n",
    "    print(f\"  Metadata: {doc.metadata}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb02cae",
   "metadata": {},
   "source": [
    "### Using Chroma\n",
    "Chroma is another popular open-source vector store. It's also easy to use in-memory, but it also offers the ability to persist the database to disk, which is useful for saving your index between sessions. It also has more advanced features like metadata filtering, which we will explore in a later notebook.\n",
    "\n",
    "The process is nearly identical to FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080e5ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Setup: Generating Example Documents and Embeddings\n",
    "\n",
    "To ensure a fair benchmark, we'll generate a synthetic set of maintenance documents. We'll use a lightweight Hugging Face embedding model to convert these documents into vectors. This setup simulates a real-world RAG deployment, where each document chunk is embedded and stored for fast retrieval.\n",
    "\n",
    "We'll use the same set of documents and embeddings for all vector stores to ensure a level playing field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0e45c3",
   "metadata": {},
   "source": [
    "## 2. Benchmarking Vector Stores\n",
    "\n",
    "We'll now benchmark two popular vector stores: **Chroma** and **FAISS**. For each store, we'll measure:\n",
    "- Ingestion time (how long it takes to index all documents)\n",
    "- Query latency (how fast it can retrieve relevant documents)\n",
    "\n",
    "We'll use the same set of embeddings for both stores. All results will be logged to MLflow for future audits and comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06e845e",
   "metadata": {},
   "source": [
    "### Benchmarking Chroma\n",
    "\n",
    "Chroma is a fast, open-source vector database that can run in-memory or persist to disk. It's easy to use and great for prototyping. We'll measure ingestion and query latency, and log the results to MLflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9655f684",
   "metadata": {},
   "source": [
    "### Generate Synthetic Maintenance Documents\n",
    "\n",
    "We'll create a set of 200 synthetic documents‚Äîhalf are SOPs, half are maintenance logs. Each document will have a unique `doc_id` in its metadata. This simulates a realistic RAG knowledge base for a manufacturing plant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2891b7b2",
   "metadata": {},
   "source": [
    "### Generate Embeddings for Each Document\n",
    "\n",
    "We'll use the `all-MiniLM-L6-v2` model from Hugging Face to convert each document into a vector. This model is fast and provides good semantic similarity for English text. The same embeddings will be used for all vector stores to ensure a fair comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6c21d5",
   "metadata": {},
   "source": [
    "### Install Required Libraries\n",
    "\n",
    "We'll need the following libraries:\n",
    "- `langchain` and `langchain-community` for vector store integrations\n",
    "- `sentence-transformers` for the embedding model\n",
    "- `mlflow` for experiment tracking\n",
    "- `faiss-cpu` for FAISS (if not already installed)\n",
    "\n",
    "> ‚ö†Ô∏è If running on Colab or a fresh environment, uncomment and run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250d2fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -qU langchain langchain-community sentence-transformers mlflow faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004eb0e1",
   "metadata": {},
   "source": [
    "> ‚ö†Ô∏è pgvector benchmarking requires a running Postgres instance with the extension enabled. Refer to `infrastructure/pgvector_setup.sql`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcd060a",
   "metadata": {},
   "source": [
    "## üìä Evaluation Matrix\n",
    "| Criterion | Chroma | FAISS | pgvector |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e29fb5",
   "metadata": {},
   "source": [
    "## üß™ Lab Assignment\n",
    "1. Run pgvector benchmark using CloudSQL dev instance and capture metrics.\n",
    "2. Evaluate recall by scoring against labeled maintenance Q&A set.\n",
    "3. Log benchmarks to MLflow (`mlflow.log_metrics` & `mlflow.log_dict`).\n",
    "4. Draft recommendation memo for CIO summarizing trade-offs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6409ecc7",
   "metadata": {},
   "source": [
    "## ‚úÖ Checklist\n",
    "- [ ] Benchmarks executed for all stores\n",
    "- [ ] Metrics logged\n",
    "- [ ] Recommendation memo drafted\n",
    "- [ ] Governance evidence archived"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef273a8",
   "metadata": {},
   "source": [
    "## üìö References\n",
    "- LangChain Vectorstore Docs\n",
    "- pgvector Extension Guide\n",
    "- Week 05 Data Storage Policy"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
