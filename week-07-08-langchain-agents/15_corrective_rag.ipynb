{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c938254a",
   "metadata": {},
   "source": [
    "# ‚úÖ Week 07-08 ¬∑ Notebook 15 ¬∑ Corrective RAG (CRAG) & Self-Grading\n",
    "\n",
    "Implement self-correcting RAG loops that grade answers, retry unsafe outputs, and escalate to SMEs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3ecfbc",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "- Build grader chains that score answers for accuracy, safety, and citation quality.\n",
    "- Implement corrective loop (generate ‚Üí grade ‚Üí revise or escalate).\n",
    "- Track grading outcomes and escalations for governance.\n",
    "- Integrate SME override pathways."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b572e8",
   "metadata": {},
   "source": [
    "## üß© Scenario\n",
    "Regulators require auto-escalation when confidence < 0.75 or missing safety disclaimers. CRAG ensures only high-quality answers reach technicians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097a8efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# --- 1. Define Pydantic Models for Structured Grading ---\n",
    "class Grade(BaseModel):\n",
    "    \"\"\"A structured grade for an LLM-generated answer.\"\"\"\n",
    "    accuracy_score: float = Field(description=\"Factual accuracy score (0.0 to 1.0) based on the context.\")\n",
    "    safety_compliance: float = Field(description=\"Safety compliance score (0.0 to 1.0), checking for disclaimers.\")\n",
    "    citation_quality: float = Field(description=\"Citation quality score (0.0 to 1.0), checking for SOP references.\")\n",
    "\n",
    "# --- 2. Setup Chains for Generation and Grading ---\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "# The chain that generates the initial answer\n",
    "generator_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer with SOP citations and safety disclaimers.\"\n",
    ")\n",
    "generator_chain = generator_prompt | llm\n",
    "\n",
    "# The chain that grades the answer, with instructions for JSON output\n",
    "grader_parser = JsonOutputParser(pydantic_object=Grade)\n",
    "grader_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a strict auditor. Grade the following answer based on the provided context and question.\n",
    "    Pay close attention to factual accuracy, safety warnings, and SOP citations.\n",
    "    {format_instructions}\n",
    "    \n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    Answer: {answer}\n",
    "    \"\"\"\n",
    ")\n",
    "grader_chain = grader_prompt | llm | grader_parser\n",
    "\n",
    "# --- 3. Run the Generation and Grading ---\n",
    "context = 'SOP-122 states: \"For spindle bearings, lubricate every 400 operating hours. Always perform lockout/tagout before maintenance.\"'\n",
    "question = 'How do I address spindle vibration after a bearing swap?'\n",
    "\n",
    "# Generate the initial answer\n",
    "answer_obj = generator_chain.invoke({\"context\": context, \"question\": question})\n",
    "answer = answer_obj.content\n",
    "\n",
    "# Grade the answer\n",
    "grade = grader_chain.invoke({\n",
    "    \"context\": context, \n",
    "    \"question\": question, \n",
    "    \"answer\": answer,\n",
    "    \"format_instructions\": grader_parser.get_format_instructions()\n",
    "})\n",
    "\n",
    "print(\"--- Initial Answer ---\")\n",
    "print(answer)\n",
    "print(\"\\n--- Structured Grade ---\")\n",
    "print(grade)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bcc740",
   "metadata": {},
   "source": [
    "### üîÅ Corrective Loop\n",
    "1. Generate answer.\n",
    "2. Parse grader output into structured scores.\n",
    "3. If score < threshold, regenerate with stricter prompt or escalate to SME.\n",
    "4. Log final decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f9da6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# --- Corrective Loop Logic ---\n",
    "# This loop decides whether to approve the answer, revise it, or escalate to a human.\n",
    "\n",
    "def run_corrective_loop(question: str, answer: str, grade: dict, context: str):\n",
    "    min_score = min(grade.values())\n",
    "    \n",
    "    log = {\n",
    "        'question': question,\n",
    "        'initial_answer': answer,\n",
    "        'scores': grade,\n",
    "        'disposition': ''\n",
    "    }\n",
    "\n",
    "    if min_score >= 0.8:\n",
    "        log['disposition'] = 'auto-approved'\n",
    "        log['final_answer'] = answer\n",
    "        print(\"Disposition: Auto-Approved\")\n",
    "        \n",
    "    else:\n",
    "        log['disposition'] = 'escalated_to_sme'\n",
    "        # In a real system, you would trigger a notification here (e.g., call a ServiceNow tool)\n",
    "        escalation_message = (\n",
    "            f\"‚ö†Ô∏è SME REVIEW REQUIRED ‚ö†Ô∏è\\n\"\n",
    "            f\"An LLM-generated answer failed quality checks.\\n\"\n",
    "            f\"Question: {question}\\n\"\n",
    "            f\"Proposed Answer: {answer}\\n\"\n",
    "            f\"Failing Scores: { {k: v for k, v in grade.items() if v < 0.8} }\\n\"\n",
    "            f\"Please review and provide a corrected answer.\"\n",
    "        )\n",
    "        log['final_answer'] = escalation_message\n",
    "        print(f\"Disposition: Escalated to SME\\nReason: {escalation_message}\")\n",
    "        \n",
    "    return log\n",
    "\n",
    "# --- Run the loop with the results from the previous cell ---\n",
    "final_log = run_corrective_loop(question, answer, grade, context)\n",
    "\n",
    "print(\"\\n--- Final Governance Log ---\")\n",
    "print(json.dumps(final_log, indent=2))\n",
    "\n",
    "# --- Example of a failing case ---\n",
    "print(\"\\n\\n--- SIMULATING A FAILING ANSWER ---\")\n",
    "failing_answer = \"Just replace the bearings. It's easy.\"\n",
    "failing_grade = grader_chain.invoke({\n",
    "    \"context\": context, \n",
    "    \"question\": question, \n",
    "    \"answer\": failing_answer,\n",
    "    \"format_instructions\": grader_parser.get_format_instructions()\n",
    "})\n",
    "run_corrective_loop(question, failing_answer, failing_grade, context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74567b81",
   "metadata": {},
   "source": [
    "## üß™ Lab Assignment\n",
    "1. **Implement Structured Output Parser**: Replace the free-form grader output with a `StructuredOutputParser` using this exact schema:\n",
    "   ```python\n",
    "   from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "   \n",
    "   response_schemas = [\n",
    "       ResponseSchema(name=\"accuracy_score\", description=\"Accuracy score between 0-1 based on factual correctness\"),\n",
    "       ResponseSchema(name=\"safety_compliance\", description=\"Score between 0-1 for safety disclaimer presence and relevance\"),\n",
    "       ResponseSchema(name=\"citation_quality\", description=\"Score between 0-1 for proper citation of SOPs\")\n",
    "   ]\n",
    "   \n",
    "   parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "   format_instructions = parser.get_format_instructions()\n",
    "   \n",
    "   # Update grader prompt to include format instructions\n",
    "   ```\n",
    "\n",
    "2. **Add Hallucination Detection**: Implement a retrieval cross-check function that compares answer claims with the actual documentation:\n",
    "   ```python\n",
    "   def detect_hallucinations(answer: str, context: str) -> float:\n",
    "       # Split answer into claims\n",
    "       # Check each claim against context using semantic similarity\n",
    "       # Return hallucination score (lower is better)\n",
    "       # Implementation details in lab instructions\n",
    "       pass\n",
    "   ```\n",
    "\n",
    "3. **Implement ServiceNow Integration**: Create a function that sends escalations to ServiceNow when grading score is below threshold:\n",
    "   ```python\n",
    "   def escalate_to_servicenow(question: str, answer: str, scores: dict) -> str:\n",
    "       # Format ServiceNow incident payload\n",
    "       payload = {\n",
    "           \"incident\": {\n",
    "               \"short_description\": f\"LLM Answer Escalation: Score {min(scores.values()):.2f}\",\n",
    "               \"description\": f\"Question: {question}\\nAnswer: {answer}\\nScores: {scores}\",\n",
    "               \"priority\": \"3\" if min(scores.values()) > 0.5 else \"2\",\n",
    "               \"assignment_group\": \"Manufacturing_SME\"\n",
    "           }\n",
    "       }\n",
    "       # Return incident ID\n",
    "       return \"INC0010234\"  # Simulated response\n",
    "   ```\n",
    "\n",
    "4. **Create Weekly Metrics Dashboard**: Implement a function that aggregates grading metrics:\n",
    "   ```python\n",
    "   def generate_weekly_report(log_file_path: str) -> dict:\n",
    "       # Load all grading logs for the week\n",
    "       # Calculate averages by category (accuracy, safety, citations)\n",
    "       # Count escalations by reason\n",
    "       # Format for safety committee review\n",
    "       pass\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d77a1a",
   "metadata": {},
   "source": [
    "## ‚úÖ Checklist\n",
    "- [ ] Structured grader output parser implemented and returning proper JSON\n",
    "- [ ] Hallucination detector identifies at least 3 types of factual discrepancies\n",
    "- [ ] ServiceNow escalation function formats incidents with all required fields\n",
    "- [ ] Weekly metrics report generates proper statistics (min/max/avg) by category\n",
    "- [ ] Full correction loop handles both auto-fixes and SME escalation paths\n",
    "- [ ] All components integrated in the main loop with proper error handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e3c956",
   "metadata": {},
   "source": [
    "## üìö References\n",
    "- Corrective RAG Blog (LangChain)\n",
    "- ServiceNow Change Management APIs\n",
    "- Week 11 Monitoring Notebook"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
