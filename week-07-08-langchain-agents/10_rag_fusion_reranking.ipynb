{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38d11885",
   "metadata": {},
   "source": [
    "# ‚öñÔ∏è Week 07-08 ¬∑ Notebook 10 ¬∑ RAG Fusion & Reranking\n",
    "\n",
    "Blend multiple retrieval signals and apply rerankers to deliver precise, auditable answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb039525",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "- Implement reciprocal rank fusion across vector stores.\n",
    "- Integrate cross-encoder rerankers (e.g., Cohere, HuggingFace).\n",
    "- Evaluate precision@k and response quality improvements.\n",
    "- Capture reranker explainability metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b51bd79",
   "metadata": {},
   "source": [
    "## üß© Scenario\n",
    "Combining SOP and incident stores yields better context. The QA team wants data on how rerankers improve top-3 accuracy for safety-critical questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7794b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# --- 1. Setup: Create multiple, distinct knowledge sources ---\n",
    "emb = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# SOPs (Standard Operating Procedures) - The \"how-to\" guides\n",
    "sop_docs = [\n",
    "    Document(page_content=\"SOP-122: For bearing lubrication, use Grade 2 lithium grease only.\"),\n",
    "    Document(page_content=\"SOP-900: Lockout/Tagout procedure must be followed before any maintenance.\"),\n",
    "    Document(page_content=\"SOP-123: If a bearing is overheating, shut down the machine immediately and inspect for damage.\")\n",
    "]\n",
    "sop_store = FAISS.from_documents(sop_docs, emb)\n",
    "\n",
    "# Incident Logs - Historical data of what went wrong\n",
    "incident_docs = [\n",
    "    Document(page_content=\"Incident 2023-55: Bearing failure on Press-07 led to 8 hours of downtime. Root cause: improper lubrication.\"),\n",
    "    Document(page_content=\"Incident 2024-12: Overheating bearing on CNC-02 was caught during a routine check. Re-lubricated per SOP-122.\"),\n",
    "    Document(page_content=\"Incident 2024-21: Coolant shortage caused multiple machines to overheat.\")\n",
    "]\n",
    "incident_store = FAISS.from_documents(incident_docs, emb)\n",
    "\n",
    "\n",
    "# --- 2. RAG Fusion using a simple combined retriever ---\n",
    "# A simple fusion approach is to just search over both stores.\n",
    "# LangChain's `merger_retriever` is another more advanced option.\n",
    "class CombinedRetriever:\n",
    "    def __init__(self, retrievers):\n",
    "        self.retrievers = retrievers\n",
    "    def get_relevant_documents(self, query):\n",
    "        docs = []\n",
    "        for retriever in self.retrievers:\n",
    "            docs.extend(retriever.get_relevant_documents(query))\n",
    "        return docs\n",
    "\n",
    "# Create a retriever that searches both SOPs and Incidents\n",
    "fusion_retriever = CombinedRetriever(retrievers=[sop_store.as_retriever(k=3), incident_store.as_retriever(k=3)])\n",
    "\n",
    "query = 'Bearing is overheating after maintenance shift'\n",
    "fused_docs = fusion_retriever.get_relevant_documents(query)\n",
    "\n",
    "print(\"--- Fused Documents (Before Reranking) ---\")\n",
    "for doc in fused_docs:\n",
    "    print(f\"- {doc.page_content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dedab29",
   "metadata": {},
   "source": [
    "### üîÅ Cross-Encoder Reranking\n",
    "Score fused results using a transformer reranker for improved precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8452927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 3. Cross-Encoder Reranking ---\n",
    "# A cross-encoder model takes the query and a document and outputs a relevance score.\n",
    "# It is much more accurate than vector similarity but slower, so it's used to rerank the top N results.\n",
    "model = HuggingFaceCrossEncoder(model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "compressor = CrossEncoderReranker(model=model, top_n=3) # Keep the top 3 most relevant docs\n",
    "\n",
    "# The compression retriever wraps the base retriever and the reranker\n",
    "reranking_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, \n",
    "    base_retriever=fusion_retriever\n",
    ")\n",
    "\n",
    "# --- 4. Get Reranked Documents ---\n",
    "reranked_docs = reranking_retriever.get_relevant_documents(query)\n",
    "\n",
    "print(\"\\n--- Reranked & Compressed Documents (Top 3) ---\")\n",
    "for doc in reranked_docs:\n",
    "    print(f\"- {doc.page_content} (Relevance: {doc.metadata['relevance_score']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e587f4a",
   "metadata": {},
   "source": [
    "## üìä Evaluation\n",
    "1. Use labeled QA pairs (Week 09 dataset) to compute precision@3, recall@5, and reranker uplift.\n",
    "2. Log metrics to MLflow: `baseline_precision`, `reranked_precision`, `uplift_pct`.\n",
    "3. Record reranker version, model checksum, and calibration notes for audits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26b6e43",
   "metadata": {},
   "source": [
    "## üß™ Lab Assignment\n",
    "1. Evaluate multiple weight combinations in RRF and identify optimal settings per plant.\n",
    "2. Benchmark rerankers (Cohere vs. HuggingFace) for latency and accuracy.\n",
    "3. Produce a dashboard (Looker or Superset) tracking retrieval KPIs.\n",
    "4. Share findings with governance board."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ae6494",
   "metadata": {},
   "source": [
    "## ‚úÖ Checklist\n",
    "- [ ] Fusion retriever implemented\n",
    "- [ ] Reranker evaluated\n",
    "- [ ] Metrics logged\n",
    "- [ ] Governance artefacts stored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1923ae0f",
   "metadata": {},
   "source": [
    "## üìö References\n",
    "- Reciprocal Rank Fusion Research\n",
    "- Cross-Encoder Reranking Guides\n",
    "- Week 09 Evaluation Harness"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
