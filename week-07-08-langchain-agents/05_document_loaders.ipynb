{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b703405",
   "metadata": {},
   "source": [
    "# üì• Week 07-08 ¬∑ Notebook 05 ¬∑ Document Loaders & Compliance Filters\n",
    "\n",
    "Ingest manufacturing knowledge sources safely using LangChain loaders and custom sanitizers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb41d47f",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "- Load SOP manuals (PDF), maintenance logs (CSV), and safety bulletins (HTML).\n",
    "- Enrich documents with plant metadata for downstream retrieval.\n",
    "- Apply PII redaction and policy filters during ingestion.\n",
    "- Validate ingestion pipelines with governance evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e94146",
   "metadata": {},
   "source": [
    "## üß© Scenario\n",
    "Corporate legal mandates that all maintenance tickets older than 365 days be excluded from RAG context. Build loaders that enforce the rule and tag documents by plant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3866515d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import PyPDFLoader, CSVLoader, UnstructuredHTMLLoader\n",
    "import re\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the cutoff for old data as per legal requirements\n",
    "CUTOFF_DATE = datetime.utcnow() - timedelta(days=365)\n",
    "DATA_DIR = Path('data')\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def load_and_filter_csv(path: Path, metadata: dict):\n",
    "    \"\"\"\n",
    "    Loads a CSV file and filters records based on a timestamp.\n",
    "    Assumes a 'timestamp_utc' column in ISO format.\n",
    "    \"\"\"\n",
    "    loader = CSVLoader(\n",
    "        file_path=str(path),\n",
    "        csv_args={'delimiter': ','},\n",
    "        source_column='ticket_id' # Use ticket_id as the source identifier\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    \n",
    "    filtered_docs = []\n",
    "    for doc in docs:\n",
    "        # The CSVLoader puts all columns into page_content. We need to parse it.\n",
    "        # This is a simplification; a real implementation might use a more robust parser.\n",
    "        try:\n",
    "            # A simple way to find the timestamp in the unstructured page_content\n",
    "            timestamp_str = re.search(r\"timestamp_utc: ([\\d-T:Z]+)\", doc.page_content)\n",
    "            if timestamp_str:\n",
    "                timestamp = datetime.fromisoformat(timestamp_str.group(1).replace('Z', '+00:00'))\n",
    "                if timestamp.replace(tzinfo=None) >= CUTOFF_DATE:\n",
    "                    doc.metadata.update(metadata)\n",
    "                    filtered_docs.append(doc)\n",
    "        except (ValueError, TypeError):\n",
    "            # Handle cases where the timestamp is missing or malformed\n",
    "            print(f\"Skipping record due to missing/malformed timestamp in {doc.metadata.get('source')}\")\n",
    "            continue\n",
    "            \n",
    "    return filtered_docs\n",
    "\n",
    "def load_generic(loader_cls, path: Path, metadata: dict):\n",
    "    \"\"\"Generic loader for file types that don't need special filtering.\"\"\"\n",
    "    loader = loader_cls(str(path))\n",
    "    docs = loader.load()\n",
    "    for doc in docs:\n",
    "        doc.metadata.update(metadata)\n",
    "    return docs\n",
    "\n",
    "def redact_pii(text: str) -> str:\n",
    "    \"\"\"A simple PII redactor. In production, use a more advanced tool like Presidio.\"\"\"\n",
    "    # This regex looks for names like \"Technician John Doe\"\n",
    "    return re.sub(r\"Technician\\s+[A-Z][a-z]+\\s+[A-Z][a-z]+\", \"Technician [REDACTED]\", text)\n",
    "\n",
    "# --- Data Ingestion Pipeline ---\n",
    "\n",
    "# Define file paths\n",
    "pdf_path = DATA_DIR / 'sop_manuals' / 'press_safety.pdf'\n",
    "csv_path = DATA_DIR / 'logs' / 'maintenance_logs.csv'\n",
    "html_path = DATA_DIR / 'bulletins' / 'ehs_update.html'\n",
    "\n",
    "# Load documents from different sources with appropriate metadata\n",
    "pdf_docs = load_generic(PyPDFLoader, pdf_path, {'plant': 'PNQ', 'doc_type': 'SOP'})\n",
    "log_docs = load_and_filter_csv(csv_path, {'plant': 'PNQ', 'doc_type': 'ticket'})\n",
    "bulletin_docs = load_generic(UnstructuredHTMLLoader, html_path, {'plant': 'PNQ', 'doc_type': 'ehs'})\n",
    "\n",
    "print(f'Loaded {len(pdf_docs)} SOP pages.')\n",
    "print(f'Loaded {len(log_docs)} recent maintenance tickets (older than 1 year excluded).')\n",
    "print(f'Loaded {len(bulletin_docs)} HTML notices.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16de34f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_documents(docs):\n",
    "    for doc in docs:\n",
    "        doc.page_content = redact_pii(doc.page_content)\n",
    "        doc.metadata['ingested_at'] = datetime.utcnow().isoformat() + 'Z'\n",
    "    return docs\n",
    "\n",
    "sanitized_docs = sanitize_documents(pdf_docs + log_docs + bulletin_docs)\n",
    "sanitized_docs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9161c98",
   "metadata": {},
   "source": [
    "## üîê Compliance Checklist\n",
    "- Tickets older than 365 days removed (`timestamp_utc` filter).\n",
    "- PII sanitized using redaction function.\n",
    "- Metadata tags (`plant`, `doc_type`, `ingested_at`) applied.\n",
    "- Evidence exported to governance storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861b9c29",
   "metadata": {},
   "source": [
    "## üß™ Lab Assignment\n",
    "1. Extend the loader to ingest CAD manuals (DOCX) via Unstructured.\n",
    "2. Plug in the Great Expectations suite from Week 05 to validate metadata fields.\n",
    "3. Export sanitized documents to `data/processed/{plant}` with checksum manifest.\n",
    "4. Document ingestion run (dataset, operator, duration) and attach to change ticket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1e3dd0",
   "metadata": {},
   "source": [
    "## ‚úÖ Checklist\n",
    "- [ ] Datasets ingested with metadata\n",
    "- [ ] PII redaction validated\n",
    "- [ ] Cutoff policy enforced\n",
    "- [ ] Evidence archived\n",
    "\n",
    "## üìö References\n",
    "- LangChain Document Loaders Documentation\n",
    "- Corporate Data Retention Policy 2024\n",
    "- Week 05 Data Intake Notebook"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
