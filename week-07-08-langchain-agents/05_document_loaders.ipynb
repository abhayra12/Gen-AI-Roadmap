{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b703405",
   "metadata": {},
   "source": [
    "# üì• Week 07-08 ¬∑ Notebook 05: Document Loaders & The Ingestion Pipeline\n",
    "\n",
    "**Objective:** Build a robust and compliant data ingestion pipeline for a RAG application by loading, transforming, and enriching documents from various sources.\n",
    "\n",
    "In this notebook, we will simulate a real-world scenario where a manufacturing company needs to build a knowledge base for a maintenance chatbot. The process of preparing data for a RAG system is known as the **ingestion pipeline**. It involves several critical steps:\n",
    "\n",
    "1.  **Loading:** Reading raw data from different file formats (PDFs, CSVs, web pages).\n",
    "2.  **Transforming:** Cleaning, filtering, and modifying the content (e.g., redacting sensitive information).\n",
    "3.  **Enriching:** Adding valuable metadata to each document for better retrieval and context.\n",
    "\n",
    "We will use LangChain's powerful `DocumentLoader` ecosystem to handle this process efficiently and apply custom logic to meet strict business and legal requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb41d47f",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1.  **Load Documents from Multiple Sources:** Use LangChain's `DocumentLoader` classes to ingest data from PDF, CSV, and HTML files.\n",
    "2.  **Apply Custom Transformations:** Implement functions to filter documents based on business rules (e.g., date cutoffs).\n",
    "3.  **Redact Sensitive Information:** Create a sanitization step to remove Personally Identifiable Information (PII) from document content.\n",
    "4.  **Enrich Documents with Metadata:** Systematically add metadata (like source, plant, and ingestion timestamp) to each document for improved traceability and retrieval.\n",
    "5.  **Construct a Reusable Ingestion Pipeline:** Combine loading, transforming, and enriching into a coherent workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e94146",
   "metadata": {},
   "source": [
    "## üß© Scenario: Building a Compliant Knowledge Base for a Maintenance Chatbot\n",
    "\n",
    "You are an AI engineer at a large manufacturing company. Your task is to build the data foundation for a new RAG-based chatbot that will help maintenance technicians diagnose and repair machinery.\n",
    "\n",
    "The knowledge base must be built from three sources:\n",
    "1.  **SOP Manuals (PDF):** Official Standard Operating Procedures for all equipment.\n",
    "2.  **Maintenance Logs (CSV):** A running log of all maintenance tickets, including technician notes.\n",
    "3.  **Safety Bulletins (HTML):** Internal web pages with the latest safety alerts.\n",
    "\n",
    "However, there are strict compliance requirements:\n",
    "-   **Data Retention Policy:** The company's legal department mandates that any maintenance ticket older than 365 days must be excluded from the RAG system's knowledge base to avoid providing outdated advice.\n",
    "-   **PII Redaction:** To protect employee privacy, all technician names mentioned in the maintenance logs must be redacted.\n",
    "-   **Traceability:** Every document ingested into the system must be tagged with metadata indicating its source, the plant it belongs to, and when it was ingested.\n",
    "\n",
    "Your goal is to build an ingestion pipeline that loads data from these sources while enforcing all three compliance rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcff857c",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Creation\n",
    "\n",
    "First, let's install the required libraries. We'll need `langchain` for the core framework, `langchain-community` for the document loaders, and `unstructured` for processing HTML and other file types.\n",
    "\n",
    "> ‚ö†Ô∏è **Kernel Restart**: After running the installation cell below, you may need to restart the kernel for the changes to take effect. You can do this from the \"Kernel\" menu in your Jupyter environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63729774",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain langchain-community unstructured pypdf beautifulsoup4 reportlab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c091e321",
   "metadata": {},
   "source": [
    "### Creating Dummy Data Files\n",
    "\n",
    "To make this notebook self-contained, we'll programmatically create the dummy data files. This cell will generate the PDF, CSV, and HTML files needed for our scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda76817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.pagesizes import letter\n",
    "\n",
    "# --- Create Directories ---\n",
    "DATA_DIR = \"data\"\n",
    "os.makedirs(os.path.join(DATA_DIR, \"sop_manuals\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(DATA_DIR, \"logs\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(DATA_DIR, \"bulletins\"), exist_ok=True)\n",
    "\n",
    "# --- 1. Create a Dummy PDF SOP Manual ---\n",
    "pdf_path = os.path.join(DATA_DIR, \"sop_manuals\", \"press_safety.pdf\")\n",
    "c = canvas.Canvas(pdf_path, pagesize=letter)\n",
    "c.drawString(72, 800, \"Standard Operating Procedure: Hydraulic Press H-45\")\n",
    "c.drawString(72, 780, \"1. Ensure all safety guards are in place before operation.\")\n",
    "c.drawString(72, 760, \"2. Perform daily maintenance checks as per the log.\")\n",
    "c.save()\n",
    "print(f\"Created dummy PDF: {pdf_path}\")\n",
    "\n",
    "# --- 2. Create a Dummy CSV Maintenance Log ---\n",
    "csv_path = os.path.join(DATA_DIR, \"logs\", \"maintenance_logs.csv\")\n",
    "now = datetime.utcnow()\n",
    "two_years_ago = now - timedelta(days=730)\n",
    "six_months_ago = now - timedelta(days=180)\n",
    "\n",
    "log_data = [\n",
    "    [\"ticket_id\", \"timestamp_utc\", \"issue_description\", \"technician_notes\"],\n",
    "    [\"TICKET-001\", two_years_ago.isoformat() + \"Z\", \"Hydraulic fluid leak\", \"Replaced seal. Work completed by Technician Jane Doe.\"],\n",
    "    [\"TICKET-002\", six_months_ago.isoformat() + \"Z\", \"Pressure sensor fault\", \"Recalibrated sensor. Work completed by Technician John Smith.\"],\n",
    "    [\"TICKET-003\", now.isoformat() + \"Z\", \"Emergency stop button stuck\", \"Replaced the button assembly. Work completed by Technician Emily Jones.\"]\n",
    "]\n",
    "\n",
    "with open(csv_path, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(log_data)\n",
    "print(f\"Created dummy CSV: {csv_path}\")\n",
    "\n",
    "# --- 3. Create a Dummy HTML Safety Bulletin ---\n",
    "html_path = os.path.join(DATA_DIR, \"bulletins\", \"ehs_update.html\")\n",
    "html_content = \"\"\"\n",
    "<html>\n",
    "<head><title>EHS Update</title></head>\n",
    "<body>\n",
    "    <h1>Safety Bulletin: Q3 2024</h1>\n",
    "    <p>All personnel must complete the mandatory annual safety training by October 31st.</p>\n",
    "    <p>A new lockout/tagout procedure is now in effect for the assembly line.</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "with open(html_path, \"w\") as f:\n",
    "    f.write(html_content)\n",
    "print(f\"Created dummy HTML: {html_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefc03c0",
   "metadata": {},
   "source": [
    "## 2. Loading Documents with LangChain Loaders\n",
    "\n",
    "LangChain provides a rich ecosystem of `DocumentLoader` classes, each designed for a specific file type or data source. A `Document` is a simple object with `page_content` (the text) and `metadata` (a dictionary of attributes).\n",
    "\n",
    "We will use three different loaders for our sources:\n",
    "-   `PyPDFLoader`: For loading and parsing PDF files.\n",
    "-   `UnstructuredHTMLLoader`: For parsing HTML files. It's part of the powerful `unstructured` library that can handle many complex formats.\n",
    "-   `CSVLoader`: For loading data from CSV files.\n",
    "\n",
    "### Generic Loader Function\n",
    "For simple cases like PDF and HTML, we can create a generic function to load the document and add our desired metadata. We'll tag each document with the `plant` and `doc_type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17df1d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, UnstructuredHTMLLoader\n",
    "from pathlib import Path\n",
    "\n",
    "def load_and_enrich(loader_cls, file_path: str, metadata: dict):\n",
    "    \"\"\"\n",
    "    Generic function to load a document and enrich it with metadata.\n",
    "    \"\"\"\n",
    "    # Instantiate the loader with the file path\n",
    "    loader = loader_cls(file_path)\n",
    "    # Load the documents\n",
    "    docs = loader.load()\n",
    "    # Add the provided metadata to each loaded document\n",
    "    for doc in docs:\n",
    "        doc.metadata.update(metadata)\n",
    "    return docs\n",
    "\n",
    "# Define file paths and metadata\n",
    "pdf_path = os.path.join(DATA_DIR, \"sop_manuals\", \"press_safety.pdf\")\n",
    "html_path = os.path.join(DATA_DIR, \"bulletins\", \"ehs_update.html\")\n",
    "\n",
    "# Load the PDF and HTML documents\n",
    "pdf_docs = load_and_enrich(PyPDFLoader, pdf_path, {\"plant\": \"PNQ\", \"doc_type\": \"SOP\"})\n",
    "html_docs = load_and_enrich(UnstructuredHTMLLoader, html_path, {\"plant\": \"PNQ\", \"doc_type\": \"EHS_Bulletin\"})\n",
    "\n",
    "print(f\"Loaded {len(pdf_docs)} PDF document(s).\")\n",
    "print(f\"Loaded {len(html_docs)} HTML document(s).\")\n",
    "print(\"\\n--- Sample PDF Metadata ---\")\n",
    "print(pdf_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e6ffdd",
   "metadata": {},
   "source": [
    "### Custom Loader for CSV with Date Filtering\n",
    "\n",
    "The maintenance logs (CSV) require special handling to meet the legal requirement of excluding records older than 365 days. We can't use a generic loader because we need to inspect the content of each row and apply a filter.\n",
    "\n",
    "We will create a custom function that:\n",
    "1.  Defines a `CUTOFF_DATE` (365 days ago).\n",
    "2.  Uses `CSVLoader` to load all records from the file. `CSVLoader` creates one `Document` per row.\n",
    "3.  Iterates through each `Document` and parses the `timestamp_utc` from the `page_content`.\n",
    "4.  Compares the record's timestamp to the `CUTOFF_DATE`.\n",
    "5.  Only keeps the records that are more recent than the cutoff date.\n",
    "6.  Enriches the valid records with the appropriate metadata.\n",
    "\n",
    "This demonstrates how to inject custom business logic directly into your ingestion pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2d5377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "import re\n",
    "\n",
    "# Define the cutoff for old data as per legal requirements\n",
    "CUTOFF_DATE = datetime.utcnow() - timedelta(days=365)\n",
    "\n",
    "def load_and_filter_csv(file_path: str, metadata: dict):\n",
    "    \"\"\"\n",
    "    Loads a CSV file, filters records based on a timestamp, and enriches metadata.\n",
    "    \"\"\"\n",
    "    # Use CSVLoader, specifying which column to use as the 'source' in metadata\n",
    "    loader = CSVLoader(\n",
    "        file_path=file_path,\n",
    "        source_column='ticket_id'\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    \n",
    "    filtered_docs = []\n",
    "    print(f\"Loaded {len(docs)} total records from CSV. Applying date filter...\")\n",
    "    \n",
    "    for doc in docs:\n",
    "        # The CSVLoader crams all columns into page_content. We need to parse it.\n",
    "        # A simple regex can find the timestamp in the unstructured page_content.\n",
    "        timestamp_match = re.search(r\"timestamp_utc: ([\\d\\-T:Z.]+)\", doc.page_content)\n",
    "        \n",
    "        if timestamp_match:\n",
    "            timestamp_str = timestamp_match.group(1).replace('Z', '')\n",
    "            record_timestamp = datetime.fromisoformat(timestamp_str)\n",
    "            \n",
    "            # Compare the record's date with our cutoff date\n",
    "            if record_timestamp >= CUTOFF_DATE:\n",
    "                # If the record is recent enough, add it to our list\n",
    "                doc.metadata.update(metadata)\n",
    "                filtered_docs.append(doc)\n",
    "            else:\n",
    "                print(f\"  - Excluding old record: {doc.metadata['source']} (date: {record_timestamp.date()})\")\n",
    "        else:\n",
    "            print(f\"  - Skipping record with missing timestamp: {doc.metadata['source']}\")\n",
    "            \n",
    "    return filtered_docs\n",
    "\n",
    "# Define the path to the CSV file\n",
    "csv_path = os.path.join(DATA_DIR, \"logs\", \"maintenance_logs.csv\")\n",
    "\n",
    "# Load and filter the CSV documents\n",
    "csv_docs = load_and_filter_csv(csv_path, {\"plant\": \"PNQ\", \"doc_type\": \"MaintenanceLog\"})\n",
    "\n",
    "print(f\"\\nLoaded {len(csv_docs)} recent maintenance log(s) after filtering.\")\n",
    "print(\"\\n--- Sample Filtered CSV Record ---\")\n",
    "if csv_docs:\n",
    "    print(csv_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03097e40",
   "metadata": {},
   "source": [
    "## 3. Transforming and Sanitizing Documents\n",
    "\n",
    "Loading is just the first step. A robust ingestion pipeline must also clean and transform the data to meet compliance and quality standards. We will perform two transformations:\n",
    "\n",
    "1.  **PII Redaction:** We'll create a function to find and replace technician names in the document content, replacing them with `[REDACTED]`. In a production environment, you would use a more sophisticated tool like Amazon Comprehend, Google Cloud DLP, or Microsoft Presidio, but a simple regex is sufficient for this example.\n",
    "2.  **Final Enrichment:** We'll add a final piece of metadata‚Äîan `ingested_at` timestamp‚Äîto every document for audit and traceability purposes.\n",
    "\n",
    "We'll wrap these steps in a `sanitize_documents` function that processes a list of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691838c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def redact_pii(text: str) -> str:\n",
    "    \"\"\"\n",
    "    A simple PII redactor that looks for names following 'Technician'.\n",
    "    In production, use a more advanced tool like Microsoft Presidio or AWS Comprehend.\n",
    "    \"\"\"\n",
    "    # This regex finds \"Technician\" followed by a capitalized first and last name.\n",
    "    return re.sub(r\"Technician\\s+([A-Z][a-z]+\\s+[A-Z][a-z]+)\", \"Technician [REDACTED]\", text)\n",
    "\n",
    "def sanitize_and_enrich(docs: list) -> list:\n",
    "    \"\"\"\n",
    "    Applies PII redaction and adds a final ingestion timestamp to each document.\n",
    "    \"\"\"\n",
    "    sanitized_docs = []\n",
    "    for doc in docs:\n",
    "        # Apply the PII redaction to the document's content\n",
    "        doc.page_content = redact_pii(doc.page_content)\n",
    "        \n",
    "        # Add the final enrichment metadata\n",
    "        doc.metadata['ingested_at'] = datetime.utcnow().isoformat() + 'Z'\n",
    "        \n",
    "        sanitized_docs.append(doc)\n",
    "    return sanitized_docs\n",
    "\n",
    "# Combine all our loaded documents into a single list\n",
    "all_docs = pdf_docs + html_docs + csv_docs\n",
    "\n",
    "# Run the final sanitization and enrichment step\n",
    "final_docs = sanitize_and_enrich(all_docs)\n",
    "\n",
    "print(f\"Total documents in the final knowledge base: {len(final_docs)}\")\n",
    "print(\"\\n--- Sample Sanitized Document (from CSV) ---\")\n",
    "# Find a sanitized CSV doc to display\n",
    "for doc in final_docs:\n",
    "    if doc.metadata['doc_type'] == 'MaintenanceLog':\n",
    "        print(\"Original Content Snippet from CSV log might contain a name.\")\n",
    "        print(\"Sanitized Content:\")\n",
    "        print(doc.page_content)\n",
    "        print(\"\\nFinal Metadata:\")\n",
    "        print(doc.metadata)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3866515d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import PyPDFLoader, CSVLoader, UnstructuredHTMLLoader\n",
    "import re\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the cutoff for old data as per legal requirements\n",
    "CUTOFF_DATE = datetime.utcnow() - timedelta(days=365)\n",
    "DATA_DIR = Path('data')\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def load_and_filter_csv(path: Path, metadata: dict):\n",
    "    \"\"\"\n",
    "    Loads a CSV file and filters records based on a timestamp.\n",
    "    Assumes a 'timestamp_utc' column in ISO format.\n",
    "    \"\"\"\n",
    "    loader = CSVLoader(\n",
    "        file_path=str(path),\n",
    "        csv_args={'delimiter': ','},\n",
    "        source_column='ticket_id' # Use ticket_id as the source identifier\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    \n",
    "    filtered_docs = []\n",
    "    for doc in docs:\n",
    "        # The CSVLoader puts all columns into page_content. We need to parse it.\n",
    "        # This is a simplification; a real implementation might use a more robust parser.\n",
    "        try:\n",
    "            # A simple way to find the timestamp in the unstructured page_content\n",
    "            timestamp_str = re.search(r\"timestamp_utc: ([\\d-T:Z]+)\", doc.page_content)\n",
    "            if timestamp_str:\n",
    "                timestamp = datetime.fromisoformat(timestamp_str.group(1).replace('Z', '+00:00'))\n",
    "                if timestamp.replace(tzinfo=None) >= CUTOFF_DATE:\n",
    "                    doc.metadata.update(metadata)\n",
    "                    filtered_docs.append(doc)\n",
    "        except (ValueError, TypeError):\n",
    "            # Handle cases where the timestamp is missing or malformed\n",
    "            print(f\"Skipping record due to missing/malformed timestamp in {doc.metadata.get('source')}\")\n",
    "            continue\n",
    "            \n",
    "    return filtered_docs\n",
    "\n",
    "def load_generic(loader_cls, path: Path, metadata: dict):\n",
    "    \"\"\"Generic loader for file types that don't need special filtering.\"\"\"\n",
    "    loader = loader_cls(str(path))\n",
    "    docs = loader.load()\n",
    "    for doc in docs:\n",
    "        doc.metadata.update(metadata)\n",
    "    return docs\n",
    "\n",
    "def redact_pii(text: str) -> str:\n",
    "    \"\"\"A simple PII redactor. In production, use a more advanced tool like Presidio.\"\"\"\n",
    "    # This regex looks for names like \"Technician John Doe\"\n",
    "    return re.sub(r\"Technician\\s+[A-Z][a-z]+\\s+[A-Z][a-z]+\", \"Technician [REDACTED]\", text)\n",
    "\n",
    "# --- Data Ingestion Pipeline ---\n",
    "\n",
    "# Define file paths\n",
    "pdf_path = DATA_DIR / 'sop_manuals' / 'press_safety.pdf'\n",
    "csv_path = DATA_DIR / 'logs' / 'maintenance_logs.csv'\n",
    "html_path = DATA_DIR / 'bulletins' / 'ehs_update.html'\n",
    "\n",
    "# Load documents from different sources with appropriate metadata\n",
    "pdf_docs = load_generic(PyPDFLoader, pdf_path, {'plant': 'PNQ', 'doc_type': 'SOP'})\n",
    "log_docs = load_and_filter_csv(csv_path, {'plant': 'PNQ', 'doc_type': 'ticket'})\n",
    "bulletin_docs = load_generic(UnstructuredHTMLLoader, html_path, {'plant': 'PNQ', 'doc_type': 'ehs'})\n",
    "\n",
    "print(f'Loaded {len(pdf_docs)} SOP pages.')\n",
    "print(f'Loaded {len(log_docs)} recent maintenance tickets (older than 1 year excluded).')\n",
    "print(f'Loaded {len(bulletin_docs)} HTML notices.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16de34f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_documents(docs):\n",
    "    for doc in docs:\n",
    "        doc.page_content = redact_pii(doc.page_content)\n",
    "        doc.metadata['ingested_at'] = datetime.utcnow().isoformat() + 'Z'\n",
    "    return docs\n",
    "\n",
    "sanitized_docs = sanitize_documents(pdf_docs + log_docs + bulletin_docs)\n",
    "sanitized_docs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9161c98",
   "metadata": {},
   "source": [
    "## üîê Compliance Checklist\n",
    "- Tickets older than 365 days removed (`timestamp_utc` filter).\n",
    "- PII sanitized using redaction function.\n",
    "- Metadata tags (`plant`, `doc_type`, `ingested_at`) applied.\n",
    "- Evidence exported to governance storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861b9c29",
   "metadata": {},
   "source": [
    "## üß™ Lab Assignment\n",
    "1. Extend the loader to ingest CAD manuals (DOCX) via Unstructured.\n",
    "2. Plug in the Great Expectations suite from Week 05 to validate metadata fields.\n",
    "3. Export sanitized documents to `data/processed/{plant}` with checksum manifest.\n",
    "4. Document ingestion run (dataset, operator, duration) and attach to change ticket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1e3dd0",
   "metadata": {},
   "source": [
    "## ‚úÖ Checklist\n",
    "- [ ] Datasets ingested with metadata\n",
    "- [ ] PII redaction validated\n",
    "- [ ] Cutoff policy enforced\n",
    "- [ ] Evidence archived\n",
    "\n",
    "## üìö References\n",
    "- LangChain Document Loaders Documentation\n",
    "- Corporate Data Retention Policy 2024\n",
    "- Week 05 Data Intake Notebook"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
