{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dd7a6d6",
   "metadata": {},
   "source": [
    "# ðŸ” Week 07-08 Â· Notebook 07: Advanced RAG - Retrievers\n",
    "\n",
    "**Objective:** Move beyond basic similarity search and build sophisticated, multi-stage retrieval pipelines that deliver more accurate and context-aware results.\n",
    "\n",
    "In the previous notebook, we built a basic retrieval system: we indexed documents in a vector store and used similarity search to find relevant chunks. While this is a great start, it often falls short in real-world scenarios. User questions can be ambiguous, and simple vector similarity doesn't always capture the true relevance of a document.\n",
    "\n",
    "**Advanced RAG** introduces more sophisticated techniques to improve the \"Retrieval\" step. A better retriever leads to a better final answer from the LLM. In this notebook, we will focus on the **Retriever** component itself and explore several powerful patterns:\n",
    "\n",
    "1.  **Multi-Query Retriever:** Tackles ambiguity by generating multiple variations of a user's question from different perspectives.\n",
    "2.  **Parent Document Retriever:** Solves the \"lost-in-the-middle\" problem by retrieving small, precise chunks but providing the larger parent document to the LLM for better context.\n",
    "3.  **Re-ranking:** Adds a secondary sorting step to the retrieved results, applying custom business logic or a more powerful model to improve the final ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92d3c19",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1.  **Implement a Multi-Query Retriever:** Use an LLM to automatically generate and run multiple search queries to improve recall.\n",
    "2.  **Use the Parent Document Retriever:** Structure your data to retrieve small chunks but return their parent documents for improved LLM context.\n",
    "3.  **Apply a Re-ranking Step:** Implement a custom re-ranking function to sort retrieved documents based on specific business heuristics (e.g., prioritizing safety alerts).\n",
    "4.  **Understand the Trade-offs:** Compare and contrast these advanced techniques with basic similarity search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf8180a",
   "metadata": {},
   "source": [
    "## ðŸ§© Scenario: Improving the Maintenance Chatbot's Retrieval Accuracy\n",
    "\n",
    "The maintenance chatbot is live, but technicians are reporting some issues. A vague query like *\"spindle vibrating\"* sometimes misses critical safety alerts or fails to find the root cause from past incidents. A simple similarity search isn't enough.\n",
    "\n",
    "Your task is to upgrade the retrieval system to be more robust and intelligent. You will implement and test three advanced patterns:\n",
    "\n",
    "1.  When a technician asks, *\"Why is the spindle vibrating?\"*, you'll use a **Multi-Query Retriever** to also search for related concepts like *\"causes of spindle vibration\"* and *\"troubleshooting high spindle vibration\"*.\n",
    "2.  You'll use a **Parent Document Retriever** to ensure that when a small, relevant sentence from a long SOP is found, the LLM receives the full section for complete context.\n",
    "3.  Finally, you'll add a **re-ranking** step that explicitly boosts the score of any retrieved document containing \"Safety-Alert\" or \"Incident\", ensuring that critical information always appears first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec7e2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# --- 1. Setup: Create a sample vector store ---\n",
    "# This represents our knowledge base of SOPs, maintenance logs, etc.\n",
    "sample_texts = [\n",
    "    \"SOP-101: For high spindle vibration, first check for loose tool holders. Torque to 50 Nm.\",\n",
    "    \"Incident-552: High vibration on Press-04 was caused by bearing failure. Downtime: 8 hours. Cost: $50,000.\",\n",
    "    \"SOP-102: Spindle bearing replacement must be done in a clean environment. Use only approved lubricants.\",\n",
    "    \"Safety-Alert-7: Immediate shutdown is required if spindle vibration exceeds 10 mm/s. This is a critical safety issue.\",\n",
    "    \"Troubleshooting-Guide: Common causes of vibration include tool imbalance, bearing wear, and incorrect speed settings.\"\n",
    "]\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "vectorstore = Chroma.from_texts(sample_texts, embeddings)\n",
    "\n",
    "# --- 2. Multi-Query Retriever ---\n",
    "# This retriever generates multiple variations of the user's question to improve recall.\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(), \n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# --- 3. Build the RAG Chain using LCEL ---\n",
    "# This is the modern way to build RAG chains in LangChain.\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful manufacturing assistant. Answer the user's question based on the following context. Prioritize safety alerts.\"),\n",
    "    (\"human\", \"Context:\\n{context}\\n\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# --- 4. Run the chain ---\n",
    "question = 'Why is spindle vibration high after maintenance? Provide troubleshooting steps.'\n",
    "response = rag_chain.invoke(question)\n",
    "\n",
    "print(\"--- RAG Response ---\")\n",
    "print(response)\n",
    "\n",
    "# --- Bonus: See the generated queries ---\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "\n",
    "retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c906e8",
   "metadata": {},
   "source": [
    "### ðŸ§­ Hierarchical Retrieval\n",
    "1. Retrieve relevant SOP sections via embeddings.\n",
    "2. Drill down to paragraphs referencing maintenance step IDs.\n",
    "3. Apply heuristic scoring: safety incidents > downtime costs > general tips.\n",
    "4. Log retrieval path for explainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5dee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents(docs):\n",
    "    \"\"\"\n",
    "    Applies a simple heuristic to rank documents based on keywords.\n",
    "    In a real system, this would be more sophisticated, using metadata, etc.\n",
    "    \"\"\"\n",
    "    ranked_docs = []\n",
    "    for doc in docs:\n",
    "        content = doc.page_content.lower()\n",
    "        score = 0\n",
    "        if \"safety-alert\" in content or \"critical safety\" in content:\n",
    "            score = 100\n",
    "        elif \"incident\" in content or \"downtime\" in content:\n",
    "            score = 50\n",
    "        elif \"sop\" in content:\n",
    "            score = 20\n",
    "        else:\n",
    "            score = 10\n",
    "        \n",
    "        ranked_docs.append({\"doc\": doc, \"score\": score})\n",
    "        \n",
    "    # Sort documents by score in descending order\n",
    "    return sorted(ranked_docs, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "# --- Example Usage ---\n",
    "# First, retrieve the documents\n",
    "retrieved_docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "# Then, rank them\n",
    "ranked_results = rank_documents(retrieved_docs)\n",
    "\n",
    "print(\"\\n--- Heuristically Ranked Documents ---\")\n",
    "for result in ranked_results:\n",
    "    print(f\"Score: {result['score']}, Content: {result['doc'].page_content[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75719df9",
   "metadata": {},
   "source": [
    "## ðŸ§ª Lab Assignment\n",
    "1. Implement hierarchical retriever using LangChain `ParentDocumentRetriever` for SOP tree.\n",
    "2. Add weight adjustments based on downtime impact from cost database.\n",
    "3. Export retrieval traces to JSON for auditors.\n",
    "4. Present RAG improvements vs. baseline to stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96047e55",
   "metadata": {},
   "source": [
    "## âœ… Checklist\n",
    "- [ ] Multi-query retriever configured\n",
    "- [ ] Hierarchical retrieval implemented\n",
    "- [ ] Heuristic ranking documented\n",
    "- [ ] Lab deliverables shared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12113fe",
   "metadata": {},
   "source": [
    "## ðŸ“š References\n",
    "- LangChain Multi-Query Retriever\n",
    "- Hierarchical Retrieval Patterns\n",
    "- Week 09 Evaluation Harness"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
