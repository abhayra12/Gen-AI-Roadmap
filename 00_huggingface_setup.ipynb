{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6b154ec",
   "metadata": {},
   "source": [
    "# üöÄ HuggingFace LLM Setup Guide for Course Notebooks\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to use **HuggingFace Inference Endpoints** instead of local models or OpenAI.\n",
    "\n",
    "### Why HuggingFace Inference API?\n",
    "\n",
    "‚úÖ **No Local GPU Required** - Runs on any machine\n",
    "‚úÖ **Free Tier Available** - Great for learning\n",
    "‚úÖ **Production Ready** - Scalable and reliable\n",
    "‚úÖ **Many Models** - Llama-2, Mistral, Flan-T5, and more\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "1. HuggingFace Account: [Sign up here](https://huggingface.co/join)\n",
    "2. HuggingFace Token: [Create here](https://huggingface.co/settings/tokens)\n",
    "3. Add token to `.env` file in project root\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156e798b",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup: Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949e33b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get HuggingFace token\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    raise ValueError(\n",
    "        \"‚ùå HUGGINGFACE_TOKEN not found!\\n\"\n",
    "        \"Please ensure .env file exists in project root with:\\n\"\n",
    "        \"HUGGINGFACE_TOKEN=hf_your_token_here\"\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ HuggingFace token loaded successfully!\")\n",
    "print(f\"   Token: {HF_TOKEN[:10]}...{HF_TOKEN[-4:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295d4704",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Method 1: Using LangChain HuggingFaceEndpoint (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b105f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required package if not already installed\n",
    "# !pip install langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699ec8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "# Initialize HuggingFace LLM\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=os.getenv(\"HF_LLM_MODEL\", \"meta-llama/Llama-2-7b-chat-hf\"),\n",
    "    huggingfacehub_api_token=HF_TOKEN,\n",
    "    temperature=float(os.getenv(\"HF_TEMPERATURE\", \"0.7\")),\n",
    "    max_new_tokens=int(os.getenv(\"HF_MAX_TOKENS\", \"512\")),\n",
    "    timeout=int(os.getenv(\"HF_REQUEST_TIMEOUT\", \"60\")),\n",
    ")\n",
    "\n",
    "print(\"‚úÖ HuggingFace LLM initialized!\")\n",
    "print(f\"   Model: {llm.repo_id}\")\n",
    "print(f\"   Temperature: {llm.temperature}\")\n",
    "print(f\"   Max tokens: {llm.max_new_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30ca280",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Test the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201ad03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a simple prompt\n",
    "response = llm.invoke(\"What is machine learning? Explain in one sentence.\")\n",
    "print(\"ü§ñ LLM Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9687a74",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Using with LangChain Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0bb83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# Create a prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a helpful manufacturing expert. Answer this question: {question}\"\n",
    ")\n",
    "\n",
    "# Create a chain\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Test the chain\n",
    "result = chain.invoke({\"question\": \"What are common CNC machine maintenance tasks?\"})\n",
    "print(\"üîó Chain Response:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c9003b",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Alternative Models You Can Try\n",
    "\n",
    "Update your `.env` file with different `HF_LLM_MODEL` values:\n",
    "\n",
    "### Recommended Models:\n",
    "\n",
    "1. **Llama-2-7b-chat-hf** (Default)\n",
    "   - Model: `meta-llama/Llama-2-7b-chat-hf`\n",
    "   - Best for: General conversation and Q&A\n",
    "   - Requires: HuggingFace account agreement to terms\n",
    "\n",
    "2. **Mistral-7B-Instruct-v0.2**\n",
    "   - Model: `mistralai/Mistral-7B-Instruct-v0.2`\n",
    "   - Best for: Instruction following, coding\n",
    "   - Faster inference times\n",
    "\n",
    "3. **Flan-T5-Large**\n",
    "   - Model: `google/flan-t5-large`\n",
    "   - Best for: Quick responses, lightweight\n",
    "   - No special access needed\n",
    "\n",
    "4. **Zephyr-7B-Beta**\n",
    "   - Model: `HuggingFaceH4/zephyr-7b-beta`\n",
    "   - Best for: High-quality responses\n",
    "   - Great alternative to Llama-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b8cd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Try Mistral model\n",
    "mistral_llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    huggingfacehub_api_token=HF_TOKEN,\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "\n",
    "response = mistral_llm.invoke(\"Explain what RAG is in AI.\")\n",
    "print(\"üîÆ Mistral Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5bfc0b",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ For RAG: HuggingFace Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60719f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=os.getenv(\"HF_EMBEDDING_MODEL\", \"sentence-transformers/all-MiniLM-L6-v2\"),\n",
    "    model_kwargs={'device': 'cpu'}  # Works on CPU!\n",
    ")\n",
    "\n",
    "# Test embeddings\n",
    "test_text = \"This is a test sentence for embeddings.\"\n",
    "embedding = embeddings.embed_query(test_text)\n",
    "\n",
    "print(\"‚úÖ Embeddings initialized!\")\n",
    "print(f\"   Model: {embeddings.model_name}\")\n",
    "print(f\"   Embedding dimension: {len(embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b513ee0",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Complete RAG Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae9741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Sample documents\n",
    "docs = [\n",
    "    Document(page_content=\"CNC machines require regular maintenance including coolant checks and tool calibration.\"),\n",
    "    Document(page_content=\"Safety protocols require all operators to wear protective equipment when operating machinery.\"),\n",
    "    Document(page_content=\"Quality control involves visual inspection and dimensional measurements of manufactured parts.\"),\n",
    "]\n",
    "\n",
    "# Create vector store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"test_rag\"\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# RAG prompt\n",
    "rag_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the following context:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "# RAG chain\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Test RAG\n",
    "question = \"What maintenance is needed for CNC machines?\"\n",
    "answer = rag_chain.invoke(question)\n",
    "\n",
    "print(f\"‚ùì Question: {question}\")\n",
    "print(f\"\\n‚úÖ RAG Answer:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5258c928",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Migration from ChatOpenAI\n",
    "\n",
    "### ‚ùå Old Code (OpenAI):\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7,\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "```\n",
    "\n",
    "### ‚úÖ New Code (HuggingFace):\n",
    "```python\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    temperature=0.7,\n",
    "    huggingfacehub_api_token=os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    ")\n",
    "```\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "1. **Import**: `langchain_openai` ‚Üí `langchain_huggingface`\n",
    "2. **Class**: `ChatOpenAI` ‚Üí `HuggingFaceEndpoint`\n",
    "3. **Parameter**: `model` ‚Üí `repo_id`\n",
    "4. **Parameter**: `openai_api_key` ‚Üí `huggingfacehub_api_token`\n",
    "5. **Parameter**: Add `max_new_tokens` (replaces `max_tokens`)\n",
    "\n",
    "**Everything else stays the same!** LangChain chains, agents, and LCEL work identically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee293c9",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Troubleshooting\n",
    "\n",
    "### Issue: \"Token not found\"\n",
    "**Solution**: Ensure `.env` file exists in project root with `HUGGINGFACE_TOKEN=hf_...`\n",
    "\n",
    "### Issue: \"Model loading timeout\"\n",
    "**Solution**: First time may take 30-60 seconds. Increase `timeout` parameter or wait.\n",
    "\n",
    "### Issue: \"Rate limit exceeded\"\n",
    "**Solution**: HuggingFace free tier has limits. Wait a few minutes or upgrade to Pro.\n",
    "\n",
    "### Issue: \"Model requires agreement to terms\"\n",
    "**Solution**: For Llama-2, go to model page and accept terms:\n",
    "https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
    "\n",
    "### Issue: \"Slow inference\"\n",
    "**Solution**: Try smaller/faster models like `google/flan-t5-large` or `mistralai/Mistral-7B-Instruct-v0.2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a74333c",
   "metadata": {},
   "source": [
    "## üîü Best Practices\n",
    "\n",
    "1. **Start with smaller models** - `flan-t5-large` for testing\n",
    "2. **Use timeouts** - Set reasonable timeout values (30-60 seconds)\n",
    "3. **Cache results** - Store expensive LLM calls when possible\n",
    "4. **Monitor usage** - Check HuggingFace dashboard for API usage\n",
    "5. **Error handling** - Wrap LLM calls in try-except blocks\n",
    "6. **Temperature tuning** - Lower (0.3-0.5) for factual, higher (0.7-0.9) for creative\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "You now know how to:\n",
    "- ‚úÖ Set up HuggingFace tokens\n",
    "- ‚úÖ Use HuggingFaceEndpoint in LangChain\n",
    "- ‚úÖ Create chains and RAG systems\n",
    "- ‚úÖ Migrate from OpenAI to HuggingFace\n",
    "- ‚úÖ Troubleshoot common issues\n",
    "\n",
    "**Use this pattern in all course notebooks!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
