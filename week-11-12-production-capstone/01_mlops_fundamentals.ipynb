{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e0762fe",
   "metadata": {},
   "source": [
    "# üß± Week 11-12 ¬∑ Notebook 01 ¬∑ MLOps Fundamentals for the Manufacturing Copilot\n",
    "\n",
    "This notebook lays the MLOps foundation for our capstone project: **The Manufacturing Copilot**. We will design an auditable, production-grade lifecycle for the GenAI agents that power our copilot's maintenance, quality, and reporting workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c3aad2",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "\n",
    "- **Map the GenAI Lifecycle:** Design a complete MLOps lifecycle for the Manufacturing Copilot, from data ingestion and model training to production monitoring and feedback.\n",
    "- **Implement Experiment Tracking:** Set up an MLflow server to track experiments, log parameters, and version models for the copilot's different agents (Vision, RAG, Reporting).\n",
    "- **Define Governance Artifacts:** Create the necessary documentation and artifacts (model cards, release notes) to ensure the copilot is auditable and compliant with manufacturing standards.\n",
    "- **Assess MLOps Readiness:** Produce a readiness scorecard to evaluate the maturity of our MLOps processes before deploying the final capstone project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618ad44f",
   "metadata": {},
   "source": [
    "## üß© Scenario: Building a Production-Ready Copilot\n",
    "\n",
    "As the lead AI Engineer for the **Manufacturing Copilot** project, your task is to establish a robust MLOps framework. The copilot will be deployed across multiple factory sites, each with slightly different data and requirements. Leadership requires a unified, auditable, and scalable lifecycle to ensure that all components of the copilot (models, prompts, and datasets) are versioned, tested, and approved before the final production launch in Week 12.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c1cf61",
   "metadata": {},
   "source": [
    "## üîÑ The Manufacturing Copilot MLOps Lifecycle\n",
    "\n",
    "A successful GenAI application requires a more complex lifecycle than traditional ML models. We need to manage not just code and models, but also prompts, vector embeddings, and multi-agent interactions.\n",
    "\n",
    "**Our Lifecycle Blueprint:**\n",
    "```\n",
    "[Data Ingestion] -> [Vector DB & Feature Store] -> [Model & Prompt Versioning] -> [CI/CD] -> [Deployment] -> [Monitoring] -> [Feedback Loop]\n",
    "```\n",
    "\n",
    "Here‚Äôs how each stage applies to our Manufacturing Copilot:\n",
    "\n",
    "| Stage                  | Manufacturing Copilot Considerations                                  | Tools Used                               | Governance Evidence                     |\n",
    "| ---------------------- | --------------------------------------------------------------------- | ---------------------------------------- | --------------------------------------- |\n",
    "| **Data Ingestion**     | Scrub PII from maintenance logs, ensure SOPs are current.             | `Pandas`, `Great Expectations`           | Data quality reports, SME sign-off      |\n",
    "| **Vector DB / Features** | Manage embeddings for RAG agent, version feature transformations.     | `ChromaDB`, `PostgreSQL`                 | Vector DB schema, feature definitions   |\n",
    "| **Model/Prompt Versioning** | Tag models by agent (Vision, RAG), version prompts for each task. | `Git`, `MLflow`, `DVC`                   | Model cards, prompt changelogs          |\n",
    "| **CI/CD**              | Run safety checks, agent-specific tests, and integration tests.       | `GitHub Actions`, `pytest`               | Pipeline logs, test coverage reports    |\n",
    "| **Deployment**         | Deploy API to Cloud Run, manage container versions.                   | `Docker`, `Terraform`, `GCP Cloud Run`   | Deployment manifests, runbooks          |\n",
    "| **Monitoring**         | Track agent latency, hallucination rates, and token costs.            | `Prometheus`, `Grafana`, `BigQuery`      | KPI dashboards, alert notifications     |\n",
    "| **Feedback Loop**      | Collect ratings from factory technicians on agent performance.        | Custom UI, `PostgreSQL`                  | Feedback analysis dashboard             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb71870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Logging a Manufacturing Copilot Agent to MLflow\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "# --- Configuration ---\n",
    "# In a real project, this would come from a config file (e.g., config.py or .env)\n",
    "# For local testing, you can run `mlflow ui` in your terminal to see the results.\n",
    "# This setup will create a local `mlflow.db` file to store experiment data.\n",
    "MLFLOW_TRACKING_URI = os.getenv(\"MLFLOW_TRACKING_URI\", \"sqlite:///mlflow.db\")\n",
    "EXPERIMENT_NAME = \"manufacturing_copilot_agents\"\n",
    "AGENT_NAME = \"RAG_Agent\"\n",
    "PLANT_ID = \"Pune-IN\"\n",
    "\n",
    "# --- Setup ---\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "# Create dummy artifacts for demonstration\n",
    "Path(\"artifacts/docs\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"artifacts/release_notes\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- MLflow Run ---\n",
    "# We start a run to track all the metadata for a specific version of our RAG agent.\n",
    "# A descriptive run name helps in quickly identifying experiments in the MLflow UI.\n",
    "run_name = f\"{AGENT_NAME}-{PLANT_ID}-{datetime.utcnow().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "\n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "    run_id = run.info.run_id\n",
    "    print(f\"Starting MLflow Run: {run.info.run_name} (ID: {run_id})\")\n",
    "    \n",
    "    # 1. Log Parameters: Key-value pairs that configure the agent. These are immutable.\n",
    "    print(\"Logging parameters...\")\n",
    "    mlflow.log_params({\n",
    "        \"plant_id\": PLANT_ID,\n",
    "        \"agent_type\": AGENT_NAME,\n",
    "        \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"llm_model\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        \"rag_chunk_size\": 512,\n",
    "        \"rag_chunk_overlap\": 50,\n",
    "    })\n",
    "\n",
    "    # 2. Log Metrics: Key-value pairs that measure performance. These can be updated.\n",
    "    print(\"Logging metrics...\")\n",
    "    mlflow.log_metrics({\n",
    "        \"retrieval_precision_at_5\": 0.92,\n",
    "        \"answer_relevancy\": 0.88,\n",
    "        \"average_latency_ms\": 250,\n",
    "        \"technician_satisfaction_score\": 4.6,\n",
    "    })\n",
    "\n",
    "    # 3. Log Artifacts: Any file you want to associate with the run (e.g., model files, plots, docs).\n",
    "    print(\"Logging artifacts...\")\n",
    "    model_card_content = f\"\"\"\n",
    "# Model Card: {AGENT_NAME} for {PLANT_ID}\n",
    "\n",
    "This agent provides maintenance support by answering questions based on technical documents.\n",
    "- **Embedding Model:** sentence-transformers/all-MiniLM-L6-v2\n",
    "- **LLM:** Llama-2-7b-chat-hf\n",
    "- **Performance:** Retrieval Precision@5 is 92%. See metrics for more details.\n",
    "\"\"\"\n",
    "    model_card_path = Path(\"artifacts/docs/rag_agent_model_card.md\")\n",
    "    model_card_path.write_text(model_card_content)\n",
    "    mlflow.log_artifact(str(model_card_path), artifact_path=\"documentation\")\n",
    "\n",
    "    # 4. Set Tags: Key-value pairs for organizing and filtering runs.\n",
    "    print(\"Setting tags...\")\n",
    "    mlflow.set_tags({\n",
    "        \"release_cycle\": \"Q4-2025\",\n",
    "        \"jira_ticket\": \"MFG-123\",\n",
    "        \"is_regulated\": \"True\",\n",
    "        \"status\": \"VALIDATED\"\n",
    "    })\n",
    "\n",
    "    # In a real scenario, you would also log the trained model/agent itself.\n",
    "    # For a RAG agent, this might be the serialized retriever pipeline.\n",
    "    # Example: mlflow.langchain.log_model(my_rag_chain, artifact_path=\"rag_agent_model\")\n",
    "\n",
    "    print(\"\\\\nRun completed successfully.\")\n",
    "    print(f\"To view the results, run 'mlflow ui' in your terminal and navigate to the '{EXPERIMENT_NAME}' experiment.\")\n",
    "\n",
    "run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5be2edb",
   "metadata": {},
   "source": [
    "### üóÉÔ∏è Registering the Agent in the Model Registry\n",
    "\n",
    "Once an experiment run is validated, we register its \"model\" (in this case, the agent's configuration and associated artifacts) to the MLflow Model Registry. This creates a versioned, governable asset that can be promoted through deployment stages (e.g., `Staging`, `Production`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8817a86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: To log a model with `mlflow.register_model`, you typically need a model object \n",
    "# that MLflow knows how to serialize (e.g., a scikit-learn, PyTorch, or LangChain model).\n",
    "# Since we are demonstrating the concept without a real model object, we'll create a \n",
    "# placeholder artifact and register that. In a real project, you'd use a specific flavor\n",
    "# like `mlflow.langchain.log_model` which handles both logging and registration.\n",
    "\n",
    "# Create a dummy model artifact to represent our agent\n",
    "dummy_model_dir = Path(\"artifacts/dummy_model\")\n",
    "dummy_model_dir.mkdir(exist_ok=True)\n",
    "(dummy_model_dir / \"model_config.json\").write_text('{\"agent_type\": \"RAG\", \"version\": 1}')\n",
    "(dummy_model_dir / \"retriever.pkl\").write_text(\"A pickled retriever object would go here.\")\n",
    "\n",
    "# It's best practice to log the model artifact within its original run\n",
    "with mlflow.start_run(run_id=run_id):\n",
    "    mlflow.log_artifacts(str(dummy_model_dir), artifact_path=\"rag_agent_model\")\n",
    "\n",
    "# Register the model from the completed run to the Model Registry\n",
    "model_name = \"copilot_rag_agent\"\n",
    "model_uri = f\"runs:/{run_id}/rag_agent_model\"\n",
    "\n",
    "print(f\"Registering model '{model_name}' from URI: {model_uri}\")\n",
    "\n",
    "try:\n",
    "    registered_model = mlflow.register_model(\n",
    "        model_uri=model_uri,\n",
    "        name=model_name,\n",
    "        tags={\"agent_framework\": \"langchain\", \"use_case\": \"maintenance_support\"}\n",
    "    )\n",
    "    print(f\"Model '{model_name}' (Version: {registered_model.version}) has been successfully registered.\")\n",
    "\n",
    "except mlflow.exceptions.MlflowException as e:\n",
    "    print(f\"Error registering model: {e}\")\n",
    "    print(\"This can happen if the model name is already in use with a different tracking URI or has other conflicts.\")\n",
    "    # As a fallback for demonstration, let's find the latest version if it exists\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    try:\n",
    "        latest_versions = client.get_latest_versions(name=model_name)\n",
    "        if latest_versions:\n",
    "            registered_model = latest_versions[0]\n",
    "            print(f\"Found existing model '{model_name}' (Version: {registered_model.version}).\")\n",
    "    except:\n",
    "        registered_model = None # Could not register or find\n",
    "        print(\"Could not register or find the model.\")\n",
    "\n",
    "if registered_model:\n",
    "    # You can also add descriptions to the registered model version\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    client.update_model_version(\n",
    "        name=model_name,\n",
    "        version=registered_model.version,\n",
    "        description=f\"This is Version {registered_model.version} of the RAG agent for the Manufacturing Copilot, validated for the {PLANT_ID} plant.\"\n",
    "    )\n",
    "    print(\"Added description to the registered model version.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7a2719",
   "metadata": {},
   "source": [
    "## üßæ Creating an Audit-Ready Release Note\n",
    "\n",
    "For compliance and governance, every model promoted to production needs a release note. This JSON file captures all the critical metadata, linking our agent back to the business context, data sources, and risk assessments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8fa91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Ensure the previous cells have run and `registered_model` is not None\n",
    "if 'registered_model' in locals() and registered_model is not None:\n",
    "    # Dynamically create the release note from the run data\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    run_data = client.get_run(run_id).data\n",
    "\n",
    "    release_note = {\n",
    "        \"release_id\": f\"REL-{AGENT_NAME}-{PLANT_ID}-{datetime.utcnow().strftime('%Y%m%d%H%M')}\",\n",
    "        \"release_date\": datetime.utcnow().isoformat(),\n",
    "        \"approved_by\": \"Jane Doe, Head of Quality Assurance\",\n",
    "        \"status\": \"APPROVED_FOR_STAGING\",\n",
    "        \"jira_ticket\": run_data.tags.get(\"jira_ticket\"),\n",
    "        \"mlflow_run_id\": run_id,\n",
    "        \"mlflow_model_name\": registered_model.name,\n",
    "        \"mlflow_model_version\": registered_model.version,\n",
    "        \"data_sources\": [\n",
    "            \"s3://manufacturing-docs/pune/sop_manuals_v8.pdf\",\n",
    "            \"s3://manufacturing-logs/pune/maintenance_logs_2025_Q3.parquet\"\n",
    "        ],\n",
    "        \"prompt_template_version\": \"prompts/rag_agent/v3.yaml\",\n",
    "        \"risk_assessment\": {\n",
    "            \"hallucination_mitigation\": \"Using RAG with a high relevancy threshold (0.9+) and citing sources in every response.\",\n",
    "            \"pii_leakage_control\": \"PII scrubbed during data ingestion pipeline using Presidio.\",\n",
    "            \"safety_guardrails\": \"Input/output content filtering for harmful language using Azure Content Safety.\",\n",
    "            \"overall_risk_level\": \"LOW\"\n",
    "        },\n",
    "        \"evaluation_summary\": {\n",
    "            \"retrieval_precision_at_5\": run_data.metrics.get(\"retrieval_precision_at_5\"),\n",
    "            \"answer_relevancy\": run_data.metrics.get(\"answer_relevancy\"),\n",
    "            \"technician_satisfaction_score\": run_data.metrics.get(\"technician_satisfaction_score\")\n",
    "        }\n",
    "    }\n",
    "\n",
    "    release_note_path = Path(f\"artifacts/release_notes/{release_note['release_id']}.json\")\n",
    "    with open(release_note_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(release_note, f, indent=4)\n",
    "\n",
    "    print(f\"Audit-ready release note created at: {release_note_path}\")\n",
    "    \n",
    "    # Log the release note back to the original MLflow run for a complete audit trail\n",
    "    with mlflow.start_run(run_id=run_id):\n",
    "        mlflow.log_artifact(str(release_note_path), artifact_path=\"release_documentation\")\n",
    "    \n",
    "    print(\"Release note has been logged to the MLflow run.\")\n",
    "    \n",
    "    # Display the created release note\n",
    "    print(\"\\\\n--- Release Note Content ---\")\n",
    "    print(json.dumps(release_note, indent=2))\n",
    "else:\n",
    "    print(\"Skipping release note creation because the model was not successfully registered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5692a886",
   "metadata": {},
   "source": [
    "## üßÆ MLOps Readiness Scorecard for the Copilot\n",
    "\n",
    "Before moving to full-scale deployment, we need to honestly assess the maturity of our MLOps practices. This scorecard helps us identify and prioritize gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4131697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scorecard_data = [\n",
    "    {\n",
    "        \"Dimension\": \"1. Data Management\",\n",
    "        \"Score\": 3,\n",
    "        \"Notes\": \"Automated data quality checks are in place. Need versioning for large datasets (e.g., DVC).\",\n",
    "    },\n",
    "    {\n",
    "        \"Dimension\": \"2. Model & Prompt Engineering\",\n",
    "        \"Score\": 4,\n",
    "        \"Notes\": \"MLflow tracking is robust for experiments. Prompt templates are versioned in Git.\",\n",
    "    },\n",
    "    {\n",
    "        \"Dimension\": \"3. Continuous Integration (CI)\",\n",
    "        \"Score\": 2,\n",
    "        \"Notes\": \"Basic unit tests exist. Need to add integration tests for agent workflows and security scanning (e.g., Gitleaks).\",\n",
    "    },\n",
    "    {\n",
    "        \"Dimension\": \"4. Continuous Deployment (CD)\",\n",
    "        \"Score\": 2,\n",
    "        \"Notes\": \"Manual deployment process. Need to build automated deployment pipeline (e.g., Terraform + GitHub Actions).\",\n",
    "    },\n",
    "    {\n",
    "        \"Dimension\": \"5. Monitoring & Observability\",\n",
    "        \"Score\": 1,\n",
    "        \"Notes\": \"No application or model monitoring currently in place. This is a critical gap. Need to implement Prometheus and Grafana.\",\n",
    "    },\n",
    "     {\n",
    "        \"Dimension\": \"6. Governance & Security\",\n",
    "        \"Score\": 3,\n",
    "        \"Notes\": \"Release notes provide an audit trail. Need to integrate secrets management (e.g., GCP Secret Manager or HashiCorp Vault).\",\n",
    "    },\n",
    "]\n",
    "\n",
    "scorecard = pd.DataFrame(scorecard_data)\n",
    "scorecard.set_index(\"Dimension\", inplace=True)\n",
    "\n",
    "# --- Visualize the Scorecard ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Create a color map: 1-2 are red, 3 is yellow, 4-5 are green\n",
    "colors = []\n",
    "for score in scorecard['Score']:\n",
    "    if score <= 2:\n",
    "        colors.append('#d9534f') # Red\n",
    "    elif score == 3:\n",
    "        colors.append('#f0ad4e') # Yellow\n",
    "    else:\n",
    "        colors.append('#5cb85c') # Green\n",
    "\n",
    "scorecard['Score'].plot(kind='barh', ax=ax, color=colors, width=0.8)\n",
    "\n",
    "# Add score labels to the bars\n",
    "for i, (score, note) in enumerate(zip(scorecard['Score'], scorecard['Notes'])):\n",
    "    ax.text(score + 0.1, i, str(score), va='center', ha='left', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_xlim(0, 5.5)\n",
    "ax.set_xlabel(\"Maturity Score (1=Low, 5=High)\", fontsize=12)\n",
    "ax.set_ylabel(\"\")\n",
    "ax.set_title(\"MLOps Readiness Scorecard for the Manufacturing Copilot\", fontsize=16, fontweight='bold')\n",
    "ax.tick_params(axis='y', labelsize=12)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\\\n--- Scorecard Details ---\")\n",
    "scorecard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3e37db",
   "metadata": {},
   "source": [
    "### Scorecard Interpretation & Action Plan\n",
    "\n",
    "- **Score Legend:** 1 (Non-existent) -> 5 (Fully Automated & Optimized)\n",
    "- **Critical Gaps:** Our most significant weaknesses are in **Monitoring (Score 1)** and **CD (Score 2)**.\n",
    "- **Action Plan for Capstone:**\n",
    "    1.  **Priority 1 (This Week):** Implement foundational monitoring for the FastAPI application.\n",
    "    2.  **Priority 2 (This Week):** Develop Terraform scripts for our GCP infrastructure.\n",
    "    3.  **Priority 3 (Next Week):** Build the CI/CD pipeline in GitHub Actions to automate testing and deployment.\n",
    "    4.  **Priority 4 (Next Week):** Integrate GCP Secret Manager for handling API keys and database credentials.\n",
    "\n",
    "This scorecard will be our guide for the rest of this module. We will systematically address these gaps to build a production-ready Manufacturing Copilot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d123378e",
   "metadata": {},
   "source": [
    "## üß™ Lab Assignment: Your Turn!\n",
    "\n",
    "Now it's your turn to practice these core MLOps concepts.\n",
    "\n",
    "1.  **Set Up and Explore MLflow:**\n",
    "    -   Ensure you have MLflow installed (`pip install mlflow`).\n",
    "    -   Execute all the Python cells above to create a local `mlflow.db` file and log your first experiment run.\n",
    "    -   Open a new terminal in VS Code and run the command: `mlflow ui`.\n",
    "    -   This will start the MLflow tracking server. Open the URL it provides (usually `http://127.0.0.1:5000`) in your browser.\n",
    "    -   Navigate to the `manufacturing_copilot_agents` experiment and click on the run you just created. Explore the parameters, metrics, tags, and artifacts.\n",
    "\n",
    "2.  **Log a New \"Vision Agent\" Experiment:**\n",
    "    -   Copy the main MLflow run cell (the one that starts with `with mlflow.start_run(...)`).\n",
    "    -   Modify it to log an experiment for a hypothetical `Vision_Agent`.\n",
    "    -   **Change the `run_name`** to include \"Vision_Agent\".\n",
    "    -   **Update the parameters:**\n",
    "        -   `agent_type`: \"Vision_Agent\"\n",
    "        -   `embedding_model`: \"google/vit-base-patch16-224\"\n",
    "        -   `llm_model`: \"N/A\"\n",
    "    -   **Update the metrics:**\n",
    "        -   `defect_detection_accuracy`: 0.98\n",
    "        -   `false_positive_rate`: 0.05\n",
    "        -   `average_inference_ms`: 80\n",
    "    -   **Update the tags:**\n",
    "        -   `jira_ticket`: \"MFG-456\"\n",
    "    -   Execute the cell to log this new run.\n",
    "\n",
    "3.  **Compare the Two Agent Runs:**\n",
    "    -   Go back to the MLflow UI. You should now see two runs in your experiment.\n",
    "    -   Select both the `RAG_Agent` and `Vision_Agent` runs by checking the boxes next to them.\n",
    "    -   Click the **\"Compare\"** button.\n",
    "    -   Analyze the comparison view. Notice how MLflow highlights the differences in parameters and metrics, which is invaluable for debugging and analysis.\n",
    "\n",
    "4.  **Register the Vision Agent Model:**\n",
    "    -   Using the `run_id` from your `Vision_Agent` run, adapt the model registration cell to register this new agent.\n",
    "    -   Give it a new model name, such as `copilot_vision_agent`.\n",
    "    -   Add a description to the registered model version explaining its purpose (e.g., \"This model detects scratches and dents on metal surfaces.\").\n",
    "\n",
    "5.  **Transition the RAG Agent to \"Staging\":**\n",
    "    -   In the MLflow UI, navigate to the \"Models\" page.\n",
    "    -   Click on the `copilot_rag_agent`.\n",
    "    -   Find the version you registered and use the \"Stage\" dropdown to transition it from `None` to `Staging`.\n",
    "    -   Add a comment to the transition, such as \"Initial validation passed. Ready for integration testing.\" This action creates an audit trail for model promotion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9b606b",
   "metadata": {},
   "source": [
    "## ‚úÖ Checklist for this Notebook\n",
    "\n",
    "- [X] Mapped the MLOps lifecycle for the Manufacturing Copilot.\n",
    "- [X] Successfully configured MLflow and logged a sample agent experiment.\n",
    "- [X] Registered a model version in the MLflow Model Registry.\n",
    "- [X] Generated a compliant, audit-ready release note.\n",
    "- [X] Created and analyzed the MLOps readiness scorecard to identify critical gaps.\n",
    "- [ ] **TODO:** Complete the Lab Assignment to practice these concepts yourself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446836bf",
   "metadata": {},
   "source": [
    "## üìö References and Further Reading\n",
    "\n",
    "-   [MLflow Official Documentation](https://mlflow.org/docs/latest/index.html)\n",
    "-   [The MLOps Lifecycle with MLflow (Databricks Blog)](https://www.databricks.com/blog/2019/10/22/the-mlops-lifecycle-with-mlflow.html)\n",
    "-   [Awesome MLOps (A curated list of MLOps tools)](https://github.com/visenger/awesome-mlops)\n",
    "-   [Great Expectations Documentation](https://greatexpectations.io/docs/)\n",
    "-   [Terraform for GCP Documentation](https://registry.terraform.io/providers/hashicorp/google/latest/docs)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
