{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a70d2f12",
   "metadata": {},
   "source": [
    "# ðŸ“ˆ Week 11-12 Â· Notebook 04 Â· Monitoring & Observability for the Manufacturing Copilot\n",
    "\n",
    "This notebook addresses a critical MLOps gap identified in our readiness scorecard: **Monitoring & Observability**. We will instrument our FastAPI service with production-grade logging, metrics, and alerting to track performance, cost, and the quality of the copilot's responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e6bd7b",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "- **Instrument with Prometheus:** Integrate the `prometheus-fastapi-instrumentator` library to automatically expose key metrics (latency, request counts, error rates) from our FastAPI endpoints.\n",
    "- **Implement Custom GenAI Metrics:** Create custom Prometheus metrics to track specific GenAI concerns, such as hallucination rates, token usage, and user feedback scores.\n",
    "- **Configure Structured Logging:** Set up structured (JSON) logging to capture detailed, queryable information with every request, including trace IDs for end-to-end visibility.\n",
    "- **Design Alerting Policies:** Define alerting rules in Prometheus's Alertmanager format to notify the on-call team about critical issues like SLA breaches or spikes in harmful responses.\n",
    "- **Visualize with Grafana:** Outline the key dashboards needed in Grafana to provide a real-time, holistic view of the Manufacturing Copilot's health.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e582ce5f",
   "metadata": {},
   "source": [
    "## ðŸ§© Scenario: From \"Is it working?\" to \"How well is it working?\"\n",
    "\n",
    "Our Manufacturing Copilot is now running in a staging environment. It's not enough to know that the API is \"up.\" Leadership and operations teams are asking critical questions:\n",
    "-   \"Are we meeting our latency SLA of <500ms for technicians on the floor?\"\n",
    "-   \"How often does the RAG agent fail to find an answer (hallucinate)?\"\n",
    "-   \"Which pieces of equipment are causing the most requests?\"\n",
    "-   \"How much is this service costing us in terms of LLM tokens per day?\"\n",
    "\n",
    "To answer these, we need a robust monitoring and observability stack.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eb8fc2",
   "metadata": {},
   "source": [
    "## ðŸ“Š The Three Pillars of Observability for GenAI\n",
    "\n",
    "We will focus on three key types of telemetry data:\n",
    "\n",
    "1.  **Metrics:** Numerical measurements aggregated over time (e.g., request rate, error count). Ideal for dashboards and high-level alerting.\n",
    "2.  **Logs:** Timestamped records of discrete events (e.g., a single API request). Ideal for deep-dive debugging and auditing.\n",
    "3.  **Traces:** A representation of the entire lifecycle of a request as it moves through different services. We've already laid the groundwork for this with our `X-Request-Trace-ID` header.\n",
    "\n",
    "### Key GenAI Metrics Framework\n",
    "\n",
    "| Metric Name                      | Type        | Description                                                              | Labels (`key=value`)             | Target / SLA                               |\n",
    "| -------------------------------- | ----------- | ------------------------------------------------------------------------ | -------------------------------- | ------------------------------------------ |\n",
    "| `copilot_requests_total`         | `Counter`   | Total number of requests to the copilot API.                             | `plant_id`, `status_code`, `agent` | N/A                                        |\n",
    "| `copilot_latency_seconds`        | `Histogram` | Latency distribution for API requests.                                   | `plant_id`, `agent`              | P95 < 0.5 seconds                          |\n",
    "| `copilot_llm_token_usage_total`  | `Counter`   | Total number of input and output tokens used by the LLM.                 | `plant_id`, `direction` (in/out) | Monitor for budget adherence               |\n",
    "| `copilot_hallucination_rate`     | `Gauge`     | Percentage of responses flagged as hallucinations or \"I don't know\".     | `plant_id`, `agent`              | < 5%                                       |\n",
    "| `copilot_user_feedback_score`    | `Gauge`     | Average user satisfaction score (e.g., 1-5 stars).                       | `plant_id`, `agent`              | > 4.0                                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2475c0cd",
   "metadata": {},
   "source": [
    "### 1. Instrumenting the FastAPI Service with Prometheus\n",
    "\n",
    "**Prometheus** is an open-source monitoring system that collects and stores its metrics as time series data. We will use the `prometheus-fastapi-instrumentator` library to automatically expose key metrics from our API.\n",
    "\n",
    "First, let's update our `app/main.py` to include the instrumentation and define our custom GenAI metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dbbe66",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile app/main.py -a\n",
    "\n",
    "# --- Prometheus Metrics ---\n",
    "from prometheus_fastapi_instrumentator import Instrumentator\n",
    "from prometheus_client import Counter, Gauge\n",
    "\n",
    "# Define custom metrics\n",
    "LLM_TOKEN_USAGE = Counter(\n",
    "    \"copilot_llm_token_usage_total\",\n",
    "    \"Total number of LLM tokens used\",\n",
    "    [\"plant_id\", \"agent_name\", \"direction\"]\n",
    ")\n",
    "\n",
    "HALLUCINATION_FLAG = Counter(\n",
    "    \"copilot_hallucination_total\",\n",
    "    \"Total number of responses flagged as hallucinations\",\n",
    "    [\"plant_id\", \"agent_name\"]\n",
    ")\n",
    "\n",
    "USER_FEEDBACK_SCORE = Gauge(\n",
    "    \"copilot_user_feedback_score\",\n",
    "    \"Average user feedback score on a sliding window\",\n",
    "    [\"plant_id\", \"agent_name\"]\n",
    ")\n",
    "\n",
    "# Instrument the app\n",
    "@app.on_event(\"startup\")\n",
    "async def startup():\n",
    "    \"\"\"Instrument the app before it starts.\"\"\"\n",
    "    Instrumentator().instrument(app).expose(app)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1320b9",
   "metadata": {},
   "source": [
    "Now, we need to modify our agent logic to update these metrics. Let's update `app/agents.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9cc3f2",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile app/agents.py\n",
    "\n",
    "from uuid import uuid4\n",
    "import asyncio\n",
    "import random\n",
    "from .models import DiagnosisRequest, DiagnosisResponse\n",
    "# Import the custom metrics from main\n",
    "from .main import LLM_TOKEN_USAGE, HALLUCINATION_FLAG\n",
    "\n",
    "async def run_copilot_inference(payload: DiagnosisRequest) -> DiagnosisResponse:\n",
    "    \"\"\"\n",
    "    This function simulates the full agentic workflow, including I/O-bound operations\n",
    "    and updating Prometheus metrics.\n",
    "    \"\"\"\n",
    "    agent_name = \"RAG_Agent\"\n",
    "    plant_id = payload.plant_id\n",
    "\n",
    "    # Simulate network latency\n",
    "    await asyncio.sleep(0.1)\n",
    "    \n",
    "    # 1. Vision Agent (Simulated)\n",
    "    vision_output = {\n",
    "        \"defects_found\": [\"micro-fracture\", \"surface-discoloration\"],\n",
    "        \"confidence\": 0.85,\n",
    "    }\n",
    "\n",
    "    await asyncio.sleep(0.15)\n",
    "\n",
    "    # 2. RAG Agent (Simulated)\n",
    "    rag_output = {\n",
    "        \"recommended_steps\": [\n",
    "            f\"1. For equipment {payload.equipment_id}, inspect the primary coolant line for leaks.\",\n",
    "            \"2. Verify torque settings on mounting bolts (Ref: SOP-123, Sec 4.2).\",\n",
    "            \"3. Escalate to Level-2 maintenance if vibration exceeds 5mm/s.\",\n",
    "        ],\n",
    "        \"cited_documents\": [\"SOP-123\", \"MAINT-GUIDE-V2\"],\n",
    "    }\n",
    "    \n",
    "    # --- Update Metrics ---\n",
    "    # Simulate token usage\n",
    "    input_tokens = len(payload.problem_description.split()) * 2 # Rough estimate\n",
    "    output_tokens = sum(len(step.split()) for step in rag_output[\"recommended_steps\"]) * 2\n",
    "    LLM_TOKEN_USAGE.labels(plant_id=plant_id, agent_name=agent_name, direction=\"input\").inc(input_tokens)\n",
    "    LLM_TOKEN_USAGE.labels(plant_id=plant_id, agent_name=agent_name, direction=\"output\").inc(output_tokens)\n",
    "\n",
    "    # Simulate hallucination check\n",
    "    if not rag_output[\"cited_documents\"]:\n",
    "        HALLUCINATION_FLAG.labels(plant_id=plant_id, agent_name=agent_name).inc()\n",
    "        confidence = 0.4 # Lower confidence if no docs are cited\n",
    "    else:\n",
    "        confidence = 0.91\n",
    "\n",
    "    # 3. Report Generation (Simulated)\n",
    "    report = f\"Incident Report for {payload.equipment_id} at {plant_id}: Visual inspection found {', '.join(vision_output['defects_found'])}. Recommended action: Follow RAG guidance.\"\n",
    "\n",
    "    return DiagnosisResponse(\n",
    "        request_id=uuid4(),\n",
    "        vision_analysis=vision_output,\n",
    "        rag_guidance=rag_output,\n",
    "        generated_report=report,\n",
    "        confidence_score=confidence,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91361490",
   "metadata": {},
   "source": [
    "With these changes, every time the `/v1/diagnose` endpoint is called, our custom metrics for token usage and hallucinations will be updated. The `prometheus-fastapi-instrumentator` automatically handles standard metrics like request counts and latency.\n",
    "\n",
    "After running the app, you can visit the `/metrics` endpoint (e.g., `http://localhost:8000/metrics`) to see the raw text-based exposition format that Prometheus scrapes.\n",
    "\n",
    "### 2. Structured Logging for Deeper Insights\n",
    "\n",
    "While metrics are great for aggregation, **structured logs** are essential for debugging individual requests. We want to move from simple string logs to queryable JSON objects.\n",
    "\n",
    "Let's install and configure `structlog`.\n",
    "\n",
    "```bash\n",
    "pip install structlog\n",
    "```\n",
    "\n",
    "Now, we'll create a new file, `app/logging_config.py`, to define our logging setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39110def",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile app/logging_config.py\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "import structlog\n",
    "\n",
    "def setup_logging(log_level: str = \"INFO\"):\n",
    "    \"\"\"\n",
    "    Configures structured logging for the application.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=log_level,\n",
    "        format=\"%(message)s\",\n",
    "        stream=sys.stdout,\n",
    "    )\n",
    "\n",
    "    structlog.configure(\n",
    "        processors=[\n",
    "            structlog.stdlib.filter_by_level,\n",
    "            structlog.stdlib.add_logger_name,\n",
    "            structlog.stdlib.add_log_level,\n",
    "            structlog.stdlib.PositionalArgumentsFormatter(),\n",
    "            structlog.processors.TimeStamper(fmt=\"iso\"),\n",
    "            structlog.processors.StackInfoRenderer(),\n",
    "            structlog.processors.format_exc_info,\n",
    "            structlog.processors.UnicodeDecoder(),\n",
    "            structlog.processors.JSONRenderer()\n",
    "        ],\n",
    "        context_class=dict,\n",
    "        logger_factory=structlog.stdlib.LoggerFactory(),\n",
    "        wrapper_class=structlog.stdlib.BoundLogger,\n",
    "        cache_logger_on_first_use=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21b5df2",
   "metadata": {},
   "source": [
    "And now we update `app/main.py` again to use this new logging configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c82aff5",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile app/main.py\n",
    "\n",
    "import time\n",
    "import logging\n",
    "from uuid import uuid4\n",
    "import structlog\n",
    "\n",
    "from fastapi import FastAPI, Request, Depends\n",
    "from .config import settings\n",
    "from .models import DiagnosisRequest, DiagnosisResponse, HealthStatus\n",
    "from .security import authorize_request\n",
    "from .agents import run_copilot_inference\n",
    "from .logging_config import setup_logging\n",
    "\n",
    "# --- Application Setup ---\n",
    "setup_logging(log_level=settings.LOG_LEVEL)\n",
    "logger = structlog.get_logger(\"manufacturing_copilot_api\")\n",
    "\n",
    "app = FastAPI(\n",
    "    title=settings.APP_TITLE,\n",
    "    version=settings.APP_VERSION,\n",
    "    description=\"API for interacting with the Manufacturing Copilot agents.\",\n",
    ")\n",
    "\n",
    "# --- Middleware for Observability ---\n",
    "@app.middleware(\"http\")\n",
    "async def add_observability_headers(request: Request, call_next):\n",
    "    trace_id = str(uuid4())\n",
    "    \n",
    "    # Bind key context variables to the logger for every request\n",
    "    structlog.contextvars.clear_contextvars()\n",
    "    structlog.contextvars.bind_contextvars(\n",
    "        trace_id=trace_id,\n",
    "        path=request.url.path,\n",
    "        method=request.method,\n",
    "    )\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    response = await call_next(request)\n",
    "    \n",
    "    duration_ms = (time.perf_counter() - start_time) * 1000\n",
    "    \n",
    "    response.headers[\"X-Request-Trace-ID\"] = trace_id\n",
    "    response.headers[\"X-Response-Time-ms\"] = f\"{duration_ms:.2f}\"\n",
    "    \n",
    "    logger.info(\n",
    "        \"request_completed\",\n",
    "        status_code=response.status_code,\n",
    "        duration_ms=duration_ms\n",
    "    )\n",
    "    \n",
    "    if duration_ms > 500:\n",
    "        logger.warning(\"high_latency_detected\", duration_ms=duration_ms)\n",
    "        \n",
    "    return response\n",
    "\n",
    "\n",
    "# --- API Endpoints ---\n",
    "@app.get(\"/health\", response_model=HealthStatus, tags=[\"Monitoring\"])\n",
    "async def health_check():\n",
    "    return HealthStatus(status=\"ok\")\n",
    "\n",
    "\n",
    "@app.post(\"/v1/diagnose\", response_model=DiagnosisResponse, tags=[\"Copilot\"])\n",
    "async def diagnose_problem(\n",
    "    payload: DiagnosisRequest,\n",
    "    user_id: str = Depends(authorize_request)\n",
    "):\n",
    "    logger.info(\n",
    "        \"diagnosis_request_received\",\n",
    "        user_id=user_id,\n",
    "        plant_id=payload.plant_id,\n",
    "        equipment_id=payload.equipment_id\n",
    "    )\n",
    "    \n",
    "    response = await run_copilot_inference(payload)\n",
    "    \n",
    "    logger.info(\"diagnosis_request_success\", confidence=response.confidence_score)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# --- Prometheus Metrics ---\n",
    "from prometheus_fastapi_instrumentator import Instrumentator\n",
    "from prometheus_client import Counter, Gauge\n",
    "\n",
    "LLM_TOKEN_USAGE = Counter(\n",
    "    \"copilot_llm_token_usage_total\",\n",
    "    \"Total number of LLM tokens used\",\n",
    "    [\"plant_id\", \"agent_name\", \"direction\"]\n",
    ")\n",
    "\n",
    "HALLUCINATION_FLAG = Counter(\n",
    "    \"copilot_hallucination_total\",\n",
    "    \"Total number of responses flagged as hallucinations\",\n",
    "    [\"plant_id\", \"agent_name\"]\n",
    ")\n",
    "\n",
    "USER_FEEDBACK_SCORE = Gauge(\n",
    "    \"copilot_user_feedback_score\",\n",
    "    \"Average user feedback score on a sliding window\",\n",
    "    [\"plant_id\", \"agent_name\"]\n",
    ")\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup():\n",
    "    Instrumentator().instrument(app).expose(app)\n",
    "    logger.info(\"Application startup complete. Metrics exposed at /metrics.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbc8335",
   "metadata": {},
   "source": [
    "### 3. Alerting Policy for GenAI Metrics\n",
    "\n",
    "Metrics are useless without automated alerting. We can define rules that will fire when a metric crosses a critical threshold. This example is for **Prometheus Alertmanager**. These rules would be saved in a file like `alert_rules.yml` and loaded by Prometheus.\n",
    "\n",
    "```yaml\n",
    "# alert_rules.yml\n",
    "\n",
    "groups:\n",
    "- name: GenAI_Copilot_Alerts\n",
    "  rules:\n",
    "  - alert: HighHallucinationRate\n",
    "    # This PromQL query calculates the rate of hallucinations over the last 15 minutes\n",
    "    # and compares it to the rate of total requests.\n",
    "    expr: (sum(rate(copilot_hallucination_total[15m])) by (plant_id, agent_name)) / (sum(rate(http_requests_total{job=\"fastapi\"}[15m])) by (plant_id, agent_name)) > 0.05\n",
    "    # The condition must be true for 5 minutes before firing to avoid \"flapping\" on brief spikes.\n",
    "    for: 5m\n",
    "    labels:\n",
    "      severity: critical\n",
    "      team: ai_operations\n",
    "    annotations:\n",
    "      summary: \"High hallucination rate for agent {{ $labels.agent_name }} at plant {{ $labels.plant_id }}.\"\n",
    "      description: \"The hallucination rate has been over 5% for the last 5 minutes. Current value is {{ $value | humanizePercentage }}. This indicates a potential issue with the RAG retriever or the underlying data.\"\n",
    "      runbook_url: \"https://internal-wiki.example.com/runbooks/hallucination-escalation\"\n",
    "\n",
    "  - alert: HighRequestLatency\n",
    "    # This PromQL query finds the 95th percentile latency over the last 10 minutes.\n",
    "    expr: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[10m])) by (le, job)) > 0.5\n",
    "    for: 5m\n",
    "    labels:\n",
    "      severity: warning\n",
    "      team: ai_operations\n",
    "    annotations:\n",
    "      summary: \"P95 API latency is above 500ms.\"\n",
    "      description: \"The API P95 latency has been over 500ms for 5 minutes. This is impacting user experience and violating our SLA.\"\n",
    "      runbook_url: \"https://internal-wiki.example.com/runbooks/api-latency-investigation\"\n",
    "      \n",
    "  - alert: NoUserFeedbackReceived\n",
    "    # This alert fires if no user feedback has been recorded for a specific plant in 24 hours.\n",
    "    expr: time() - max_over_time(copilot_user_feedback_score[24h]) > 86400\n",
    "    for: 1h\n",
    "    labels:\n",
    "      severity: info\n",
    "      team: product_management\n",
    "    annotations:\n",
    "      summary: \"No user feedback received from plant {{ $labels.plant_id }} in 24 hours.\"\n",
    "      description: \"The feedback mechanism might be broken or users are not engaging. Please investigate.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c0d188",
   "metadata": {},
   "source": [
    "### 4. Designing Grafana Dashboards for GenAI\n",
    "\n",
    "A picture is worth a thousand log lines. A well-designed dashboard provides an at-a-glance view of the system's health and performance. Hereâ€™s a blueprint for a Grafana dashboard for our Manufacturing Copilot.\n",
    "\n",
    "**Dashboard Title:** Manufacturing Copilot - Service Health\n",
    "\n",
    "**Variables:**\n",
    "- `plant_id`: A dropdown to filter the entire dashboard by a specific manufacturing plant.\n",
    "- `agent_name`: A dropdown to filter by a specific agent (e.g., `quality_control_agent`).\n",
    "\n",
    "---\n",
    "\n",
    "#### Row 1: Key Performance Indicators (KPIs)\n",
    "\n",
    "- **Stat Panel: P95 Latency**\n",
    "  - **Query:** `histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{job=\"fastapi\", plant_id=~\"$plant_id\"}[5m])) by (le))`\n",
    "  - **Unit:** Seconds\n",
    "  - **Thresholds:** Base: < 0.3s, Warning: 0.3s-0.5s, Critical: > 0.5s\n",
    "\n",
    "- **Stat Panel: Error Rate (%)**\n",
    "  - **Query:** `(sum(rate(http_requests_total{job=\"fastapi\", status_code=~\"5..\", plant_id=~\"$plant_id\"}[5m])) / sum(rate(http_requests_total{job=\"fastapi\", plant_id=~\"$plant_id\"}[5m]))) * 100`\n",
    "  - **Unit:** Percent\n",
    "  - **Thresholds:** Base: < 1%, Warning: 1-3%, Critical: > 3%\n",
    "\n",
    "- **Stat Panel: Hallucination Rate (%)**\n",
    "  - **Query:** `(sum(rate(copilot_hallucination_total{plant_id=~\"$plant_id\", agent_name=~\"$agent_name\"}[5m])) / sum(rate(http_requests_total{job=\"fastapi\", plant_id=~\"$plant_id\"}[5m]))) * 100`\n",
    "  - **Unit:** Percent\n",
    "  - **Thresholds:** Base: < 2%, Warning: 2-5%, Critical: > 5%\n",
    "\n",
    "- **Stat Panel: Average User Feedback**\n",
    "  - **Query:** `avg_over_time(copilot_user_feedback_score{plant_id=~\"$plant_id\"}[24h])`\n",
    "  - **Unit:** Stars (1-5)\n",
    "  - **Thresholds:** Base: > 4.0, Warning: 3.5-4.0, Critical: < 3.5\n",
    "\n",
    "---\n",
    "\n",
    "#### Row 2: Latency & Throughput Analysis\n",
    "\n",
    "- **Time Series Panel: API Latency (P95, P99, Median)**\n",
    "  - **Queries:**\n",
    "    - P99: `histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket{job=\"fastapi\", plant_id=~\"$plant_id\"}[5m])) by (le))`\n",
    "    - P95: `histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{job=\"fastapi\", plant_id=~\"$plant_id\"}[5m])) by (le))`\n",
    "    - Median: `histogram_quantile(0.5, sum(rate(http_request_duration_seconds_bucket{job=\"fastapi\", plant_id=~\"$plant_id\"}[5m])) by (le))`\n",
    "  - **Y-Axis:** Seconds\n",
    "\n",
    "- **Time Series Panel: Requests per Second (RPS)**\n",
    "  - **Query:** `sum(rate(http_requests_total{job=\"fastapi\", plant_id=~\"$plant_id\"}[1m])) by (status_code)`\n",
    "  - **Visualization:** Stacked bar chart, colored by status code (2xx green, 4xx orange, 5xx red).\n",
    "\n",
    "---\n",
    "\n",
    "#### Row 3: GenAI-Specific Metrics\n",
    "\n",
    "- **Time Series Panel: LLM Token Usage**\n",
    "  - **Query:** `sum(rate(copilot_llm_token_usage_total{plant_id=~\"$plant_id\", agent_name=~\"$agent_name\"}[5m])) by (token_type)`\n",
    "  - **Legend:** `{{token_type}}` (e.g., prompt_tokens, completion_tokens)\n",
    "  - **Y-Axis:** Tokens per second\n",
    "\n",
    "- **Time Series Panel: Hallucination Count**\n",
    "  - **Query:** `sum(increase(copilot_hallucination_total{plant_id=~\"$plant_id\", agent_name=~\"$agent_name\"}[5m])) by (agent_name)`\n",
    "  - **Visualization:** Bar chart.\n",
    "\n",
    "- **Table Panel: Top 5 Hallucinating Agents**\n",
    "  - **Query:** `topk(5, sum(increase(copilot_hallucination_total[24h])) by (agent_name))`\n",
    "  - **Columns:** Agent Name, Hallucination Count (24h)\n",
    "\n",
    "---\n",
    "\n",
    "#### Row 4: System & Resource Monitoring\n",
    "\n",
    "- **Gauge Panel: Active Agents**\n",
    "  - **Query:** `sum(copilot_active_agents{plant_id=~\"$plant_id\"})`\n",
    "  - **Unit:** Integer\n",
    "\n",
    "- **Time Series Panel: CPU / Memory Usage**\n",
    "  - **Queries:** Standard node exporter metrics for container CPU and Memory.\n",
    "  - **Y-Axis:** Percent / Bytes\n",
    "\n",
    "- **Log Panel: Recent Errors**\n",
    "  - **Source:** Loki\n",
    "  - **LogQL Query:** `{job=\"fastapi\", level=\"error\"} | json`\n",
    "  - **Description:** Shows the structured JSON logs for recent errors, allowing for quick inspection of `trace_id`, error messages, and other context.\n",
    "\n",
    "This dashboard provides a holistic view, combining high-level business metrics (feedback, hallucinations) with low-level system performance indicators (latency, resource usage). The use of variables makes it a powerful tool for targeted debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e72a032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- weekly_report_generator.py ---\n",
    "# This script could be run automatically to generate a weekly markdown report.\n",
    "\n",
    "from jinja2 import Template\n",
    "\n",
    "# In a real scenario, this data would be fetched from the Prometheus API.\n",
    "report_data = {\n",
    "    \"week_of\": \"2025-10-27\",\n",
    "    \"overall_health\": \"GREEN\",\n",
    "    \"p95_latency_ms\": 380,\n",
    "    \"sla_latency_ms\": 500,\n",
    "    \"uptime_percentage\": 99.98,\n",
    "    \"total_requests\": 150230,\n",
    "    \"hallucination_rate_percent\": 3.2,\n",
    "    \"avg_user_feedback\": 4.6,\n",
    "    \"total_cost_usd\": 750.45,\n",
    "    \"top_issue\": \"High latency spikes during shift change (7am-8am).\",\n",
    "    \"action_item\": \"Investigate database connection pool contention during peak load.\"\n",
    "}\n",
    "\n",
    "report_template_str = \"\"\"\n",
    "# Manufacturing Copilot Weekly Health Report\n",
    "\n",
    "**Week of:** {{ week_of }}\n",
    "**Overall Health:** {{ overall_health }}\n",
    "\n",
    "| Metric                      | Value                               | Target      |\n",
    "| --------------------------- | ----------------------------------- | ----------- |\n",
    "| Uptime                      | {{ uptime_percentage }}%            | > 99.9%     |\n",
    "| P95 Latency                 | {{ p95_latency_ms }}ms              | < {{ sla_latency_ms }}ms |\n",
    "| Total Requests              | {{ \"{:,}\".format(total_requests) }} | N/A         |\n",
    "| Hallucination Rate          | {{ hallucination_rate_percent }}%   | < 5%        |\n",
    "| Avg. User Feedback (1-5)    | {{ avg_user_feedback }}             | > 4.0       |\n",
    "| Estimated Weekly Cost (USD) | ${{ \"{:,.2f}\".format(total_cost_usd) }} | < $1,000    |\n",
    "\n",
    "---\n",
    "\n",
    "### Key Observations & Action Items\n",
    "\n",
    "*   **Top Issue:** {{ top_issue }}\n",
    "*   **Action Item:** {{ action_item }}\n",
    "\"\"\"\n",
    "\n",
    "template = Template(report_template_str)\n",
    "report = template.render(report_data)\n",
    "\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839e4795",
   "metadata": {},
   "source": [
    "## ðŸ§ª Lab Assignment: Instrument and Observe\n",
    "\n",
    "1.  **Instrument Your FastAPI Service:**\n",
    "    -   Add the `prometheus-fastapi-instrumentator` to your `main.py` as shown above.\n",
    "    -   Add at least one custom metric (e.g., `LLM_TOKEN_USAGE`).\n",
    "    -   Run your application and navigate to `http://localhost:8000/metrics`. You should see a long list of metrics.\n",
    "\n",
    "2.  **Set Up a Local Monitoring Stack:**\n",
    "    -   Create a `docker-compose.monitoring.yml` file that includes services for **Prometheus** and **Grafana**. You can find many examples online.\n",
    "    -   Configure Prometheus to scrape the `/metrics` endpoint of your running FastAPI application.\n",
    "    -   Run `docker-compose -f docker-compose.monitoring.yml up`.\n",
    "\n",
    "3.  **Create a Grafana Dashboard:**\n",
    "    -   Log in to Grafana (usually at `http://localhost:3000`).\n",
    "    -   Connect to your Prometheus data source.\n",
    "    -   Create a new dashboard with at least two panels:\n",
    "        -   A time-series graph showing the `http_requests_total`.\n",
    "        -   A gauge showing the value of your custom metric.\n",
    "\n",
    "4.  **Implement Structured Logging:**\n",
    "    -   Choose a structured logging library (like `structlog`).\n",
    "    -   Configure it to output JSON logs from your FastAPI application.\n",
    "    -   Modify your logging middleware to use the structured logger.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e0e4db",
   "metadata": {},
   "source": [
    "## âœ… Checklist for this Notebook\n",
    "\n",
    "- [X] Standard and custom GenAI metrics identified and defined.\n",
    "- [X] FastAPI application instrumented to export metrics to Prometheus.\n",
    "- [X] Structured logging format designed to include critical context like trace IDs.\n",
    "- [X] Alerting policies for key GenAI failure modes (hallucination, latency) drafted.\n",
    "- [X] Key Grafana dashboard panels designed for at-a-glance analysis.\n",
    "- [ ] **TODO:** Complete the Lab Assignment to set up a local monitoring stack and visualize your API's metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f056e6",
   "metadata": {},
   "source": [
    "## ðŸ“š References and Further Reading\n",
    "\n",
    "-   [Prometheus Documentation](https://prometheus.io/docs/introduction/overview/)\n",
    "-   [Grafana Documentation](https://grafana.com/docs/grafana/latest/)\n",
    "-   [prometheus-fastapi-instrumentator on GitHub](https://github.com/trallnag/prometheus-fastapi-instrumentator) - The library for instrumenting FastAPI.\n",
    "-   [Google SRE Handbook, Chapter 6: Monitoring Distributed Systems](https://sre.google/sre-book/monitoring-distributed-systems/) - A foundational text on monitoring philosophy.\n",
    "-   [structlog Documentation](https://www.structlog.org/en/stable/) - A popular library for structured logging in Python.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
