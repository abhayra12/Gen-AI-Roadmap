{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3fc5a0b",
   "metadata": {},
   "source": [
    "# â˜ï¸ Week 11-12 Â· Notebook 06 Â· Deploying to Google Cloud Platform (GCP)\n",
    "\n",
    "This notebook provides a practical guide to deploying our containerized Manufacturing Copilot application to Google Cloud, focusing on **Cloud Run** for serving and **Cloud Functions** for event-driven processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586a0897",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "- **Deploy to Cloud Run:** Configure and deploy our Docker container to Cloud Run, a serverless platform, enabling automatic scaling, version management, and HTTPS endpoints.\n",
    "- **Manage Secrets Securely:** Use Google Secret Manager to store and securely inject sensitive data (like API keys and database passwords) into the running container.\n",
    "- **Automate with Deployment Scripts:** Write Python scripts that use the `gcloud` CLI to automate and standardize the deployment process across different environments (staging, production).\n",
    "- **Implement Event-Driven Error Notifications:** Set up a pipeline using Cloud Logging, Pub/Sub, and Cloud Functions to automatically send notifications to a Teams channel or PagerDuty when errors occur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdc5be5",
   "metadata": {},
   "source": [
    "## ðŸ§© Scenario: A Scalable, Multi-Region Copilot Service\n",
    "\n",
    "The Manufacturing Copilot is a global success, and it needs to be deployed in multiple regions to serve plants in India (`asia-south1`) and Mexico (`us-central1`) with low latency.\n",
    "\n",
    "**The Architecture Requirements:**\n",
    "1.  **Primary Service:** The main FastAPI application will be deployed on **Cloud Run**. It must scale automatically based on traffic, from zero to a configured maximum.\n",
    "2.  **Asynchronous Processing:** User feedback (e.g., ratings on a RAG response) will be handled by a separate, lightweight **Cloud Function** that ingests the feedback and stores it in BigQuery without blocking the main application.\n",
    "3.  **Secure Configuration:** All prompts will be stored centrally in a Cloud Storage bucket, and all secrets (API keys, etc.) will be managed by Google Secret Manager.\n",
    "4.  **Private Networking:** The Cloud Run service needs to access an on-premises database, so it must be connected to a Virtual Private Cloud (VPC) that is linked to the factory network via Cloud VPN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5326bcd6",
   "metadata": {},
   "source": [
    "## ðŸ—ºï¸ GCP Deployment Architecture\n",
    "\n",
    "This diagram illustrates how the different GCP services work together to host the Manufacturing Copilot.\n",
    "\n",
    "```\n",
    "[Technician's Tablet] --HTTPS--> [Global Load Balancer]\n",
    "                                       |\n",
    "                                       +--> [Cloud Run: copilot-api-prod (us-central1)]\n",
    "                                       |\n",
    "                                       +--> [Cloud Run: copilot-api-prod (asia-south1)]\n",
    "\n",
    "[Cloud Run Service]\n",
    " |\n",
    " +-- Reads prompts from [Cloud Storage Bucket]\n",
    " |\n",
    " +-- Accesses secrets from [Secret Manager]\n",
    " |\n",
    " +-- Connects to on-prem DB via [VPC Connector] -> [Cloud VPN]\n",
    " |\n",
    " +-- Calls [Vertex AI API] for embeddings/models\n",
    " |\n",
    " +-- Sends logs to [Cloud Logging]\n",
    "\n",
    "[User Feedback] --HTTPS--> [Cloud Function: feedback-ingestor] --> [BigQuery]\n",
    "\n",
    "[Cloud Logging] --Log Sink--> [Pub/Sub Topic: error-notifications] --> [Cloud Function: incident-notifier] --> [Teams/PagerDuty]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f072263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- scripts/deploy_cloud_run.py ---\n",
    "# A Python script to standardize Cloud Run deployments.\n",
    "\n",
    "import subprocess\n",
    "import shlex\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict\n",
    "\n",
    "@dataclass\n",
    "class CloudRunConfig:\n",
    "    \"\"\"Configuration for a Cloud Run deployment.\"\"\"\n",
    "    service_name: str\n",
    "    image_uri: str\n",
    "    region: str\n",
    "    project_id: str\n",
    "    max_instances: int = 4\n",
    "    min_instances: int = 0  # Set to 1 for no cold starts, 0 for cost savings\n",
    "    vpc_connector: str = \"\" # Optional VPC connector name\n",
    "    secrets: Dict[str, str] = field(default_factory=dict) # Secret name -> Env var name\n",
    "\n",
    "def deploy_to_cloud_run(config: CloudRunConfig):\n",
    "    \"\"\"Constructs and runs the gcloud command to deploy a service.\"\"\"\n",
    "    \n",
    "    base_command = f\"\"\"\n",
    "        gcloud run deploy {config.service_name} \\\\\n",
    "            --image {config.image_uri} \\\\\n",
    "            --region {config.region} \\\\\n",
    "            --project {config.project_id} \\\\\n",
    "            --platform managed \\\\\n",
    "            --allow-unauthenticated \\\\\n",
    "            --min-instances {config.min_instances} \\\\\n",
    "            --max-instances {config.max_instances}\n",
    "    \"\"\"\n",
    "    \n",
    "    if config.vpc_connector:\n",
    "        base_command += f\" --vpc-connector {config.vpc_connector}\"\n",
    "        \n",
    "    if config.secrets:\n",
    "        secret_args = [f\"{env_var}={name}:latest\" for name, env_var in config.secrets.items()]\n",
    "        base_command += f\" --update-secrets={','.join(secret_args)}\"\n",
    "\n",
    "    # Use shlex to safely parse the command string\n",
    "    command_list = shlex.split(base_command.strip())\n",
    "    \n",
    "    print(\"--- Running Deployment Command ---\")\n",
    "    print(\" \".join(command_list))\n",
    "    \n",
    "    try:\n",
    "        # In a real script, you would run this command.\n",
    "        # subprocess.run(command_list, check=True, capture_output=True, text=True)\n",
    "        print(\"\\n--- Deployment Successful (Simulated) ---\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"\\n--- Deployment Failed ---\")\n",
    "        print(f\"Error: {e.stderr}\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage for deploying to the staging environment\n",
    "    staging_config = CloudRunConfig(\n",
    "        project_id=\"manufacturing-copilot-dev\",\n",
    "        service_name=\"copilot-api-staging\",\n",
    "        image_uri=\"gcr.io/manufacturing-copilot-dev/copilot-api:sha-123abc\",\n",
    "        region=\"us-central1\",\n",
    "        min_instances=0,\n",
    "        max_instances=2,\n",
    "        secrets={\n",
    "            \"OPENAI_API_KEY\": \"ENV_OPENAI_API_KEY\",\n",
    "            \"DB_PASSWORD_STAGING\": \"ENV_DB_PASSWORD\"\n",
    "        }\n",
    "    )\n",
    "    deploy_to_cloud_run(staging_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3cc827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- scripts/deploy_cloud_function.py ---\n",
    "# A shell script is often simpler for deploying a single Cloud Function.\n",
    "\n",
    "deploy_function_script = \"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "# Deploys the feedback ingestor Cloud Function\n",
    "\n",
    "PROJECT_ID=\"manufacturing-copilot-dev\"\n",
    "REGION=\"us-central1\"\n",
    "FUNCTION_NAME=\"feedback-ingestor\"\n",
    "ENTRY_POINT=\"ingest_feedback\"\n",
    "RUNTIME=\"python310\"\n",
    "TRIGGER=\"--trigger-http\" # HTTP-triggered function\n",
    "SOURCE_DIR=\"./src/functions/feedback\"\n",
    "BIGQUERY_TABLE=\"feedback.user_ratings\"\n",
    "\n",
    "echo \"Deploying function ${FUNCTION_NAME} to project ${PROJECT_ID}...\"\n",
    "\n",
    "gcloud functions deploy ${FUNCTION_NAME} \\\\\n",
    "  --project=${PROJECT_ID} \\\\\n",
    "  --region=${REGION} \\\\\n",
    "  --runtime=${RUNTIME} \\\\\n",
    "  --entry-point=${ENTRY_POINT} \\\\\n",
    "  ${TRIGGER} \\\\\n",
    "  --source=${SOURCE_DIR} \\\\\n",
    "  --allow-unauthenticated \\\\\n",
    "  --set-env-vars=BIGQUERY_TABLE=${BIGQUERY_TABLE}\n",
    "\n",
    "echo \"Deployment complete.\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Cloud Function Deployment Script ---\")\n",
    "print(deploy_function_script)\n",
    "# In a real project, you would save this as a .sh file and execute it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442f9d79",
   "metadata": {},
   "source": [
    "## ðŸ” Securely Managing Secrets and Configuration\n",
    "\n",
    "Hardcoding secrets is a major security risk. We use specific GCP services to manage different types of configuration.\n",
    "\n",
    "| Configuration Type      | GCP Service             | How it's Used                                                                                             |\n",
    "| ----------------------- | ----------------------- | --------------------------------------------------------------------------------------------------------- |\n",
    "| **Sensitive Secrets**   | **Secret Manager**      | API keys, database passwords, signing keys. Injected into Cloud Run as environment variables or mounted as files. |\n",
    "| **Non-Sensitive Config**| **Environment Variables** | `LOG_LEVEL`, `PLANT_ID`. Set directly on the Cloud Run service during deployment.                         |\n",
    "| **Large Config Files**  | **Cloud Storage**       | LLM prompt templates, model configuration YAMLs. The application fetches these from a GCS bucket on startup. |\n",
    "\n",
    "### Example: Accessing a Secret in Python\n",
    "\n",
    "```python\n",
    "# This code would run inside the Cloud Run container.\n",
    "import os\n",
    "\n",
    "# The secret was mapped to an environment variable during deployment.\n",
    "api_key = os.getenv(\"ENV_OPENAI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"OpenAI API key not found in environment variables.\")\n",
    "\n",
    "# Now you can use the api_key to interact with the OpenAI client.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5420f74",
   "metadata": {},
   "source": [
    "## âš ï¸ Automated Error Notification Pipeline\n",
    "\n",
    "We can build a powerful, serverless pipeline to automatically notify us of problems.\n",
    "\n",
    "1.  **Log Sink:**\n",
    "    -   Go to the GCP Console > Logging > Log Router.\n",
    "    -   Create a sink with a filter like `severity>=ERROR AND resource.type=\"cloud_run_revision\"`.\n",
    "    -   Set the destination to a new Pub/Sub topic called `error-notifications`.\n",
    "\n",
    "2.  **Notifier Cloud Function:**\n",
    "    -   Create a new Cloud Function (`incident-notifier`) that is triggered by messages on the `error-notifications` Pub/Sub topic.\n",
    "    -   This function's code will parse the log entry and format a message to be sent to a webhook.\n",
    "\n",
    "    ```python\n",
    "    # main.py for the incident-notifier Cloud Function\n",
    "    import base64\n",
    "    import json\n",
    "    import requests\n",
    "\n",
    "    TEAMS_WEBHOOK_URL = \"https://your-webhook-url.office.com/...\"\n",
    "\n",
    "    def send_error_notification(event, context):\n",
    "        \"\"\"Triggered by a message on a Pub/Sub topic.\"\"\"\n",
    "        log_entry = json.loads(base64.b64decode(event['data']).decode('utf-8'))\n",
    "        \n",
    "        service_name = log_entry['resource']['labels']['service_name']\n",
    "        error_text = log_entry['textPayload']\n",
    "        trace_id = log_entry['trace'].split('/')[-1]\n",
    "\n",
    "        message = {\n",
    "            \"@type\": \"MessageCard\",\n",
    "            \"summary\": f\"Critical Error in {service_name}\",\n",
    "            \"text\": f\"**Service:** {service_name}\\\\n\\\\n**Error:** {error_text}\\\\n\\\\n**Trace ID:** {trace_id}\"\n",
    "        }\n",
    "        \n",
    "        requests.post(TEAMS_WEBHOOK_URL, json=message)\n",
    "    ```\n",
    "\n",
    "3.  **Result:**\n",
    "    -   Now, every time your Cloud Run service logs an error, a message will automatically appear in your configured Microsoft Teams channel within seconds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26046ba",
   "metadata": {},
   "source": [
    "## ðŸ§ª Lab Assignment: Deploy the Copilot\n",
    "\n",
    "1.  **Set Up Your GCP Project:**\n",
    "    -   Create a new GCP project.\n",
    "    -   Enable the required APIs: Cloud Run, Artifact Registry, Secret Manager, Cloud Build.\n",
    "    -   Create a service account for GitHub Actions to use for deployment.\n",
    "\n",
    "2.  **Push Your Container Image:**\n",
    "    -   Configure Docker to authenticate with Artifact Registry: `gcloud auth configure-docker`.\n",
    "    -   Build your Docker image and tag it with the Artifact Registry path: `docker build -t us-central1-docker.pkg.dev/your-project/copilot-repo/copilot-api:latest .`\n",
    "    -   Push the image: `docker push us-central1-docker.pkg.dev/your-project/copilot-repo/copilot-api:latest`.\n",
    "\n",
    "3.  **Create Secrets:**\n",
    "    -   In Secret Manager, create a secret for a placeholder API key.\n",
    "\n",
    "4.  **Deploy to Cloud Run:**\n",
    "    -   Modify and run the `deploy_cloud_run.py` script from this notebook, pointing it to your project, service name, and image URI.\n",
    "    -   Make sure to map the secret you created to an environment variable.\n",
    "    -   Once deployed, access the public URL provided by Cloud Run to test your service.\n",
    "\n",
    "5.  **Set Up a Health Check:**\n",
    "    -   Create a Cloud Scheduler job that sends a GET request to your service's `/health` endpoint every 5 minutes.\n",
    "    -   Configure it to post to a Pub/Sub topic if the health check fails.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d394f95",
   "metadata": {},
   "source": [
    "## âœ… Checklist for this Notebook\n",
    "\n",
    "- [X] Deployment architecture designed for multi-region, serverless deployment on GCP.\n",
    "- [X] Automated deployment scripts created for both Cloud Run and Cloud Functions.\n",
    "- [X] Secure configuration strategy defined using Secret Manager and Cloud Storage.\n",
    "- [X] A serverless error notification pipeline designed to provide real-time alerts.\n",
    "- [ ] **TODO:** Complete the Lab Assignment to deploy the containerized application to your own GCP project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fd13e2",
   "metadata": {},
   "source": [
    "## ðŸ“š References and Further Reading\n",
    "\n",
    "-   [GCP Cloud Run Documentation](https://cloud.google.com/run/docs)\n",
    "-   [GCP Secret Manager Documentation](https://cloud.google.com/secret-manager/docs)\n",
    "-   [Deploying to Cloud Run from GitHub Actions](https://github.com/google-github-actions/deploy-cloudrun)\n",
    "-   [Cloud Logging Sinks](https://cloud.google.com/logging/docs/export/configure_export_v2)\n",
    "-   [Tutorial: Serverless CI/CD on Google Cloud](https://cloud.google.com/architecture/serverless-ci-cd)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
