{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0659e298",
   "metadata": {},
   "source": [
    "# ðŸ Notebook 02: Python Essentials for Gen AI\n",
    "\n",
    "**Week 1-2: Python & ML Foundations**  \n",
    "**Gen AI Masters Program**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Objectives\n",
    "\n",
    "By the end of this notebook, you will master:\n",
    "1. âœ… Python data structures (lists, dicts, sets, tuples)\n",
    "2. âœ… Control flow (if/else, loops, comprehensions)\n",
    "3. âœ… Functions and lambda expressions\n",
    "4. âœ… Object-oriented programming basics\n",
    "5. âœ… File I/O and exception handling\n",
    "6. âœ… Python best practices for ML\n",
    "\n",
    "**Estimated Time:** 2-3 hours\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Why Python for Gen AI?\n",
    "\n",
    "Python is the de facto language for AI/ML because:\n",
    "- ðŸš€ **Rich Ecosystem**: PyTorch, TensorFlow, HuggingFace\n",
    "- ðŸ“Š **Data Science**: NumPy, Pandas, Matplotlib\n",
    "- ðŸ¤ **Easy Integration**: REST APIs, databases, cloud services\n",
    "- ðŸ‘¥ **Community**: Massive support and resources\n",
    "\n",
    "Let's master the essentials! ðŸŽ¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cfa7d7",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Python Data Structures\n",
    "\n",
    "### Lists - Ordered, Mutable Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece901ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating lists\n",
    "models = [\"GPT-4\", \"Claude\", \"Gemini\", \"Llama\"]\n",
    "scores = [0.95, 0.92, 0.90, 0.88]\n",
    "\n",
    "print(\"Models:\", models)\n",
    "print(\"First model:\", models[0])\n",
    "print(\"Last model:\", models[-1])\n",
    "\n",
    "# Adding elements\n",
    "models.append(\"Mistral\")\n",
    "print(\"\\nAfter append:\", models)\n",
    "\n",
    "# Slicing\n",
    "print(\"First 3 models:\", models[:3])\n",
    "print(\"Last 2 models:\", models[-2:])\n",
    "\n",
    "# List operations\n",
    "print(f\"\\nTotal models: {len(models)}\")\n",
    "print(f\"Is GPT-4 in list? {'GPT-4' in models}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06ee804",
   "metadata": {},
   "source": [
    "### Dictionaries - Key-Value Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c42e005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration (common in ML)\n",
    "model_config = {\n",
    "    \"name\": \"GPT-4\",\n",
    "    \"parameters\": \"175B\",\n",
    "    \"context_length\": 8192,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9\n",
    "}\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "for key, value in model_config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Accessing values\n",
    "print(f\"\\nModel name: {model_config['name']}\")\n",
    "print(f\"Context: {model_config.get('context_length', 'N/A')}\")\n",
    "\n",
    "# Adding new key\n",
    "model_config[\"provider\"] = \"OpenAI\"\n",
    "print(f\"\\nUpdated config: {model_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747b7344",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Control Flow\n",
    "\n",
    "### If-Else Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476737c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection based on score\n",
    "def select_model(score):\n",
    "    if score >= 0.95:\n",
    "        return \"Excellent model - Use for production\"\n",
    "    elif score >= 0.85:\n",
    "        return \"Good model - Fine-tune further\"\n",
    "    elif score >= 0.75:\n",
    "        return \"Average model - Consider alternatives\"\n",
    "    else:\n",
    "        return \"Poor model - Re-train needed\"\n",
    "\n",
    "# Test different scores\n",
    "test_scores = [0.98, 0.87, 0.76, 0.65]\n",
    "\n",
    "for score in test_scores:\n",
    "    result = select_model(score)\n",
    "    print(f\"Score {score:.2f}: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d00fa0",
   "metadata": {},
   "source": [
    "### Loops and Comprehensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893edada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional for loop\n",
    "prompts = [\"Summarize this text\", \"Translate to Spanish\", \"Extract entities\"]\n",
    "\n",
    "print(\"For loop:\")\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"  {i}. {prompt}\")\n",
    "\n",
    "# List comprehension (Pythonic way!)\n",
    "prompt_lengths = [len(p) for p in prompts]\n",
    "print(f\"\\nPrompt lengths: {prompt_lengths}\")\n",
    "\n",
    "# Filtering with comprehension\n",
    "long_prompts = [p for p in prompts if len(p) > 15]\n",
    "print(f\"Long prompts: {long_prompts}\")\n",
    "\n",
    "# Dictionary comprehension\n",
    "prompt_dict = {i: p for i, p in enumerate(prompts)}\n",
    "print(f\"\\nPrompt dictionary: {prompt_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d16707",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Functions and Lambda Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6d73ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional\n",
    "\n",
    "# Type hints make code clearer (important for ML pipelines)\n",
    "def preprocess_text(text: str, lowercase: bool = True, remove_punctuation: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess text for NLP tasks.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to preprocess\n",
    "        lowercase: Whether to convert to lowercase\n",
    "        remove_punctuation: Whether to remove punctuation\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed text\n",
    "    \"\"\"\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    if remove_punctuation:\n",
    "        import string\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Test the function\n",
    "sample_text = \"Hello, World! This is Gen AI.\"\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Lowercased:\", preprocess_text(sample_text))\n",
    "print(\"Clean:\", preprocess_text(sample_text, remove_punctuation=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3ebfa6",
   "metadata": {},
   "source": [
    "### Lambda Functions (Anonymous Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786776fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda for quick operations\n",
    "normalize = lambda x, min_val, max_val: (x - min_val) / (max_val - min_val)\n",
    "\n",
    "scores = [65, 75, 85, 95, 100]\n",
    "min_score, max_score = min(scores), max(scores)\n",
    "\n",
    "normalized_scores = [normalize(s, min_score, max_score) for s in scores]\n",
    "print(\"Original scores:\", scores)\n",
    "print(\"Normalized (0-1):\", [f\"{s:.2f}\" for s in normalized_scores])\n",
    "\n",
    "# Using lambda with map and filter\n",
    "texts = [\"hello world\", \"gen ai is amazing\", \"python rocks\"]\n",
    "\n",
    "# Map: Apply function to all elements\n",
    "word_counts = list(map(lambda x: len(x.split()), texts))\n",
    "print(f\"\\nWord counts: {word_counts}\")\n",
    "\n",
    "# Filter: Keep elements that meet condition\n",
    "long_texts = list(filter(lambda x: len(x) > 15, texts))\n",
    "print(f\"Long texts: {long_texts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb86b025",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Object-Oriented Programming (OOP)\n",
    "\n",
    "Classes are essential for building ML systems!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b401caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "\n",
    "@dataclass\n",
    "class LLMConfig:\n",
    "    \"\"\"Configuration for an LLM.\"\"\"\n",
    "    name: str\n",
    "    model_id: str\n",
    "    temperature: float = 0.7\n",
    "    max_tokens: int = 512\n",
    "    top_p: float = 0.9\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        \"\"\"Convert config to dictionary.\"\"\"\n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"model_id\": self.model_id,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"max_tokens\": self.max_tokens,\n",
    "            \"top_p\": self.top_p\n",
    "        }\n",
    "\n",
    "class TextGenerator:\n",
    "    \"\"\"Simple text generator class.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: LLMConfig):\n",
    "        self.config = config\n",
    "        self.history: List[str] = []\n",
    "    \n",
    "    def generate(self, prompt: str) -> str:\n",
    "        \"\"\"Generate text from prompt.\"\"\"\n",
    "        # Simulate generation\n",
    "        response = f\"[{self.config.name}] Response to: '{prompt}'\"\n",
    "        self.history.append(prompt)\n",
    "        return response\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear generation history.\"\"\"\n",
    "        self.history = []\n",
    "        print(\"History cleared\")\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"TextGenerator(model={self.config.name}, history_len={len(self.history)})\"\n",
    "\n",
    "# Create and use the generator\n",
    "config = LLMConfig(name=\"GPT-4\", model_id=\"gpt-4-turbo\")\n",
    "generator = TextGenerator(config)\n",
    "\n",
    "print(generator)\n",
    "print(\"\\nGeneration 1:\", generator.generate(\"What is Gen AI?\"))\n",
    "print(\"Generation 2:\", generator.generate(\"Explain transformers\"))\n",
    "print(f\"\\nHistory: {generator.history}\")\n",
    "print(f\"Config: {config.to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63ce66a",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ File I/O and Exception Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9701701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create a sample dataset\n",
    "dataset = {\n",
    "    \"prompts\": [\n",
    "        {\"id\": 1, \"text\": \"Explain machine learning\", \"category\": \"education\"},\n",
    "        {\"id\": 2, \"text\": \"Summarize this article\", \"category\": \"summarization\"},\n",
    "        {\"id\": 3, \"text\": \"Translate to French\", \"category\": \"translation\"}\n",
    "    ],\n",
    "    \"metadata\": {\n",
    "        \"version\": \"1.0\",\n",
    "        \"created\": \"2025-10-13\",\n",
    "        \"total_samples\": 3\n",
    "    }\n",
    "}\n",
    "\n",
    "# Write to JSON file\n",
    "output_file = \"sample_dataset.json\"\n",
    "\n",
    "try:\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(dataset, f, indent=2)\n",
    "    print(f\"âœ… Dataset saved to {output_file}\")\n",
    "    \n",
    "    # Read back\n",
    "    with open(output_file, 'r') as f:\n",
    "        loaded_dataset = json.load(f)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Loaded {len(loaded_dataset['prompts'])} prompts\")\n",
    "    print(f\"Metadata: {loaded_dataset['metadata']}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ File not found\")\n",
    "except json.JSONDecodeError:\n",
    "    print(\"âŒ Invalid JSON format\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "finally:\n",
    "    print(\"\\nâœ¨ File operation completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d45abb",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Python Best Practices for ML\n",
    "\n",
    "### Type Hints and Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e552dfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple, Union\n",
    "import numpy as np\n",
    "\n",
    "def calculate_metrics(\n",
    "    predictions: List[float],\n",
    "    targets: List[float]\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate regression metrics.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Model predictions\n",
    "        targets: Ground truth values\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing MSE and RMSE\n",
    "    \n",
    "    Example:\n",
    "        >>> preds = [1.0, 2.0, 3.0]\n",
    "        >>> targets = [1.1, 2.1, 2.9]\n",
    "        >>> metrics = calculate_metrics(preds, targets)\n",
    "        >>> print(metrics['mse'])\n",
    "        0.01\n",
    "    \"\"\"\n",
    "    if len(predictions) != len(targets):\n",
    "        raise ValueError(\"Predictions and targets must have same length\")\n",
    "    \n",
    "    mse = sum((p - t) ** 2 for p, t in zip(predictions, targets)) / len(predictions)\n",
    "    rmse = mse ** 0.5\n",
    "    \n",
    "    return {\n",
    "        \"mse\": round(mse, 4),\n",
    "        \"rmse\": round(rmse, 4),\n",
    "        \"samples\": len(predictions)\n",
    "    }\n",
    "\n",
    "# Test\n",
    "preds = [1.0, 2.0, 3.0, 4.0]\n",
    "targets = [1.1, 2.2, 2.9, 4.1]\n",
    "\n",
    "metrics = calculate_metrics(preds, targets)\n",
    "print(\"Metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14461ea",
   "metadata": {},
   "source": [
    "### Context Managers and Decorators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d696aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "# Decorator for timing functions\n",
    "def timer(func):\n",
    "    \"\"\"Decorator to time function execution.\"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        print(f\"â±ï¸  {func.__name__} took {end - start:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "# Apply decorator\n",
    "@timer\n",
    "def process_large_dataset(size: int):\n",
    "    \"\"\"Simulate processing a large dataset.\"\"\"\n",
    "    data = [i ** 2 for i in range(size)]\n",
    "    return sum(data)\n",
    "\n",
    "# Test\n",
    "result = process_large_dataset(1000000)\n",
    "print(f\"Result: {result:,}\")\n",
    "\n",
    "# Context manager example\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def model_inference_mode():\n",
    "    \"\"\"Context manager for model inference.\"\"\"\n",
    "    print(\"ðŸ”„ Entering inference mode...\")\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        print(\"âœ… Exiting inference mode\")\n",
    "\n",
    "# Use context manager\n",
    "with model_inference_mode():\n",
    "    print(\"   Running inference...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edd9199",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Practice Exercises\n",
    "\n",
    "Try these exercises to reinforce your learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ecc832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Create a function that tokenizes text\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into tokens (words).\n",
    "    \n",
    "    - Convert to lowercase\n",
    "    - Split on whitespace\n",
    "    - Remove empty strings\n",
    "    \"\"\"\n",
    "    return [token for token in text.lower().split() if token]\n",
    "\n",
    "# Test\n",
    "tokens = tokenize(\"Hello World! This is Gen AI.\")\n",
    "print(f\"Tokenize Exercise: {tokens}\")\n",
    "\n",
    "# Exercise 2: Build a simple caching decorator\n",
    "def cache_results(func):\n",
    "    \"\"\"\n",
    "    Decorator that caches function results.\n",
    "    \"\"\"\n",
    "    _cache = {}\n",
    "    @wraps(func)\n",
    "    def wrapper(*args):\n",
    "        if args in _cache:\n",
    "            print(f\"(from cache)\")\n",
    "            return _cache[args]\n",
    "        result = func(*args)\n",
    "        _cache[args] = result\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@cache_results\n",
    "@timer\n",
    "def expensive_calculation(a, b):\n",
    "    time.sleep(1) # Simulate a long computation\n",
    "    return a + b\n",
    "\n",
    "print(\"\\nCaching Decorator Exercise:\")\n",
    "expensive_calculation(1, 2)\n",
    "expensive_calculation(1, 2)\n",
    "\n",
    "\n",
    "# Exercise 3: Create a DataLoader class\n",
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    Simple data loader for batching data.\n",
    "    \"\"\"\n",
    "    def __init__(self, data: List, batch_size: int):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of batches.\"\"\"\n",
    "        return (len(self.data) + self.batch_size - 1) // self.batch_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yields batches of data.\"\"\"\n",
    "        for i in range(0, len(self.data), self.batch_size):\n",
    "            yield self.data[i:i + self.batch_size]\n",
    "\n",
    "print(\"\\nDataLoader Exercise:\")\n",
    "my_data = list(range(25))\n",
    "loader = DataLoader(my_data, batch_size=10)\n",
    "print(f\"Number of batches: {len(loader)}\")\n",
    "\n",
    "print(\"Iterating through batches:\")\n",
    "for i, batch in enumerate(loader):\n",
    "    print(f\"  Batch {i+1}: {batch}\")\n",
    "\n",
    "# You can still get a specific batch if needed, but iteration is more common\n",
    "all_batches = list(loader)\n",
    "print(f\"\\nLast batch: {all_batches[-1]}\")\n",
    "\n",
    "print(\"\\n\\nâœ… All exercises complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd784e0a",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Summary\n",
    "\n",
    "You've mastered Python essentials for Gen AI! Key takeaways:\n",
    "\n",
    "### Data Structures\n",
    "- âœ… Lists, dictionaries, sets, tuples\n",
    "- âœ… List/dict comprehensions for concise code\n",
    "\n",
    "### Control Flow\n",
    "- âœ… If/else statements\n",
    "- âœ… For/while loops\n",
    "- âœ… Comprehensions\n",
    "\n",
    "### Functions\n",
    "- âœ… Type hints for clarity\n",
    "- âœ… Lambda expressions for quick operations\n",
    "- âœ… Decorators for reusable logic\n",
    "\n",
    "### OOP\n",
    "- âœ… Classes and dataclasses\n",
    "- âœ… Methods and properties\n",
    "- âœ… Inheritance\n",
    "\n",
    "### Best Practices\n",
    "- âœ… Exception handling\n",
    "- âœ… File I/O with context managers\n",
    "- âœ… Documentation and type hints\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“š Next Steps\n",
    "\n",
    "Continue to **Notebook 03: NumPy & Pandas** to learn data manipulation!\n",
    "\n",
    "<div align=\"center\">\n",
    "<b>Great job! Ready for data science libraries! ðŸ“Š</b>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
